# 第三章 存储与获取

*Wer Ordnung hält, ist nur zu faul zum Suchen*

（如果你喜欢保持事物整齐有序，你只是太懒不想去寻找罢了）

德国谚语

---

从最基础的层次来讲，数据库需要做两件事：当你给它某些数据，它应该存储这些数据，并且稍后当你问它这些数据时，它应该把数据给回你。

在第二张中我们讨论了数据模型和查询语言——比如，你（应用开发者）给数据库数据的格式，以及稍后问它再要回来的机制。在这一章我们讨论同样的东西，只是从数据库的角度出发：我们如何存储给定的数据，以及当我们被要求提供它时如何找到它。

为什么你，作为一个应用开发者，应当关心数据库内部是如何处理存储与获取的呢？你大概不会从头开始实现自己的存储引擎，但是你*确实*需要从一众可用选择中挑出适合你应用的存储引擎。为了让存储引擎在你的工作负载上运行出色，你需要对当前工作的存储引擎有一个粗略的概念。

特别的是，**为事务性负载优化的存储引擎与为分析优化的存储引擎是有很大差别的**。稍后在“事物处理还是分析？”一节探索这个差别，而在“面向列的存储”一节我们将讨论为分析优化的一个存储引擎家族。

然而，首先我们将通过讨论我们熟悉的数据库种类所使用的存储引擎开启这一章：传统的关系型数据库，以及大部分的NoSQL数据库。我们会查看储存引擎的两个家族：*日志结构*的存储引擎，以及*面向页*的存储引擎，比如B树。

## 为数据库助力的数据结构
想象一下世界上最简单的数据库，由两个Bash函数实现：
```Shell
#!/bin/bash

db_set () {
    echo "$1,$2" >> database
}

db_get () {
    grep "^$1," database | sed -e "s/^$1,//" | tail -n 1
}
```
这两个函数实现了一个键值对存储。你可以调用`db_set key value`，把`key`与`value`储存在数据库中。键与值（几乎）可以是你想要的任何事物——举个例子，值可以是一个JSON文档。之后你可以调用`db_get key`，它会查找这个键绑定的最新的值并返回它。

而它确实可以工作的：
```Shell
$ db_set 123456 '{"name":"London","attractions":["Big Ben","London Eye"]}'
$ db_set 42 '{"name":"San Francisco","attractions":["Golden Gate Bridge"]}'
$ db_get 42
```
底层存储格式非常简单：一个文本文件，每一行包含一个键值对，用逗号隔开（大致与一个CSV文件类似，除了转义字符问题）。每一个`db_set`调用把值附加在文件结尾处，所以加入你更新某个键对应的值好几次，旧版本的值没有被覆盖——你需要查找文件中键出现的最后一次从而定位到最新的值（也就是`db_get`中的`tail -n 1`）：
```Shell
$ db_set 42 '{"name":"San Francisco","attractions":["Exploratorium"]}'

$ db_get 42
'{"name":"San Francisco","attractions":["Exploratorium"]}'

$ cat database
123456 '{"name":"London","attractions":["Big Ben","London Eye"]}'
42 '{"name":"San Francisco","attractions":["Golden Gate Bridge"]}'
42 '{"name":"San Francisco","attractions":["Exploratorium"]}'
```
我们的`db_set`函数实际上对简单的事物有着相当不错的性能，因为附加到一个文件通常是非常有效率的。与`db_set`的行为类似，许多数据库在内部使用*日志*，一个只能附加的数据文件。真实的数据库有更多的问题要解决（比如并发控制、回收磁盘空间从而日志大小不会永远增长下去、以及处理错误与部分写入的条目），但是基本原则是一样的。日志出人意料的有用，在书的余下部分我们还会遇到它们几次。

> 注意
>
> *日志*这个词经常用于指代应用程序日志，那是应用程序输出描述发生了什么的文本。本书使用了*日志*更一般的含义：一个只追加的条目序列。它不必是人类可读；它也许是二进制的，只是为了让其它程序读取。

另一方面，如果在我们的数据库中有大量条目，我们的`db_get`函数性能会非常差。每一次要查找一个键，`db_get`都要从头到尾扫描整个数据库文件，查找键的出现。用算法术语的话，一次查询的代价是`O(n)`:如果数据库中的条目数`n`翻倍，那么查找也要花掉双倍的时间。这可不妙。

为了有效率地在数据库中查找特定地键，我们需要一个不一样地数据结构：*索引*。在这一章我们将看到一系列的索引结构并比较它们；它们背后的大概概念是另外保留某些额外的元数据作为路标，并帮助你定位到你想要的数据。如果你要用集中不同的方式搜索同一个数据，你也许需要几种由不同部分数据构成的索引。

索引是由主数据衍生出来的*额外的*数据结构。许多数据库允许添加和移除索引，而这并不影响数据库的内容；它只影响查询的性能。维护多余的数据结构导致额外的开销，尤其是在写入的时候。对写入来说，要击败只是附加内容到文件的性能是很难的，因为那就是最简单的写入操作。任何类型的索引通常都会使写入变慢，因为每次数据写入时都需要更新索引。

在数据系统中这是很重要的权衡：精心挑选的索引使读取查询更快，但是每一个索引都会使写入变慢。由于这个原因，数据库默认不为所有数据创建索引，而是要求你——应用开发者或是数据库管理员——手动选择索引，这需要借助你对应用的典型查询模式的理解。这样你可以选择带给应用最大便利的索引，而不必引入不必要的开销。

### 哈希索引
让我们从键值对数据索引开始。当然你不只是能为这类数据建立索引，但是它很常见，同时也是更复杂索引的有用组件。键值存储与大多数编程语言中的*字典*类型非常类似，通常实现为一个哈希映射（哈希表）。哈希映射在许多算法教科书里都有描述，所以我们在这里不会具体讨论它是怎么工作的。既然我们已经为内存内数据结构使用了哈希映射，为什么不用它们为磁盘上的数据建立映射呢？

Let’s say our data storage consists only of appending to a file, as in the preceding example. Then the simplest possible indexing strategy is this: keep an in-memory hash map where every key is mapped to a byte offset in the data file — the location at which the value can be found, as illustrated in Figure   3-1. Whenever you append a new key-value pair to the file, you also update the hash map to reflect the offset of the data you just wrote (this works both for inserting new keys and for updating existing keys). When you want to look up a value, use the hash map to find the offset in the data file, seek to that location, and read the value.

假如说我们的数据存储只是附加数据到文件，就像之前的例子一样。那么最简单可行的索引策略是这样的：维护一个在内存中的哈希映射，其中每一个键都映射到数据文件的一个字节偏移地址——在这个地址可以找到对应的值，如图3-1所示。一旦一个新键值对被附加到文件，同时更新这个哈希映射从而反映刚刚写入数据的偏移地址（这个策略对插入新键以及更新已有的键都有效）。当你需要查找一个值时，用这个哈希映射找到数据文件中的偏移地址，寻址并读取值。

*图3-1 用类似CSV格式储存键值对日志，并用在内存中的哈希映射建立索引*

这听起来很简单，但确实是一种可行的方法。实际上，Bitcask（Riak的默认存储引擎）就是这么做的。Bitcask提供了高性能的读写，满足了所有键都可以存储在可用内存的需求，因为哈希映射完全储存在内存中。值会用掉比可用内存更多的空间，因为只需要一次寻址就可以从磁盘加载这些值。如果部分数据文件已经在文件系统的缓存中的话，那么读取完全不需要任何磁盘输入输出。

类似Bitcask这样的存储引擎非常适合值经常更新的情况。举个例子，键也许是一个关于猫的视频的URL，而值也许是视频被播放的次数（每当有任点击了播放键就增加）。在这样的负载下，有许多的写入，但是没有太多不同的键——每个键写入次数很多，但是把所有键保存在内存中是可行的。

到目前位置所描述的，都只是附加数据到文件——那么我们如何避免最终耗光磁盘空间呢？一个好的解决方案是把日志拆分为特定大小的段，当段达到特定大小之后关闭这个段，并把后续数据写入到一个新的段文件中。这样我们就可以对这些段文件进行压缩，如图3-2所示。压缩意味着丢弃日志中重复的键，只保留每个键最新的更新。

*图3-2 压缩一个键值对更新日志（记录每一个猫视频的播放次数），只保留每个键最新的更新*

此外，由于压缩经常使段文件变得非常小（假设一个键在一个段中平均被复写了好几次），我们还可以在压缩的同时合并好几个段文件，如图3-3所示。段文件在被写入后不会再被修改，于是合并了的段被写入到一个新文件。合并与压缩被冻结的段可以在后台线程中完成，而同时，我们可以继续如往常一样用老的段文件服务读取和写入请求。当合并过程结束，读取请求从就的段文件切换到到新的合并过的段——在这之后旧的段文件就可以删除了。

*图3-3. 同时执行压缩与合并*

现在每一个段都有了自己的在内存中的哈希表，把键映射到了文件偏移地址。为了找到键对应的值，我们首先检查最新的段的哈希映射；如果键不存在我们检查第二新的段，以此类推。合并使得段的数量很小，所以查找并不需要检查太多哈希映射。

Lots of detail goes into making this simple idea work in practice. Briefly, some of the issues that are important in a real implementation are:

许多细节都把这个简单的想法付诸实践。简而言之，真正的实现中一些重要的问题是：

*文件格式*

对于日志来说CSV并不是最好的格式。使用二进制格式更快也更简单：编码首先是以字节为单位的字符串长度，而后是原始字符串（不需要转义）。

*删除条目*

如果你要删除一个键以及对应的值，你必须附加一个特殊的删除条目到数据文件（有时被叫做*墓碑*）。当日志段被合并的时候，墓碑会告诉合并过程丢掉被删除的键对应的任何之前的值。

*崩溃恢复*

如果数据库被重启，在内存中的哈希映射会丢失。原则上，你可以通过从头到尾读取整个段文件，并在过程中标记每个键最新的值的偏移地址的方法恢复每一个段的哈希映射。然而，如果段文件很大的话这大概会花很长时间，使得服务器重启变得很痛苦。Bitcask通过把每一个段的哈希映射快照储存在磁盘上提高了恢复速度，快照可以非常快速的被加载到内存中。

*部分写入记录*

数据库随时有可能崩溃，其中当然也包括正在附加条目到日志的时候。Bitcask文件包含校验值，从而可以检测、忽略日志受损的部分。

*并发控制*

只允许附加的日志第一眼看起来是很浪费的：为什么不就地更新文件呢，用新的值覆盖旧的值？然而只允许附加的设计结果因为这几个原因被证明是很好的：
* 附加数据与段合并时顺序写入操作，这通常比随机写入要快得多，尤其是传统机械硬盘上。某种程度上相对于基于闪存芯片的*固态硬盘*顺序写入也是更优先的选择。我们会在“比较B树与LSM树”一节深入讨论这个问题。
* 如果段文件是只允许附加的或者不可变的话，并发与崩溃恢复就会更简单。举个例子，你不需要担心当值被复写时崩溃发生了，留下了一个包含部分旧数据和部分新数据的文件。
* 合并旧的段防止了数据文件碎片化的问题。

然而，哈希表索引也有局限：
* 哈希表必须能放到内存中，如果你有很大量的键的话就很不走运了。原则上你可以在磁盘上维护哈希映射，但是不幸的是让位于磁盘上的哈希映射运行良好很困难。它需要大量随机访问输入输出，变满了之后再要增长就会有很高的代价，而哈希散列需要非常繁琐的逻辑。
* 范围查询效率低下。举个例子，扫描所有在`kitty00000`到`kitty99999`之间的键非常不容易——你需要在哈希映射中挨个查找。

下一节我们会看到没有这些限制的索引结构。

### SSTable与LSM树
在图3-3中，每一个日志结构存储段都是一个键值对序列。这些键值对出现的顺序就是写入时的顺序，而日志中稍后出现的值优先于同一个键稍早之前出现的值。除此之外，文件中键值对的顺序完全无关。

现在我们对段文件的格式做一个简单的变化：我们要求键值对的次序是*按键排序的*。乍一看，这个要求好像使得我们无法继续使用顺序写入了，但是我们稍后再讨论这个问题。

我们把这种格式叫做*排序字符串表*，简称SSTable。我们还要求在每个合并后的段文件中每个键只能出现一次（压缩过程已经保证了这一点）。相比于有哈希索引的日志段，SSTable有几个大优势：
1. 合并段既简单效率又高，哪怕段文件比可用内存大。这种方式与*合并排序*算法用到的类似，如图3-4所示：开始的时候并列读取输入文件，取每个文件的第一个键，（根据排序顺序）拷贝顺序最小的的键到输入文件，然后重复这个过程。这样产生了一个新的合并段文件，同样按键排好了序。

    *图3-4 合并几个SSTable段，为每个键只保留了最新的值*

    如果同一个键出现在几个输入段呢？记住，每个段都包含了某个时间段内所有写入数据库的值。这意味着一个输入段中所有的值必然比其它段中所有的值都要新（假如我们一直合并相邻的段的话）。当多个段包含同一个键，我们可以保留最新的段中的值而丢弃在老的段中的值。

2. 为了找到文件中的某个特定键，你不用再在内存中保留所有键的索引了。看图3-5中的一个例子：假设你要找键`handiwork`，但是你不知道这个键再段文件中的偏移地址。然而，你却知道键*handbag*与*handsome*的偏移地址，由于排序你知道*handiwork*必然出现在这二者之间。这意味着你可以先跳到*handbag*的偏移地址然后从那里开始扫描，直到你找到*handiwork*（或者没有，如果这个键没有在这个文件中出现）。
    
    *图3-5 一个有内存索引的SSTable。*

    你仍然需要一个内存索引来告诉你某些键的偏移地址，它可以是很分散的：段文件内每几个KB存一个键就足够了，因为搜索几个KB是非常快的。

3. 因为不管怎样读取请求需要扫描请求范围内的一些键值对，把这些条目分组到一个数据块然后在写入磁盘前压缩它是可能的（图3-5中的暗影区域部分）。这样，每一个分散的内存索引条目都指向了一个压缩块的起始地址。除了节省了磁盘空间以外，压缩也降低了输入输出的带宽使用。

#### 构建与维护SSTable
目前为止还不错——但是首先你如何让数据按键排序？后续的写入可以是任意顺序的。

在磁盘上维护一个排好序的结构是可能的（见“B树”一节），但是在内存中维护会更简单一点。有足够多的已知的树结构可以用，比如红黑树或者AVL树。有这些数据结构，你可以以任何顺序插入键并且按顺序重新读回它们。

现在我们可以让我们的数据引擎按照下边的方式工作：
* 当一个写入发生时，把它添加到一个内存平衡树结构（比如，一个红黑树）中。这个内存树结构有时被叫做*内存表*。
* 当内存表的大小超过阙值时——一般几个MB——把内存表写到磁盘成为一个SSTable文件。因为树结构已经保持键值对按键排序，这个动作可以完成得很有效率。新的SSTable文件成为了数据库最新的段。在SSTable被写到磁盘的同时，写入动作可以继续进行，数据保存到了一个新的内存表实例中。
* 为了服务读取请求，首先尝试在内存表里查找键，然后是在磁盘上最新的段文件，然后是在第二新的段文件，等等。
* 时不时的在后台运行一个合并与压缩过程以合并段文件，并丢弃被复写或删除的值。

这个方案工作的非常出色。只受一个问题的影响：如果数据库崩溃，最近的写入（在内存表中还没有写入到磁盘）会丢失。为了防止这个问题，我们可以在磁盘上维护一个单独的日志，每一次写入都立即附加到这里，就像上一节里一样。这个日志不排序，但是这没关系，因为它的作用就是在崩溃之后恢复内存表。每一次内存表被写入到SSTable，这个对应的日志就可以被丢掉了。

#### 从SSTable生成LSM树
这里描述的算法正是LevelDB和RocksDB在用的，它们是被设计为可以嵌入到其他应用里的键值对存储引擎库。除此以外，LevelDB可以在Riak中作为Bitcask的替代品。类似的存储引擎也有在Cassandra与HBase中，这二者都是受谷歌Bigtable论文（里边引进了数据SSTable与内存表）的启发。

起初这种索引结构是Patrick O’Neil等人提出的，当时的名字是日志结构的合并树（或者LSM树），基于日志结构文件系统的早期成果。基于这种合并压缩排序文件原则的存储引擎也通常被叫做LSM存储引擎。

Lucene，一种Elasticsearch与Solr用到的针对全文搜索的索引引擎，用到了一种类似的方法来储存它的术语词典。全文索引要比键值对索引复杂的多但是基于一个类似的想法：在搜索查询中给定一个词，找到所有提到这个词的文档（网页、产品说明等等）。它是用键值对结构实现的，其中键是这个词（*术语*）而值是由包含这个词的文档的ID构成的列表（张贴清单）。在Lucene里，从术语到张贴清单的这种映射是保存在类似SSTable的排序文件中的，并且根据需要可以在后台合并。

#### 性能优化
如往常一样，许多细节使得存储引擎在实践中表现良好。举个例子，用LSM树算法在查找数据库中查找并不存在的键是很慢的：你必须检查内存表，之后从新往旧检查段文件（也许还需要从磁盘读取每一个文件）直到确认这个键并不存在。为了优化这一类访问，存储引擎经常使用额外的*Bloom过滤器*。（Bloom过滤器是用于近似集合内容的高效内存数据结构。它能告诉你键是不是在数据库中，因此为读取不存在的键节省了许多不必要的磁盘读取。）也有不同的判断如何压缩、合并SSTable的次序与时机的策略。最常见的选择是根据大小分层以及分级压缩。LevelDB与RocksDB使用分级压缩（这也就是为什么叫做LevelDB），HBase使用大小分层，而Cassandra二者都支持。在大小分层压缩中，更新更小的SSTable依次被合并到更老更大的SSTable中。在分级压缩中，键的范围被分成了稍小的SSTable而旧的数据被移到了不同的“层，”使得压缩更递进式的进行，而且磁盘空间用量更少。

虽然有许多微妙的地方，但是LSM树的基本理念——保持级联的SSTable在后台不断合并——是简单有效的。哪怕数据集大小比可用的内存空间大很多也可以运行良好。由于数据以排好的顺序存储，你可以有效地执行范围查询（搜索从某个最小值到某个最大值之间所有的键），同时因为磁盘写入是顺序的，LSM树于是支持相当高的写入吞吐量。

### B树
截至目前我们讨论的日志结构的索引越来越被接受，然而它们不是最常见类型的索引。使用最广泛的索引结构完全是另外一个东西：B树。

1970年推出并在之后不到10年的时间里“无处不在”，B树非常好地经受住了时间的考验。几乎在所有的关系型数据库中它仍然是标准的索引实现，而许多非关系型数据库也用它。

就像SSTable一样，B树保存按键排序的键值对，这使得键值对查找以及范围查询非常有效率。但是相似的地方也就这么多：B树有着完全不一样的设计哲学。

我们之前了解的日志结构的索引把数据库拆分成大小不定的*段*，一般大小在几个MB甚至更多，并且段总是顺序写入的。相比之下，B树把数据库拆分成了固定大小的*块*或者*页*，习惯上大小是4KB（有些时候会更大一些），并且同时只读或写一个页。这种设计更贴近底层硬件，因为磁盘也是以固定大小的块进行排列的。

每一个页用地址或者位置标记，这使得页可以指向其它页——与指针类似，但是是在磁盘上而不是在内存中。我们可以用这些页引用构建页面树，如图3-6所示。

*图3-6 用B树索引查找键*

一个页被指定为B树的*根*；每当在索引中查找键的时候，都是从这里开始的。这个页包含了数个键与指向子页的引用。每一个子页覆盖一个连续区间的键，引用之间的键指明了覆盖区间的上下界。

在图3-6的例子中，我们在查找键251，于是知道我们需要沿着上下界为200与300的页引用查找。它把我们带到了一个相似的页面，它进一步把200到300区间分成了数个子区间。最终我们会下到包含独立键的页（叶子页），在那里要么包含内联键对应的值要么包含可以找到值的页引用。

B树中一个页包含的子页引用数被叫做*分支因子*。举个例子，在图3-6中分支因子为6。在实践中，分支因子受页引用的大小以及范围上下界的影响，但是通常是几百。

如果要更新B树中一个已有键对应的值，那么查找包含那个键的叶子页，修改这个页中值，然后把页写回磁盘中（任何指向这个页的引用依旧有效）。如果要添加一个新键，需要找到包含这个新键的区间所在的页然后把它添加进去。如果页没有足够的空间容纳新的键，那么把它分为两个半满的页，而父页也需要被更新以体现区间的重新划分——见图3-7。

*图3-7 通过分页使B树增长*

整个算法保证了树一直保持*平衡*：有着*n*个键的B树高度总是*O(log n)*。大多数数据库都可以放到一个三或四层高的B树里，所不需要为了找到你要的页访问很多页引用。（一个四层高的、每个页4KB、分支因子为500的树可以储存高达256TB的数据。）

#### 让B树变得可靠
B树的基本底层写入操作是用新数据覆盖磁盘上的页。这是基于复写不会改变页的位置的假设的；比如，所有指向这个页的引用在页被覆盖的时候保持不变。这与日志结构的索引形成鲜明的对比，日志结构索引只是附加数据到文件（并且最终删除废弃的文件）但是从来不会就地修改文件。

你可以把覆盖磁盘上的页当作实际的硬件操作。在一个磁盘上，这意味着移动磁头到正确的位置，等待磁盘旋转到正确的位置，之后用新数据覆盖合适的扇区。在固态硬盘上，发生的情况要更复杂一些，，因为固态硬盘必须一次擦除并重新写入存储芯片中的很大一块。

此外，一些操作需要好几个不同的页被复写。举个例子，如果插入动作导致页面需要被分成两部分，这需要写入两个页，并且重写父页以更新这两个子页的引用。这是一个危险的操作，因为如果数据库在其中一部分页被写入后崩溃，这时的索引就被破坏了（比如出现了一个不是任何页面子页的孤页）。

为了使数据库适应崩溃，通常B树的实现会包含一个额外的数据结构在磁盘上：*预写入日志*（WAL，也被称为*重做日志*）。这是一个只附加文件，每个对B树的改动都必须先被写入到这个文件然后再应用到树中的页上。当数据库从崩溃恢复，这个日志用来把B树恢复到一个一致的状态。

就地更新页带来的另一个复杂问题是如果多个线程同时访问B树就需要谨慎的并发控制——否则线程访问树会得到不一致的结果。一般是通过*锁存器*（轻量锁）保护树的数据结构解决这个问题。基于日志结构的方式再这个方面要简单一些，因为所有的合并动作是在后台完成而不会影响对系统的查询同时不时地原子化交换新旧段。

#### B树的优化
由于B树已经存在了太长时间，这些年来有许多优化方法被开发出来也不奇怪。这里只提几个：
* 相比于覆盖页并出于崩溃恢复的原因维护WAL，某些数据库（比如LMDB）使用了写入时复制方案。被修改的页写入到了另外一个位置，同时在树中创建了一个新的父页并指向它。这种方式对并发控制也很有用，这会出现在“快照隔离与重复读取”一节。
* 可以通过不存储整个键而是键的缩写来节省页内的空间。尤其是在树的上层，键只需要提供足够作为边界的信息就可以了。把更多的键存在同一个页使得树有更大的分支因子，因而有更少的层。
* 一般地，也可以被放在磁盘上地任何位置；并没有要求相邻的键区段在磁盘上的位置也靠近。如果一个查询需要以排序顺序扫描很大一部分键，那么分页布局就很没效率了，因为每个页的读取可能都需要磁盘重新寻址。许多B树的实现因此都尝试在布局时让叶子页以顺序次序出现在磁盘上。然而随着树的增长维护这个次序会变难。相比之下，LSM树在合并时只重写存储段一次，对它们来说保持顺序的键在磁盘上的位置也靠近会更简单一些。
* 额外的指针被添加到了树中。举个例子，每个叶子页可以有指向左右兄弟页的引用，使得按顺序扫描键不需要再跳回父页。
* B树的变种，比如*分形树*，为了减少磁盘寻址借鉴了某些日志结构的理念（而这与分形无关）。

### 比较B树与LSM树
虽然B树的实现一般比LSM树的实现更成熟，然而由于性能特点LSM树仍然很有趣。根据经验，LSM树一般写入更快，而B树被认为是读取更快的。在LSM树上读取一般会更慢是因为它们需要在不同的压缩阶段检查好几个不同的数据结构以及SSTable。

然而，基础测试经常是没有定论的，并且对具体的工作量很敏感。为了得到合理的比较结果，你需要用特定的工作量来测试系统。在这一节中我们将简要讨论在测试存储引擎性能时值得考虑的几件事。

#### LSM树的优势
B树索引每一份数据的写入都至少发生两次：一次写入到预写入日志，另一次写入到树中的页本身（如果需要分页的话那就再多一次）。每次都要写入一整页也产生开销，哪怕只是页中的几个字节改变了。某些存储引擎甚至会复写同一个页两次从而避免在类似断电的情况下得到一个部分被更新的页。

日志结构的索引也会因为重复的压缩合并SSTable而重写好几次数据。这个效应——在整个数据库生命期内每一次写入导致了数次的磁盘写入——被称为*写入放大*。在固态硬盘上是一个特别值得关注的问题，因为每一个区块只有有限的复写次数。

在写入繁重的应用中，性能瓶颈也许是数据库写入磁盘的速率。在这种情况下，写入放大有直接的性能代价：存储引擎写磁盘的次数越多，在可用的磁盘带宽下每秒能处理的写入次数就越少。

此外，相比于B树，LSM树通常可以维持高写入吞吐量，部分是因为有些时候它们有着较低的写入放大系数（然而这取决于存储引擎配置与工作量），部分是因为它们顺序写入压缩的SSTable文件而不是复写书中的部分页。在机械硬盘上这个差别特别重要，因为顺序写入比随机写入快多了。

LSM树可以被压缩得更好，因而相比于B树常常在磁盘上生成更小的文件。B树搜索引擎由于碎片会留下一些未用空间：当页面拆分或者一个行不能在已有页放下时，页中的某些空间没有被占用。因为LSM树不是基于页的而且定期重写SSTable以移除碎片，所以有着较低的存储代价，尤其是在用了分层压缩之后。

在许多固态硬盘中，固件内部使用了日志结构算法把随机写入变成底层存储芯片的顺序写入，于是存储引擎写入模式的影响不太明显。然而，较低的写入放大以及较少的碎片仍然是固态硬盘的优势：以更紧凑的方式表示数据使得在可用的输入/输出带宽内允许有更多的读取和写入请求。

#### LSM树的缺点
日志结构存储的一个缺点是压缩过程有时影响正在进行的读取和写入的性能。虽然存储引擎尝试执行增量压缩并且不影响并行访问，但是磁盘的资源有限，于是很容易发生请求需要等待磁盘完成代价昂贵的压缩操作。对吞吐量以及平均响应时间的影响一般比较小，但是在高百分位（见“描述性能”一节）查询日志结构存储引擎的相应时间有些时候会相当得高，而换作B树的话就更好预测一些。

另一个压缩的问题伴着高写入吞吐量产生：磁盘有限的的写入带宽需要在初始写入（记日志以及清空内存表到磁盘上）与后台运行的压缩线程之间共享。当写入一个空的数据库时，所有的磁盘带宽都可以被用来进行初始写入，但是随着数据库越来越大，更多的带宽被用于压缩。

如果写入吞吐量很高而且压缩配置不仔细的话，是会发生压缩跟不上写入速率的情况的。在这样的情况下，磁盘上没有被合并的段数量持续增长，直至磁盘空间耗尽，而读取也会因为需要检查更多的文件而被拖慢。通常，基于SSTable地存储引擎不会限制写入地速率，哪怕压缩无法跟上，所以你需要明确地监控以检测这种情况。

B树的一个优势是在索引中每个键只存在于一个位置，而日志结构的存储引擎可以在多个不同段中有数个同一个键的拷贝。这一方面使得B树在想要提供强大的事务语义的数据库中很有吸引力：在血多关系型数据库中，事务隔离是通过在键的区间上使用锁实现的，而在B树索引中，这些锁可以直接用于树。在第七章中我们会更详细地讨论这一点。

在数据库架构中B树已经相当根深蒂固了，并且针对多种级别工作量提供了一致好的性能，所以短时间内它不可能消失。在新的数据存储中，日志结构索引变得越来越流行了。没有快速简便的法则来判断那种存储引擎更适合你的场景，所以这值得凭经验测试一下。

### 其它索引结构
截至目前我们只讨论了键值对索引，它与关系型模型中的*主键*索引。主键唯一标识了关系型表中的一行，或者是文档型数据库中的一篇文档，或者是图数据库中的一个顶点。数据库中的其它条目可以通过这个主键（或者ID）引用那一行/一篇文档/一个顶点，而索引是用来解析这样的引用的。

*副键*也是很常见的。在关系型数据库中，你可以通过命令`CREATE INDEX`在同一张表中创建数个副键，为了使连接执行得有效率它们通常是必要的。举个例子，在第二章中的图2-1中你很有可能为`user_id`一列创建副键从而可以在每张表中找到所有属于同一个用户的行。

副键是很容易从键值索引中构建的。主要的差别在于键不唯一；即有可能许多行（文档，顶点）有着相同的键。有两种方式可以解决这个问题：要么让所以索引中的每一个值都是一个匹配行标识符的列表（类似全文索引中的TODO: posting list），要么通过附加行标识符使每个键都是唯一的。任意一种方式，B树与日志结构的索引都可以被用作副索引。

#### 在索引中储存值
索引中的键是查询搜索的目标，但是值可以是下面二者之一：它可以是查询的实际的行（文档，顶点），或者是指向存在别处的行的引用。在后一种情况中，存储行的地方被称为堆文件，它在储存数据时没有特定的顺序（也许是只附加式的，也许保留着已删除行的记录方便稍后用新数据覆盖）。使用堆文件的方式很常见，因为它避免了多个副索引出现时数据重复的问题：每个索引只是引用了堆文件中的位置，而实际数据保存在另一个地方。

在不改变键的情况下更新值时，使用堆文件的方式是很有效率的：只要在尺寸上新的值不大于旧的值，条目可以就地复写。如果新的值在尺寸上更大的话情况就变得复杂了，因为有可能需要移到堆内一个有足够空间的新的位置。在这种情况下，要么所有的索引都需要更新以指向条目在堆中的新位置，要么需要在堆中旧的位置留下一个转发指针。

在某些情况里，从索引到堆文件的这个额外的动作对于读取来说是性能损失，所以把被索引指向的行直接存在索引中是可取的。这被称为*聚合索引*。举个例子，在MySQL的InnoDB存储引擎中，表的主键总是一个聚合索引，而副索引指向主键（而不是堆文件位置）。在SQL Server中，你可以为每张表指明一个聚合索引。

聚合索引（把所有数据储存在索引中）与非聚合索引（只是把数据的引用储存在索引中）的一个折中方案
是*覆盖索引*或是*带有包含列的索引*，它在索引中储存表的部分列。这使得响应某些查询时只需要访问索引（在这种情况下，我们说索引覆盖了查询）。