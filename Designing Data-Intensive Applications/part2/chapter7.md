# 第七章 事务

*有些作者声称一般性的二阶段提交支持起来代价太大，因为它带来了性能或者可用性问题。我们认为让应用程序开发者处理由于过度使用事务而出现瓶颈的性能问题会更好，而不应该在编程时完全没有事务可用。*

詹姆斯·科贝特等，*Spanner：谷歌的全球级分布式数据库*（2012）

---

数据系统的残酷现实世界中，很多东西都可能会出错：

* 数据库软件或是硬件可能在任何时间崩溃（包括在一次写入过程当中）。

* 应用程序可能在任何时间崩溃（包括在执行一系列操作的中途）。

* 网络中断可以意外地将应用程序从数据库断开，或者把一个数据库节点与另外一个节点断开。

* 数个客户端可以同时写入数据库，覆盖彼此提交的变更。

* 客户端会读取到没有意义的数据，因为先前只被部分地更新了。

* 客户端之间的竞争条件可以导致以外的bug。

为了可靠，系统需要处理这些故障并且保证它们不会导致整个系统灾难性的故障。然而，实现容错机制需要大量的工作。它需要对所有可能出错的事进行仔细的考虑，以及大量的测试从而保证解决方案是实际可行的。

数十年来，*事务*都是简化这些问题的首选机制。事务是应用程序把数个读写请求组合为一个逻辑单元的方式。从概念上讲，事务中的所有读写操作执行时被当作一次操作：整个事务要么成功（*提交*）要么失败（*取消*，*回滚*）。如果失败，应用可以安全地重试。有了事务，错误处理对应用程序来说变得相当简单，因为它不再需要担心部分失败——也就是（无论出于任何原因）某些操作成功了而某些失败了的情况。

如果你已经有了使用事务许多年的经验，它们看起来很浅显，但是我们不应该视它们为理所应当。事务不是一条自然规则；它们被发明出来是有目的的，就是为了在应用程序访问数据库时*简化编程模型*。通过使用事务，应用程序可以自如地忽略某些潜在的错误场景以及并发问题，因为数据库会替你处理它们（我们把这叫做*安全性保证*）。

不是所有的应用程序都需要用到事务，有些时候弱化事务性保证或是干脆完全抛弃它也是有好处的（比如，为了实现更高的性能或是更高的可用性）。某些安全属性是可以不通过事务实现的。

那么如何判断你需不需要事务呢？为了回答这个问题，我们首先需要理解到底事务可以提供什么样的安全性保证，以及随之而来的代价是什么。虽然乍一看事务简单明了，但是实际上有很多细微但是重要的细节在起作用。

在这一张，我们会研究许多会出错的事情，并且探索数据库用来针对这些问题的算法。我们会特别深入到并发控制的领域，讨论可能发生的各种种类的竞争条件以及数据库是如何实现比如*提交读*、*快照隔离*以及*可串行化*这样的隔离界别的。

这一章既适用于单节点数据库也适用于分布式数据库；在第八章我们将着重讨论只在分布式系统中出现的难题。

## 事务难以掌握的概念

今天，几乎所有的关系型数据库，以及一些非关系型数据库，都支持事务。它们中的绝大多数都沿用第一款SQL数据库，IBM System R在1975年引入的风格。虽然某些实现细节发生了变化，但是总体思路在40年内基本保持不变：MySQL、PostgreSQL、Oracle、SQL Server等对事务的支持与System R的惊人地相似。

在2000年代末，非关系型（NoSQL）数据库开始变得流行起来。它们旨在通过提供新的数据模型来改善关系性数据库的现状（见第二章），并且默认包含了复制（见第五章）以及分区（见第六章）。事务是这场运动的主要受害者：许多新一代的数据库完全放弃了事务，或者重新定义了这个次用以描述比先前所理解的要弱得多的一套保证。

围绕着对这一类新的分布式数据库的炒作，出现了一种普遍的看法，就是事务是站在可扩展性的对立面的，任何大型系统都必须放弃事务从而维持良好的性能和高可用性。另一方面，事务性保证有时被数据库厂商呈现为“有价值数据”的“严肃的应用程序”的基本要求。

事实可没有那么简单：就像任何其它技术设计选择一样，事务也有优点与限制。为了理解这些权衡，让我们看看事务可以提供的保证的细节——无论是在正常运行还是各种极端的（却也现实）的情况之下。

### ACID的含义

事务提供的安全性保证常常用知名的缩写ACID描述，它代表了*原子性*、*一致性*、*隔离性*和*持久性*。它由Theo Härder和Andreas Reuter于1983年创造，旨在为数据库中的容错机制建立精确的术语。

然而，在现实中，一个数据库的ACID实现与另外一个的实现并不等同。举个例子，之后我们会看到，关于*隔离*的含义就有许多歧义。在高层次的理念合理的，但是问题处在了细节上。今天，当一个系统号称“符合ACID”，实际上尚不清楚有哪些保证。ACID非常不幸地变成了一个商业用词。

（不满足ACID标准地系统有时被称作BASE，它代表了*基本可用*、*软状态*以及*最终一致性*，这比ACID的定义还要模糊。看起来BASE唯一合理的定义是“非ACID”；也就是说它可以意味着任何事。）

让我们深入研究原子性、一致性、隔离性和持久性的定义，因为这会让我们改进我们对事务的理解。

#### 原子性

通常，*原子性*指的是无法分割成更小部件的东西。这个词在计算机学不同的分支内指代的是相似但又稍微不同的东西。举个例子，在多线程编程中，如果一个线程执行原子操作，那意味着另外一个线程没有办法看到这个操作一半的结果。系统只会要么处于操作之前的状态或者操作之后的状态，而不是介于二者之间。

相比之下，在ACID的上下文中，原子性与并发*无关*。它也不是描述如果几个进程同时访问同一个数据会发生什么，因为那是字母*I*涵盖的，即*隔离性*（详见“隔离性”一节）。

相反的事，ACID原子性描述的是当客户端想要进行数个写入请求，但是在某些写入请求处理结束后发生了故障时——比如，进程崩溃，网络连接中断，磁盘写满，或者是违反了某些完整性约束——会发生什么。如果写入请求被归为一个原子事务，而事务由于故障不能完成（*提交*），那么事务会*中止*，而数据库必须丢弃或者复原任何迄今为止事务完成的写入。

没有原子性，如果在进行多次更改的过程中发生错误，那么很难知道哪些变化已经生效而哪些没有。应用可以再次尝试写入，但是写入同样变更两次带来的分线，会导致重复或是不正确的数据。原子性简化了这个问题：如果事务被中止，应用程序可以确定它没有改变任何东西，所以它可以安全地重试。

ACID原子性的定义特征是能够在出错的时候放弃事务并把所有事务的写入放弃。相比于*原子性*来说也许*可取消性*是一个更好的词，但是我们会继续用*原子性*，因为那才是常用词。

#### 一致性

*一致性*这个词已经被重用地太厉害了：

* 在第五章我们讨论了*复制一致性*，以及异步复制系统中出现的*最终一致性*问题（见“复制延迟的问题”一节）。

* *一致性哈希*是某些系统用于再平衡的分区方法。（见“一致性哈希”）。

* 在CAP定理中（见第九章），*一致性*一词是用来指*可线性化*（见“可线性化”）。

* 在ACID的语境下，*一致性*指的是一种应用程序特定的概念——数据库目前处于“良好的状态”。

* In the context of ACID, consistency refers to an application-specific notion of the database being in a “good state.”

同一个词用来表达至少四种不同的含义实在是不幸。

ACID一致性的理念是关于数据的某些描述（不变量）必须始终为真——举个例子，在账户系统中，所有账户的借贷必须平衡。如果事务起始时数据库满足那些不变量，而事务处理期间的任何写入都保持有效性，那么可以确定不变量还是满足的。

然而，这种一致性的理念取决于应用程序的不变量定义，正确定义事务使得它们保持一致性。这也是应用程序的责任。这不是数据库可以保证的：如果你写入了错误的数据破坏了这些不变量，数据库也不能阻止你。（某些特定种类的不变量可以由数据库检查，比如使用外键限制或是唯一性限制。然而通常，引用程序定义哪些数据合法哪些数据不合法——数据库只负责存储它们。）

原子性、隔离性和持久性是数据库的特性，而一致性（在ACID意义上）是应用程序特性。应用程序依赖数据库的原子性与隔离性来实现一致性，但是这不只取决于数据库。因而，字母C其实不属于ACID。

#### 隔离性

大多数数据库可以同时为多个客户端访问。如果它们读写的是数据库不同的部分那是没问题的，然而如果它们访问的是同一个数据库条目，就会碰到并发问题（竞争条件）。

图7-1是这种问题的一个简单示例。假设有两个客户端在持续递增存储在数据库中的计数器值。每个客户端需要读取当前的值，增加1，然后把新值写回去（假设数据库没有内建递增操作）。在图7-1中计数器应该从42增加到了44，因为发生了两次递增，但是由于竞争条件实际上只到了43。

ACID意义上的隔离性是指并发执行的事务相互之间是隔离的：它们不会互相干预彼此的操作。经典数据库教科书把隔离性形式化为可串行化性，意味着每个事务都可以假装自己是整个数据库上唯一正在执行的事务。数据库保证在事务提交之后，结果与它们顺序执行（一个接一个）的结果是一样的，哪怕实际上它们是并发执行的。

*图7-1. 两个并发递增计数器的客户端之间的竞争条件。*

然而在实际当中，可串行化的隔离性很少用到，因为它会带来性能损失。一些流行的数据库，比如Oracle 11g，根本没实现它。在Oracel数据库中有一个隔离界别叫做“可串行化”，但是实际上实现的是*快照隔离*，相比可串行化它是一个弱保证。我们会在“弱隔离级别”一节中探索快照隔离以及其它形式的隔离。

#### 持久性

数据库系统存在的目的是为了提供一个安全的储存数据的场所而不用担心丢失它。持久性承诺一旦事务提交成功，它写入的任何数据不会被遗忘，哪怕是发生了硬件故障或是数据库崩溃了。

在单节点数据库中，持久性通常意味着数据库已经被写入到诸如机械硬盘或是固态硬盘这样的非易失性存储中。它常常也涉及到预写入日志或是类似的东西（见“让B树变得可靠”一节），它在磁盘上的数据结构被破坏之后可以进行恢复。在一个复制数据库中，持续性意味着数据被成功地复制到了一定数量的节点。为了保证持久性，数据库必须等待所有的写入或是复制完成以后才能报告事务成功地提交了。

就如“可靠性”一节讨论到地，完美地持久性是不存在的：如果所有的硬盘与所有的备份同时被破坏了，显然没有什么数据库可以帮到你的了。

> **复制与持久性**
>
> 历史上，持久性是指写入到备份磁带上。之后被理解为写入到磁盘或是固态硬盘。最近，它开始意味着复制。那种实现更好呢？
>
> 事实是，没有什么是完美的：
>
> * 如果写入到了磁盘而设备死掉了，虽然数据没有丢失，但是直到你修复了设备或是把磁盘转移到另一台设备，数据都是无法访问的。复制了的系统则继续可用。
>
> * 相关性故障——断电或是由于特定输入导致所有节点崩溃的bug——可以立刻让所有副本下线（见“可靠性”一节），丢失任何还在内存中的数据。因此对于内存数据库来说，写入到磁盘仍然是相关的操作。
>
> * 在异步复制系统中，当领机离线时最新的写入请求都会丢失（见“处理节点离线”一节）。
>
> * 当电源突然被掐断，固态硬盘尤其表现出有时会违反它们本应提供的保证：哪怕`fsync`也无法保证工作正常。硬盘固件会有bug，就像任何其它类型的软件一样。
>
> * 存储引擎与文件系统实现之间的细微互动可以导致很难跟踪的bug，并会导致磁盘上的文件在崩溃后被破坏。 
>
> * 磁盘上的数据会慢慢地不知不觉地被破坏。如果数据被破坏已经有了一段时间，副本以及最新地备份也可能已经破坏了。在这种情况下，你需要尝试从一份老的备份中恢复数据。
>
> * 一份关于固态硬盘的研究发现在使用的头四年中产生至少一个坏块的可能性在30%到80%。机械硬盘相比于固态硬盘有更低的坏扇区率，但是有更高的完全故障率。
>
> * 如果SSD被断电，在几个礼拜之内就会开始丢失数据，而它取决于温度。
>
> 在实践中，没有任何一种技巧可以提供绝对的保证。只有各种降低风险的技巧，包括写入到磁盘，复制到远端设备以及备份——且它们可以也应该一起使用。与往常一样，用怀疑的眼光看待任何理论性的“保证”总是明智的。

### 单对象与多对象操作

概括一下，在ACID中，原子性与隔离性描述了在同一个事务中如果客户端发起了几个写入请求数据库应该做什么：

*原子性*

如果在执行一系列写入中途发生了错误，事务应当中止，而已经完成了的写入应当被丢弃。换句话说，数据库通过提供全有或者全无的保证，使你不用担心发生部分故障。

*隔离性*

并发执行的事务不应该互相干预彼此。举个例子，如果一个事务发起了数个写入请求，之后的另一个事务应该要么看到所有的写入，要么什么都没看到，但是不会看到其中一部分。

这些定义都假设你一次要修改数个对象（行，文档，记录）。如果好几份数据需要保持同步的话就经常需要这种*多对象事务*。图7-2展示了一个来自电子邮件应用的例子。为了显示用户未读邮件的数量，你会进行类似这样的查询：

```SQL
SELECT COUNT(*) FROM emails WHERE recipient_id = 2 AND unread_flag = true
```

然而，如果邮件太多你会发现这个查询太慢了，于是决定把未读消息的数量储存在一个单独的字段里（一种反规范化）。现在，每当收到一封新邮件，你也必须为未读计数器加一，而每当一封邮件被标记为已读，你也必须为唯独计数器减一。

在图7-2中，用户2经历了一个异常现象：邮箱列表显示有一封未读邮件，但是因为计数器加一还没发生，计数器显示零个未读邮件。隔离性可以通过保证用户2要么同时看到插入的邮件和更新了的计数器，要么什么都没有，而不是一个不一致的中间状态来预防这个问题。

*图7-2 违反隔离性：一个事务读到了另一个事务未提交的写入（“脏读”）。*

图7-3展示了对原子性的需要：如果在事务的过程中发生了错误，邮箱的内容与未读计数器就变得不同步了。在一个原子性事务中，如果对计数器的更新失败了，事务会终止且被插入的邮件会回滚。

*图7-3 原子性保证如果发生了错误任何来自事务之前的写入都会被撤销，以避免不一致的状态。*

多对象事务需要某些方式判定哪些读写操作属于同一个事务。在关系型数据库中，这通常是基于客户端到数据库服务器的TCP连接完成的：在任意特定的连接上，在`BEING TRANSACTION`与`COMMIT`之间的所有语句被认为属于同一个事务。

另一方面，许多非关系型数据库没有这种把操作分组到一起的方式。即使有多对象的API（举个例子，键值对存储可以有一次操作更新好几个键值的*多赋值*操作），但是这不意味它有事务的语义：这个命令有可能成功更新了某些键值而其它的失败了，使数据库出在了部分更新的状态。

#### 单对象写入

当单对象被更新时原子性与隔离性也适用。举个例子，假设你正在写入20KB大的JSON文档到数据库：

* 如果网络连接在发送10KB之后中断了，数据库会存储这无法解析的10KB JSON片段么？

* 如果数据库正在覆盖磁盘上的前一个值时断电了，你最终得到的是新旧值拼接在一起的值么？

* 如果另一个客户端在写入进行时读取那个文档，它看到的是一个部分更新的值么？

这些问题会令人难以置信地困惑，所以存储引擎几乎普遍提供单节点上的单对象（比如键值对）级别的原子性与隔离性。原子性在针对崩溃恢复时可以用日志实现（见“使B树稳定”一节），而隔离性可以用每个对象的锁来实现（同一时刻只允许一个线程访问对象）。

一些数据库孩提攻了更复杂的原子操作，比如增量操作，它消除了对如图7-1所示的读取-修改-写入循环的需求。类似的流行操作还有比较然后设置操作，它使得写入只有在值没有并发地被其它人修改才会发生（见“比较然后设置”一节）。

这些单对象操作很有用，因为它们可以防止好几个客户端并发尝试写入同一个对象时丢失更新的数据（见“防止丢失更新的数据”）。然而，它们不是通常意义上的事务。比较然后设置以及其它单对象操作由于商业目的已经被称为“轻量级事务”，甚至是“ACID”，但是这些术语都是误导人的。事务通常被理解为一种把在多个对象上的多个操作分组成单个操作单元的机制。

#### 多对象事务的需要

许多分布式数据存储放弃了多对象事务，因为在多分区上实现很困难，而且在某些需要非常高的可用性和性能的场景中会很碍事。然而，本质上没有任何东西阻拦分布式数据库使用事务，我们会在第九章讨论分布式事务的实现。

但是我们到底需不需要多对象事务呢？只用键值对数据模型和单对象操作是不是就可以实现任意的应用程序呢？

某些使用场景中单对象插入、更新、删除就足够了。然而在许多其他场景中我们需要协调对好几个不同对象的写入操作：

* 在关系型数据模型中，一张表中的一行经常有一个指向其它表中一行的外键。（类似的在图数据模型中，一个顶点有到其它顶点的边。）多对象事务允许你保证这些引用保持有效：当插入几条互相指向的记录，那外键必须正确而且保持更新，否则数据就没有意义了。

* 在文档型数据模型中，需要被一起更新的字段经常是在同一个文档里的，它们被视为单个对象——在更新单个文档是不需要多对象事务。然而，文档型数据库没有连接功能，也鼓励反规范化（见“今天关系型 vs 文档型数据库”一节）。当反规范化的信息需要更新时，比如图7-2中的例子，你需要一次更新好几个文档。事务在这种场景下非常有用，防止非规范化的数据不同步。

* 在有二级索引的数据库（几乎所有数据库都有，除了纯键值对存储）中，每当你更新一个值时索引也需要更新。从事务的角度上看这些索引是不同的数据库对象：举个例子，没有事务隔离性，一条记录出现在一个索引中但是不在另外一个索引中是可能的，因为对第二个索引的更新还没有发生。

这样的应用程序仍然可以不用事务实现。然而没有原子性的话错误处理变得非常复杂，没有隔离性则会导致并发问题。我们会在“弱隔离级别”一节讨论这些问题，并在第十二章探索其它替代方案。

#### 处理错误与中止

事务的一个核心功能是当错误发生时他可以被终止然后安全地重试。符合ACID地数据库是基于这样的哲学地：如果数据库面临原子性、隔离性或者持久性被破坏的危险，它宁愿放弃整个事务也不会允许它部分执行。

然而并不是所有的系统都遵从这个哲学。特别是，无领机复制的数据库（见“无领机复制”一节）更多的是基于“尽最大努力”信条，它可以被总结为“数据库会尽可能地执行语句，当遇到问题时，他不会撤销它已经完成了的结果”——所以从错误中恢复编程了应用程序的责任。

错误的发生难以避免，但是许多软件开发者更倾向于只考虑不出错的路径而不是复杂的错误处理。举个例子，非常流行的对象关系映射（ORM）框架，比如Rail的ActiveRecord以及Django，不会重试中止了的事务——错误通常引发堆栈异常，所以任何用户输入都被丢弃而用户获得一条错误信息。这很可惜，因为中止的全部意义就是为了可以安全的重试。

虽然重试中止了的事务是一个既简单又有效的错误处理机制，但是它并不完美：

* 如果事务实际成功了，但是服务器试图向服务器确认提交成功时网络瘫痪了（于是客户端认为它失败了），那么再次事务重试会导致它被执行两次——除非你已经有了另外一个应用级别的去重机制。

* 如果错误是因为负载过高，重试事务会导致问题更加严重，而不会更好。为了避免这种反馈循环，你可以限制重试的次数，使用指数回退，并且（可能的话）把负载过高相关错误的处理机制与其它的区别开来。

* 只有在发生了瞬态错误（比如说死锁、违反了隔离性、临时网络中断以及故障迁移）之后才值得重试；而发生了永久错误（比如违反了约束）之后重试是没有意义的。

* 如果事务还会导致数据库之外的副作用，即使事务被中止了这些副作用也许已经发生了。举个例子，如果你再发送电子邮件，你不会想要在重试事务时每次重新发送这封电子邮件。如果你想要确定好几个不同的系统要么一起提交或者中止了事务，两段提交可以有所帮助（我们会在“原子提交与两段提交（2PC）”一节中讨论它）。

* 如果客户端进程在重试时崩溃，它尝试写入到数据库的任何数据都会丢失。

## 弱隔离级别

如果两个事务没有访问同一份数据，那么它们是可以安全地并行执行地，因为互相没有依赖。并发问题（竞争条件）只有在一个事务读取另外一个事务正在修改地数据时，或者当两个事务尝试同时修改同一份数据才会出现。

并发bug很难通过测试找到，因为这样的bug只有在时间不对的时候才会触发。这样的时间问题非常少发生，通常很难重现。并发也非常难以推理，尤其是在大型应用中你都不知道还有什么地方的代码也在同时访问数据库。应用开发在一次只有一个用户就已经很困难了；有许多并发用户让它变得更加困难，因为任何一段数据都可能在任何时候意外地发生变化。

由于这个原因，数据库一直试图通过提供*事务隔离性*向应用程序开发者隐藏并发问题。理论上，隔离性可以通过假设没有并发正在发生简化问题：*可串行化*隔离性意味着数据库保证事务有着顺序执行同样的效果（即一次一个，没有任何并发情形）。

实践中，可惜隔离性没有那么简单。可串行化隔离性有性能损耗，而许多数据库不打算付出这样的代价。因此通常系统会使用稍弱的隔离级别，他能防范*某些*并发问题，但不是全部的。这些隔离级别更难理解，并且会导致很微妙的bug，但是事件中它们依然在使用。

由弱事务隔离性导致的并发性bug不只是理论性问题。它们曾经导致了大量资金的损失，财务审计师进行调查，以及客户数据被破坏。关于这些问题启示的一条受欢迎的评论是“处理财务数据时要使用ACID数据库！”——但是这没有意义。甚至许多流行的关系型数据库系统（通常被认为是“ACID”的）使用弱隔离，所以他们不一定会防止这些错误的发生。

与其盲目地依赖工具，我们需要提高对存在的各种并发问题的认识，以及如何防止它们。之后我们才能用手边的工具构建可靠和正确的应用。

在这一节我们会思考好几个实践中用到的弱（非串行化）的隔离级别，并详细讨论那种竞争条件可以或者不会发生，所以你可以决定那种级别是适合你的应用的。一旦完成，我们会详细讨论可串行化（见“可串行化”一节）。我们对隔离级别的讨论是随意的，并使用示例。如果你需要对其属性进行严格的定义和分析，你可以在学术文献中找到它们。

### 提交读

事务隔离性最基本的级别时提交读。它做出了两项保证：

1. 从数据库读取时，你只会看到已经被提交了的数据（没有脏读）。

2. 当写入到数据库时，你只会覆盖已经被提交了的数据（没有脏写）。接下来让我们详细讨论这两项保证吧。

#### 没有脏读

设想一个事务写入了某些数据到数据库，但是事务还没有提交或是中止。另外一个事务可以看到那些未提交的数据吗？如果是，那就叫做脏读。

运行在提交读隔离级别的事务必须防止脏读的发生。这意味着事务的任何写入只有在提交之后才对其它事务可见（之后它的所有写入立刻变得可见了）。这如图7-4所示，其中用户1设*x* = 3，但是用户2*获取x*任然得到的是旧值，2，因为用户1还没有提交。

*图7-4 没有脏读：用户2只有在用户1的事务提交之后才能看到x的新值。*

为什么防止脏读很有用，这里有几个理由：

* 如果事务需要更新好几个对象，脏读意味着另外一个事务会看到部分更新而其它的看不到。举个例子，在图7-2中，用户看到新的未读邮件但是没有看到更新了的计数器。这是邮件的脏读。看到数据库处在部分更新的状态对用户来说很困惑，也会导致其它事务做出错误的决定。

* 如果事务中止，任何它已经完成了的写入都需要回滚（如图7-3中那样）。如果数据库允许脏读，那意味着事务会看到之后被混滚了的数据——也就是那些从来没有被实际提交到数据库中的数据。关于后果的推理很快会变得让人费解。

#### 没有脏写

如果两个事务并发地尝试更新数据库中同一个对象时会发生什么？我们不知道写入的发生顺序，但是一般我们假设稍后的写入会覆盖稍早的写入。

然而，如果稍早的写入是还没有提交的事务的一部分，那么稍后的写入会覆盖未提交的数据么？这被称为脏写。运行在提交读隔离级别的事务必须防止脏写的发生，通常是通过延迟第二个写入直到第一个写入的事务提交或是中止完成的。

通过防止脏写，这个隔离级别避免了一些类型的并发问题：

* 如果事务更新数个对象，脏写回导致恶劣的后果。举个例子，考虑一下图7-5，它展示一个二手车销售网站，其中两个人Alice和Bob，同时尝试购买同一部车。买一部车需要两次数据库写入：网站列表需要被更新来体现买家，而销售发票需要发送给买家。在图7-5的场景中，销售结果授予了Bob（因为她对销售列表所在的表进行了最终写入），但是发票却寄给了Alice（因为她对发票所在的表进行了最终写入）。提交读防止了这样的事故。

* 然而，提交读不不能防止图7-1中的两个计数器增加一的竞争条件。在这种情况下，第二次写如发生在第一个事务提交之后，所以这不是脏写。但是它仍然不对，却是由于另外一个原因——在“防止丢失更新的数据”我们回讨论如何使这样的计数器加一是安全的。

*图7-5 有脏写的话，来自不同事务的冲突写入会混在一起。*

#### 实现提交读

提交读是非常受欢迎的隔离级别。在Oracle 11g、PostgreSQL、SQL Server 2012、MemSQL以及许多其它数据库中都是默认设置。

最常见的是，为了防止脏写数据库使用了行级别的锁：当事务要修改特定对象（行或是文档）时，它必须首先获取对象的锁。然后它必须占有那个锁直至事务被提交或是被中止。只有一个事务可以占有属于任何给定对象的锁；如果另一个事务想要写入同一个对象，它首先必须等待直到第一个事务提交或者中止，然后它才能获取锁并继续执行。在提交读模式（或是更强的隔离级别）这种锁动作是由数据库自动完成的。

我们如何防止脏读呢？一种选择是使用同样的锁，并且要求任何要读取对象的事务简短地获取锁然后在读取之后再次释放它。这保证了读取无法在对象有修改后未提交的值时发生（因为在那个时候锁正在被发起写入的事务占有）。

然而，实践中获取读的锁的方式工作得并不是很好，因为一次长时间写入得事务可以强制许多只读的事务等待它完成。这会危及只读事务的相应时间，对于运营来说是很恶劣的：程序一个部分的缓慢对完全不同的另一部分有连锁反应，只是因为在等待锁。

由于这个原因，许多数据库防止脏读使用了图7-4中展示的方法：对于每个被写入的对象，数据库同时记住旧值与当前占据写入锁的事务设置的新值。当事务进行的时候，任何其它读取对象的事务获得旧的值。只有在新的值被提交了以后所有事务才会切换到读取新的值。

### 快照隔离与可重复读

如果只看提交读隔离的表面，认为它已经做了事务需要的所有事情，可以原谅：它允许中止（原子性需要），它避免了读取事务不完整的结果，也防止并发写入混在一起。确实，这些都是有用的功能，同时相比于那些不提供事务的系统提供的保证要强多了。

然而，使用这种隔离级别还是有许多种方式遇到并发bug的。举个例子，图7-6展示了使用提交读会发生的问题。

*图7-6 阅读扭曲：Alice观察到数据库处于不一致的状态。*

假设Alice在银行存了1000块，拆分到两个账户，每个账户500块。现在一个事务分别从其中一个账户转100块到另外一个账户。如果事务被处理的同时她不幸看到了账户余额列表，她会看到一个账户余额处于收款到达之前的状态（余额500块），另一个账户处于付款之后的状态（新余额400块）。对于Alice来说现在看起来她的账户加起来只有900块了——好像那100块凭空消失了。

这个异常现象叫做不可重复读或者读偏：如果Alice在事务结束再一次读取账户1的余额，他会看到另一个值（600块）而不是前一次查询到的值。读偏在提交读隔离中被认为是可接受的：Alice看到的账户余额在她读取的时候确实是提交过了的。

> 注意
>
> 术语*偏*不幸地被赋予了新的含义：之前我们把它用在了热点的不平衡负载方面，而在这里它指的是时间上的异常现象。

在Alice的场景中，这不是一个持久的问题，因为她一定会在几秒钟后重新加载在线银行网站看到一致的账户余额。然而，一些场景无法忍受这种临时的不一致性：

*备份*

做备份需要制作整个数据库的拷贝，在大型数据库上会花几个小时的时间。在备份过程中，写入请求会继续发送到数据库。这样，最终备份的某部分包含的是旧版本的数据，而其它部分包含的是新版本的。如果你需要从这样的备份恢复，不一致性（比如消失了的钱）就变成了永久的。

*分析性查询与完整性检查*

有时，你会想要执行一个扫描大部分数据库的查询操作。这样的查询操作在分析时很常见（见“事务处理还是分析？”一节），亦或者是周期性的完整性检查一切是否正常（检测数据损坏）的一部分。如果这些查询在不同的时间点观察数据库的某些部分，就很可能会返回没有意义的结果。

*快照隔离*是这个问题最常见的解决方案。它的理念是每个事务读自数据库的一致性快照——也就是，事务看到所有事务开始时被提交到数据库的所有数据。即使数据之后被另一个事务修改了，从那个特定时间点起，每个事务只看到旧数据。

对于诸如备份和分析这种长时间运行的只读查询来说，快照隔离是一个好事情。如果查询执行过程中数据也在同时变化，那么推理查询的意义就很难了。当事务可以看到数据库的一致性快照。冻结在某个特定时间点，就很容易理解了。快照隔离是个受欢迎的功能：PostgreSQL、使用InnoDB存储引擎的MySQL、Oracle、SQL Server以及其它数据库都支持。

#### 实现快照隔离

与提交读隔离类似，快照隔离的实现通常使用写入锁来防止脏写（见“实现提交读”一节），这意味着发起写入的事务可以阻塞其它写入同一个对象的事务进展。然而，读不需要任何锁。从性能的角度看，快照隔离的核心原则是*读者从来不阻塞写者，写者也从来不阻塞读者*。这使得数据库在处理长时间运行在一致性快照上的读取查询的同时也可以按往常一样处理写入请求，而两者之间不会争夺锁。

要实现快照隔离，数据库使用了图7-4中我们看到的防止脏读的概括机制。数据库必须能保存对象几份不同的提交版本，因为众多正在进行的事务需要看到不同时间点的数据库状态。因为它同时维护了对象的几个版本，这种技巧叫做*多版本并发控制*（MVCC）。

如果数据库只需要提供提交读隔离，而不是快照隔离，那么保存两个版本的对象就足够了：提交了的版本与覆盖了但还没有提交的版本。然而支持快照隔离的存储引擎通常也为提交读隔离等级使用MVCC。一种典型的做法是提交读为每个查询请求使用单独的快照，而快照隔离为整个事务使用同一个快照。

图7-7展示了在PostgreSQL中基于MVCC的快照隔离是如何实现的（其它的实现也是类似的）。当事务开始之后，被分配了唯一的、编号持续上升的事务ID（`txid`）。每当事务写入任何东西到数据库的时候，它写入的数据会标记上写者的事务ID。

*图7-7 用多版本对象实现快照隔离。*

表中的每一行都有`created_by`字段，包含了插入这行到表里的事务ID。此外，每一行也有一个`deleted_by`字段，初始值为空。如果事务删除了一行，这行实际上并没有从数据库中删除，而只是通过设置`deleted_by`字段为请求删除的事务ID从而标记删除。晚些时候，当确定没有事务再访问被删除的数据时，数据库中的垃圾回收进程删除所有被标记要删除的行并回收空间。

一次更新请求再内部被转换为一次删除请求和一次创建请求。举个例子，在图7-7中，事务13从账户2中扣除了100块，余额从500块变成了400块。账户表现在实际上包含两个账户2的行：一行余额500块被事务13标记为删除，一行余额400块被事务13创建。

#### 观察一致性快照的可见性规则

当事务读取数据库时，会用事务ID决定哪些对象它能看到而哪些是不可见的。通过小心地定义可见性规则，数据库可以向应用呈现数据库的一致性快照。工作方法如下：

1. 在每个事务开始的时候，数据库构建一个那个时候所有其它正在进行的（还没有提交或中止的）事务列表。这些事务发起的任何写入都被忽略掉，哪怕之后事务提交了。

2. 任何中止了的事务发起的写入被忽略掉。

3. 有着更新的事务ID的（即在当前事务启动之后启动的）的事务发起的任何写入被忽略掉，而不管这些事务有没有提交。

4. 其它所有的写入对应用的查询都可见。

这些规则同时使用与对象的创建与删除。在图7-7中，当事务12读了账户2，因为500块余额的删除动作是事务13发起的，于是它看到了500块余额（根据规则3，事务12看不到事务13发起的删除动作），而400块余额的创建也还不可见（由于同样的规则）。换句话说，如果下列两个条件都为真那么对象是可见的：

* 当读者的事务启动时，事务构建的对象已经提交了。

* 对象没有删除的标记，或者如果有，读者事务启动时请求删除的事务还没有提交。

长时间运行的事务会持续使用一个快照很长时间，继续读取那些（从其它事务的角度看）很早就被覆盖或者是删除了的值。通过从来不就地更新值而是每当值发生变化就创建一个新版本，数据库在产生很小开销的同时提供了一致性快照。

#### 索引与快照隔离

在一个多版本数据库中索引是如何工作的呢？一种选择是让索引直接指向所有版本的对象然后要求索引请求过滤掉任何对当前事务不可见的对象版本。当垃圾处理移除了对任何事务都不可见的旧对象版本时，对应的索引条目也会被删除。

实践中，许多实现细节决定了多版本并发控制的性能。举个例子，如果同一个对象的不同版本可以放在同一个页面，PostgreSQL为了避免索引更新而进行了优化。另外一个方法用在了CouchDB、Datomic以及LMDB。虽然他们也使用B树（见“B树”一节），但是当页面更新时它们使用不会覆盖树中的页的只附加/写入时复制变种，而是构建每个被修改的页的新拷贝。父页，最高到树的根节点，被拷贝并更新到指向新版本的子页面。任何没有收到写入影响的页面不需要被拷贝，而是保持不变。

对于只附加的B树，每一个写入事务（或者是一批事务）构建一个新的B树根节点，其中一个特定的根节点是数据库创建时的一致性快照。由于之后的写入不会修改已有的B树，所以没有必要按照事务ID过滤对象；它们只会创建新的树的根节点。然而，这种方式也需要一个北京进程用来压缩与垃圾回收。

#### 可重复读与命名困惑

快照隔离是有用的隔离级别，特别是对于只读事务。然而，许多实现了它的数据库用不同的名字称呼它。在Oracle里叫做*可串行化*，在PostgreSQL和MySQL里叫做*可重复读*。

这种命名困惑的原因是SQL标准并没有快照隔离的概念，标准是基于System R 1975年对隔离级别的定义而那个时候快找个里还没有被发明出来。然而，他定义了可重复读，从表面上看它与快照隔离非常类似。PostgreSQL以及MySQL把它们的快找个里叫做可重复读是因为它满足了标准的要求，于是它们可以声明是符合标准的。

不幸的是，SQL标准对于隔离级别的定义是有缺陷的——它有歧义，也不精确，更不是标准应有的那样与实现无关。虽然好几个数据库实现了可重复读，尽管表面上是标准化的可它们实际提供的保证有很大的差别。学术文献中对可重复读已经有了正式的定义，但是绝大部分实现不满足这个正式定义。而最糟糕的是，IBM DB2用“可重复读”来指代可串行化。

所以，没有人真的知道可重复读到底是什么意思。

### 防止丢失更新的数据

截至目前我们讨论了的提交读与快照隔离主要是关于在出现并发写入时只读事务可以看到什么的保证。我们几乎完全忽略了两个并发写入的事务问题——我们只讨论了脏写（见“不脏写”），一种可能发生的特定类型写入-写入冲突。

还有好几种其它有意思的并发写入事务之间的冲突。其中最有名的问题时丢失更新数据问题，如图7-1中两个并发计数器加一的例子所示。

如果应用从数据库读取某些值，修改了它，然后把修改后的值写回去（一个读取-修改-写入周期）丢失更新问题就会发生。如果两个事务同时这样做，其中的一个修改就会丢失，因为第二个写入没有包括第一个的修改结果。（优势我们说稍后的写入覆盖了稍前的写入。）这个模式会发生在好几个不同的场景中：

* 计数器加一或者更新账户余额（需要读取当前的值，计算新值，然后把更新了的值写回去）

* 对复杂的值进行本地改动，比如，添加一个元素到JSON文档中的列表中（需要解析分档，做变动，然后把修改后的文档写回去）

* 两个用户同时修改一个维基页面，每个用户通过发送整个页面内容到服务器来保存他们的修改，覆盖任何当前在数据库中的内容

因为这是一个常见问题，所以开发出了各种解决方案。

#### 原子写操作

许多数据库提供了原子更新操作，有了它就不再需要在应用程序代码中实现读取-修改-写入周期。如果你的代码可以用这些操作表达，这通常是最佳方案。比如说，下边的指令在大多数关系型数据库中是并发安全的：

```SQL
UPDATE counters SET value = value + 1 WHERE key = 'foo';
```

类似地，文档型数据库，例如MongoDB，提供对JSON文档的一部分进行本地修改的原子操作，而Redis提供修改诸如优先队列这样的数据结构的原子操作。并不是所有的写入操作都可以简单地用原子操作表达——比如，对一个维基页面的更新牵扯到任意的文字编辑——但是在可以使用原子操作的情况下，他们通常都是最佳选择。

原子操作的实现一般是通过获取对象的独占锁，当对象被读取时其它事务无法读取直至更新被应用。这个技巧被称为*游标稳定性*。另一种选择是强制所有的原子操作执行在单线程上。

不幸的是，对象关系映射框架使得很容易意外写出执行不安全的读取-修改-写入周期的代码而不是使用数据库提供的原子操作。如果你知道你在做什么这不是个问题，但是它很可能是通过测试也难以发现的微妙错误的来源。

#### 显式锁定

如果数据库内建的原子操作没有提供必要的功能，那么防止丢失更新的另一种选择是应用显式地锁定即将被更新的对象。然后应用而可以执行读取-修改-写入周期，如果任何其它事务尝试并发地读取同一个对象，会被强制等待直到第一个读取-修改-写入周期完成。

举个例子，设想一个多人游戏，游戏中好几个玩家都可以并发移动同一个人物。在这种情况下，一个原子操作已经不够用了，因为应用程序也需要保证玩家的行动遵守游戏的规则，设计到某些不能实现为数据库查询的逻辑。而是你可以使用锁来防止两个玩家同时移动同一个对象，如示例7-1所示。

*Example 7-1 显式锁定行，防止丢失更新的数据*

---

```SQL
BEGIN TRANSACTION;

SELECT * FROM figures
    WHERE name = 'robot' AND game_id = 222
    FOR UPDATE; ➊

-- Check whether move is valid, then update the position
-- of the piece that was returned by the previous SELECT.
UPDATE figures SET position = 'c4' WHERE id = 1234;

COMMIT;
```

➊ `FOR UPDATE`语句表明数据库应该为这个查询返回的所有行加锁。

这是可行的，但是要做对，你需要仔细地考虑应用程序的逻辑。代码的某处忘记加上必要的锁是很轻易就发生的，并因此导致了竞争条件。

#### 自动检测丢失的更新数据

原子操作与锁都是通过强制读取-修改-写入周期按顺序发生来防止丢失更新数据的方式。一种替代方式是允许他们并行执行，当事务管理器检测到丢失了更新数据，中止事务并强制它重启读取-修改-写入周期。

这种方式的优势是结合快照隔离数据库可以高效地执行这种检测。确实，PostgreSQL的可重复读，Oracle的可串行化以及SQL Server的快照隔离级别都会自动检测何时丢失了更新数据并中止出错的事务。然而，MySQL/InnoDB的可重复读并不检测丢失的更新。有些作者争辩说数据库必须防止丢失更新才算提供了快照隔离，所以在这种定义之下MySQL就不算提供了快照隔离。

丢失更新的检测是一个伟大的功能，因为它不要求应用程序代码使用任何特殊的数据库功能——你会忘记使用锁或者原子操作并因而引入bug，但是丢失更新的检测是自动发生的因而不太容易出错。

#### 比较然后设置

在不提供事务的数据库中，有时你会看到一个原子性的比较然后设置操作（之前在“单对象写入”一节中提到过）。这个操作的目的是防止丢失更新，如果值自从上一次读取都一致没变的情况下才允许更新。如果当前的值与先前读到的值不匹配，那么更新是无效的，于是必须重试读取-修改-写入周期。

举个例子，为了防止两个用户同时更新同一个维基页面，你可以尝试下面的语句，期待更新只有在页面内容自从用户开始编辑时都没有更新才发生：

```SQL
-- This may or may not be safe, depending on the database implementation 
UPDATE wiki_pages SET content = 'new content'
    WHERE id = 1234 AND content = 'old content';
```

如果内容已经改变并且与`'old content'`不同，这次更新动作没有任何效果，所以你需要检查更新是否生效，如果必要还要重试。然而，如果数据库允许`WHERE`语句读取旧的快照，这个语句也没办法阻止丢失更新，毕竟即使另一个并发写入正在进行条件也为真。必须检查数据库的比较然后设置操作是否安全之后才能决定是否依赖它。

#### 解决冲突与复制

在复制的数据库中（见第五章），防止丢失更新发生在另外一个维度：既然它们在数个节点上有数据的多份拷贝，且数据很有可能在不同的节点上同时被修改，于是需要采取一些额外步骤来防止丢失更新。

锁与比较然后设置操作都假设有一份最新的数据拷贝。然而，支持多主机复制或者无主机复制的数据库通常允许好几个写入同时发生并且异步地复制它们，所以它们无法保证有一份最新的数据拷贝。因此，基于锁或者比较然后设置操作的技术在这里都不适用。（我们会在“可线性化”一节更详细地重新审视这个问题。）

相反的是，如在“检测并发写入”一节中讨论到的，在这种复制的数据库中一种常见的方法是允许并发写入创建某个值的好几个互相冲突的版本（也叫做sibling），事后使用应用程序代码或者是特殊的数据结构来解决和合并这些版本的数据。

原子操作在复制的数据库可以工作得很好，特别是在它们满足交换律（即，你可以在不同的副本上以不同的顺序应用它们，并且仍然得到相同的结果）的情况下。举个例子，计数器加一或者添加元素到集合是都是可交换操作。这就是Riak 2.0版本数据类型背后的理念，它可以防止副本之间丢失更新。当一个之同时被几个不同的客户端更新时，Riak自动合并所有的更新使得没有更新会被丢掉。

另一方面，以最后一次写入为准（LWW）的冲突解决方法很容易丢失更新，正如在“以最后一次写入为准（丢弃并发写入）”一节讨论到的。不幸的是，LWW时许多复制的数据库的默认选项。

### 写偏与幻影

在之前几节我们认识了脏写与丢失了的更新——两种在不同事物同时尝试写入同一个对象时会发生的竞争条件。为了防止数据被破坏，就需要避免竞争条件的发生——要么数据库自动完成，要么使用锁或者原子写入操作这种手动保护。

然而，它们并不是同时写入时可能发生的最后两种竞争条件。在这一节我们会看到一些更微妙的冲突示例。

首先，设想这样的例子：你正在为医生们管理他们在医院随叫随到的轮班写应用程序。医院通常尝试同一时间有好几名医生随时待命，但是它必须至少有一名医生待命。医生们可以放弃他们的轮班（比如，他们自己病了），只要至少有一位同事在那个班次中继续工作。

现在假设Alice和Bob是两个特定轮班的值班医生。两个人都感觉不舒服，所以他们都决定请假。不幸的是，他们大约同时点击了去值的按钮。接下来发生的事如图7-8所示。

*图7-8 写偏造成应用程序bug的例子*

在每一个事务中，你的应用首先检查两个或者更多医生目前当值；如果是，它假设一个医生去值是安全的。由于数据库使用的是快照隔离，两个检查都返回2，所以两个事务都进入到下一个阶段。Alice更新了自己的记录以去值，而Bob也这样做了。两个事务提交，于是现在没有医生在值了。违反了必须有至少一个医生在值的需求。

#### 写偏的特征

这种异常现象叫做写偏。它既不是脏写也不是丢失更新，因为两个事务在更新两个不同的对象（分别是Alice的与Bob的当值记录）。并不是很明显地能看出发生了冲突，但是它确实是一个竞争条件：如果两个事务先后执行，第二个医生就不能去值。异常现象只可能在同时运行的时候才会发生。

你可以把写偏认为是一般化的丢失更新问题。如果两个事务读取同样的几个对象，然后更新其中的一部分（不同的事务更新不同的对象）就会发生写偏。在不同事务更新同一个对象这个特例中，你就会得到脏写或者丢失更新的异常（具体哪一种这取决于发生的时间）。

我们知道有许多种不同的方式防止出现丢失更新。对于写偏，我们的选择受到更多限制：

* 原子单对象操作没有用，因为牵扯多个对象。

* 在许多快照隔离实现中的丢失更新自动检测机制不幸地也没有用：在PostgreSQL的可重复读、MySQL/InnoDB的可重复读、Oracle的可串行化以及SQL Server的快照隔离等级中写偏都不是自动检测的。自动防止写偏需要真正的可串行化隔离（见“可串行化”一节）。

* 有些数据库允许你配置约束条件，之后数据库会强制应用（比如，唯一性、外键约束、或者对某个特定值的限制）。然而，为了描述至少有一个医生当值，你需要牵扯到多个对象的约束条件。绝大多数数据库没有对这样的约束条件有内建支持，但是你可以用触发器或是物化视图来实现，具体取决于是那种数据库。

* 如果没有办法用可串行化隔离等级，那么在这种情况下次好的选择大概是显式地锁定事务依赖地行。在医生例子里，你可以这样写:

```SQL
BEGIN TRANSACTION;

SELECT * FROM doctors
  WHERE on_call = true
  AND shift_id = 1234 FOR UPDATE; ➊

UPDATE doctors
  SET on_call = false
  WHERE name = 'Alice'
  AND shift_id = 1234;

COMMIT;
```

➊ 跟之前一样, `FOR UPDATE`告诉数据库要锁定这条查询请求返回的所有行。

#### 更多写偏的例子

写偏起先看起来是一个深奥的问题，可一旦你知道了它，你会注意到其它可能它会发生的场景。这里还有几个例子：

*会议室预订系统*

假设你要强制同时对同一个会议室不能有两个预订记录。当某人要进行预订时，首先检查是否有冲突的预订（即，同一个会议室时间重叠的预订），如果没有，创建会议（见示例7-2）。

*示例7-2 会议预定系统尝试避免双重预订（在快照隔离下不安全）*

```SQL
BEGIN TRANSACTION;

-- Check for any existing bookings that overlap with the period of noon-1pm
SELECT COUNT(*) FROM bookings
  WHERE room_id = 123 AND
  end_time > '2015-01-01 12: 00' AND start_time < '2015-01-01 13: 00';

-- If the previous query returned zero:
INSERT INTO bookings
  (room_id, start_time, end_time, user_id)
  VALUES (123, '2015-01-01 12: 00', '2015-01-01 13: 00', 666);

COMMIT;
```

不幸的是，快照隔离无法防止另一个用户同时插入冲突的会议预订。为了保证没有预定冲突，你再一次需要可串行化隔离。

*多人游戏*

在示例7-1中，我们用锁防止出现丢失更新的问题（也就是，保证两个玩家无法同时移动同一个人物）。然而，锁没有办法防止玩家把两个不同任务移动到棋盘的同一个位置，或者是其它可能违反游戏规则的操作。取决于你强制应用的规则类型，你也许可以使用唯一性约束条件，除此之外很容易发生写偏。

*选择用户名*

在每个用户都有独特用户名的网站上，两个用户会尝试同时用同一个用户名创建账户。你会使用事务来检查名字是否被占用了，如果没有，则用那个名字创建账户。然而，与前一个例子类似，在快照隔离级别这是不安全的。幸运的是，唯一性限制就可以简单地解决问题（第二个事务尝试注册那个用户名会因为违反了约束条件而中止）。

*防止双重花费*

一个允许用户花费金钱或是点数的服务需要检查用户的花费不能超过他们拥有的额度。允许你会通过插入临时的支出项目到用户账户，列出所有账户内项目，然后检查和是否为正来实现它。有了写偏的话，两个支出项目会被同时插入从而导致余额变成负数，但是没有事务意识到对方的存在。

#### 导致写偏的幻影

所有这些示例都有类似的模式：

1. `SELECT`查询通过搜索符合某些搜索条件（至少两个医生当值，那个房间那个时候没有被预订，棋盘上的那个位置还没有其它任务在那，用户名没有被占用，账户中还有钱）的行检查某些需求是否被满足。

2. 应用程序代码决定如何继续取决于第一步查询的结果（也许操作继续，也许向用户报告错误并且中止）。

3. 如果应用程序代码决定继续，它向数据库发起写入请求（`INSERT`，`UPDATE`或者`DELETE`）并提交事务。

    这次写入的效果改变了第二步决定的前提。换句话说，如果在提交写入之后重复第一步的`SELECT`查询，你会得到不一样的结果，因为写入改变了符合搜索条件的行的集合（现在少了一个当值的一生，那个时间会议室被预定了，棋盘上的位置现在被刚刚移动了的人物占据了，用户名现在被占用了，账户里的钱变少了）。

这些步骤可以以不同的顺序发生。举个例子，你可以首先发起写入请求，然后是`SELECT`查询，最后决定到底是中止还是基于查询的结果进行提交。

在医生当值的例子中，在第三步被修改的行是第一步返回的行中的一个，所以我们可以通过锁定第一步中的行（`SELECT FOR UPDATE`）使得事务安全并避免写偏。然而，其它四个示例是不同的：它们检查缺失的符合某些搜索条件的行，然后写入请求添加符合同一个条件的一行。如果在第一步中的查询没有返回任何行，`SELECT FOR UPDATE`也就没有办法加锁。

这种一个事务中的写入改变了另一个事务中的搜索查询结果，叫做幻影。快照隔离避免了只读查询中的幻影，但是在我们讨论的例子的读写事务中，幻影会导致特别棘手的写偏情况。

#### 物化冲突

如果幻影的问题在于没有对象可供我们加锁，也许我们可以人为地将一个所对象引入数据库？

举个例子，在会议室预订场景中你可以设想创建一个时间槽与房间的表。表中的每一行代表特定时间段（假设是15分钟）的特定房间。你可以创建，比如接下来六个月的所有房间与时间段可能的组合。

现在一个事务想要创建预订可以锁定期望的房间与时间段对应的表中的行。获取锁之后，可以如先前一样检查重叠的预订并插入一个新预订。值得注意的是没有用到额外的表来储存预订的信息——它只是一个集合的锁，用来防止并发修改时同时预订同一个房间以及时间段。

这种方式叫做物化冲突，因为它把幻影变成了数据库中已存在的行集合上的锁冲突。然而不幸的是找出如何物化冲突是很难的也很容易出错，而让并发控制机制出现在应用程序数据模型中也是很难看的。由于这些原因，物化冲突应当被视为没有任何其它可替代项时的最后一搏。绝大多数场景下可串行化隔离级别都是更优先的选择。

## 可串行化

在这一章中我们看到好几个容易导致竞争条件的事务的例子。一些竞争条件可以通过提交读和快照隔离级别防止，而其它的则不能。我们遇到了一些特别棘手的例子，有写偏和幻影。令人悲伤的是：

* 隔离级别很难理解，并且不同的数据库实现不一致（比如，“可重复读”的含义彼此非常不同）。

* 看看你的应用程序代码，很难讲在特定隔离级别执行时是否安全——在大型应用中尤其明显，你也许都没有注意到所有的事都在并发发生着。

* 没有好的工具帮助我们检测竞争条件。原则上，静态分析可以有所帮助，但是研究性质的技术还没有找到实际应用的办法。测试并发问题也很难，因为它们通常是不确定的——问题只有在时机不对的时候才会发生。

这可不是新问题——从1970年代开始，弱隔离级别刚刚引入的时候就是这样了。一直以来学者的答案都很简单：使用可序列化隔离。

可序列化隔离常常被认为是最强的隔离级别。它保证即使事务并行执行，最终结果与没有任何并发地、顺序地、一次执行一个事务的结果是完全一致的。因此，数据库保证如果事务独立执行时表现正确，并发执行时依然是正确的——换句话说，数据库可以防止所有可能的竞争条件。

但是如果可串行化隔离比其它弱隔离级别好那么多，那么为什么不是所有人都在用它呢？为了回答这个问题，我们需要看一下实现可串行化的选择，以及它们的表现如何。绝大多数提供可串行化的数据库使用下边三种技术之一，我们会在本章余下部分探索它们：

* 如字面意思，顺序执行事务（见“实际的顺序执行”一节）

* 两阶段锁（见“两阶段锁（2PL）”一节），几十年来它是唯一可行的选择。

* 比如可串行化的快照隔离这样的乐观并发控制（见“可串行化的快照隔离（SSI）”）

目前，我们主要基于单节点数据库的上下文讨论这些技术；在第九章我们将研究如何将它们推广到涉及分布式系统中多个节点的事务。

### 实际的顺序执行

避免并发问题最简单的方式就是完全放弃并发：在单线程上，以顺序的方式，一次只执行一个事务。通过这样做，我们完全回避了检测与防止事务间冲突的问题：由此产生的隔离按照定义是可序列化的。

虽然这看起来是很明显的点子，但是数据库设计师直到最近——2007年左右——才认为单线程循环执行事务是可行的。如果在过去30年里多线程并发被认为是获得良好性能的基础的话，那事什么使得单线程执行成为可能呢？

两个方面的发展引起了这样的反思：

* RAM变得最够的便宜以至于现在在许多场景中把整个活跃数据库保存在内存中是可行的了（见“把所有东西放在内存中”一节）。当事务需要访问的数据都在内存中的时候，相比于必须等待数据从磁盘加载现在事务可以执行得更快了。

* 数据库设计师意识到OLTP事务常常很短，且只发起很少量的读写请求（见“事务处理还是分析？”一节）。相比之下，长时间运行的分析查询通常是只读的，所以它们可以执行在一个一致性快照上（使用快照隔离）而不是串行的执行循环中。

串行执行事务的方式在VoltDB/H-Store、Redis以及Datomic中实现了。为单线程执行设计的系统有的时候要比支持并发的系统性能要好，因为它避免了协调锁定的消耗。然而，它的吞吐量受到单个CPU核心的限制。为了最大限度地发挥单线程的能力，构造的事务与它们的传统形式稍稍有些差别。

#### 把事务封装在存储过程中

在数据库的早期，数据库的目的是数据库事务可以包含整个用户活动流程。举个例子，预订飞机票是一个多阶段的过程（搜索航线，票价，已经可选的座位；确定行程；预订行程中每一个航段的座位；输入乘客信息；付款）。数据库设计师以为如果整个过程是一个事务，于是可以原子提交的话就很棒了。

然而，人们做决定做回应都是很慢的。如果数据库事务需要等待用户的输入，那么数据库可能需要支持海量并发事务，其中绝大部分是空闲的。绝大部分数据库无法有效的应对这个问题，所以几乎所有的OLTP应用程序保持事务简短，避免在事务中等待用户互动。在网络上，这意味着事务实在同一个HTTP请求中提交的——一个事务不会跨越多个请求。一个新的HTTP请求开启一个新的事务。

即使人为因素被排除到关键路径意外，事务仍然继续以交互式的客户端/服务器端的方式执行，每次一条语句。应用程序发起查询，读取结果，也许根据第一次查询的结果会再发起另一次查询，等等等等。查询请求与结果在应用程序代码（运行在单个设备上）与数据库服务器（在另外一台设备上）之间来回交换。

在这种交互式的事务中，许多的时间花费在应用程序与数据库之间的网络通信上。如果在数据库中不允许并发而一次只可以处理一个事务，吞吐量会非常恐怖，因为数据库绝大部分时间都在等待应用程序发起当前事务的下一个查询。在这种数据库中，同时处理数个事务从而获得合理的性能是必要的。

由于这个原因，有着单线程串行事务处理的系统不允许交互式的多语句事务。而是，应用程序必须首先向数据库提交整个事务代码，作为一个存储过程。这些方法之间的差别如图7-9展示。如果交易所需的所有数据都在内存中，存储过程会执行得很快，无需等待任何网络或者磁盘I/O。

*图7-9 交互式事务与存储过程间得差别（使用了图7-8中的示例事务）。*

#### 存储过程的优点与缺点

存储过程在关系型数据库中已经存在了一段时间了，自1999年它就成为了SQL标准（SQL/PSM）的一部分。由于各种原因它的声誉有些不好：

* 每个数据库厂商都有自己的存储过程语言（Oracle有PL/SQL，SQL Server有T-SQL，PostgreSQL有PL/pgSQL，等等）。这些语言并没有跟上通用编程语言的发展，所以从今天的角度看它们相当丑陋和古老，并且缺乏大多数编程语言都有的开发库生态系统。

* 运行在数据库中的代码很难管理：相比于应用服务器，它更难调试，更难以保持版本控制和部署，更难以测试，并且很难与指标收集系统整合进行监控。

* 相比于应用服务器数据库通常对性能更加敏感，因为单个数据库实例经常被多个应用服务器共享。相比于应用服务器中写得不好代码，数据库中写得不好的存储过程（比如，占用了大量内存或是CPU时间）会造成更大的麻烦。

然而，这些问题都可以克服。现代的存储过程实现已经抛弃了PL/SQL转而使用已有的通用编程语言：VoltDB使用Java或者Groovy，Datomic使用Java或者Clojure，而Redis使用Lua。有了存储过程和在内存中的数据，在单个线程上执行所有的事务变为可行了。由于它们不必等待I/O而且避免了其它并发控制机制的消耗，所有它们可以在单线程上实现相当不错的吞吐量。

VoltDB还把存储过程用于复制：不再把事务的写入请求从一个节点拷贝到另外一个节点，而是在每个副本上执行同样的存储过程。VoltDB因此要求存储过程是确定性的（当运行在不同节点上时，它们必须产生同样的结果）。举个例子来说如果事务需要使用同样的日期和时间，它必须通过特定的确定性API来完成。

#### 串行执行的小结

事务的串行执行已成为在特定限制条件下实现可串行化隔离的可行方法：

* 每个事务必须很小且执行得很快，因为只需要一个缓慢的事务就可以阻塞所有事务的处理。

* 它仅限于活动数据集可以放入内存的使用场景。很少被访问到的数据可能会被移动到磁盘，一旦需要在单线程事务中访问它，系统将会变得非常缓慢。

* 写入吞吐量必须足够低才能在单个CPU核心上处理，否则事务需要进行分区而不需要跨分区协调。

* 跨分区的事务是可能的，但是对它们的使用程度有很大的限制。

### 两阶段锁 (2PL)

在过去三十年里，在数据库中可串行化只有一种广泛使用的算法：两阶段锁。

> **2PL不是2PC** 
>
> 值得注意的是虽然两阶段锁（2PL）听起来与两阶段提交很像，但是它们是完全不一样的东西。我们会在第九章讨论2PC。

我们之前看到锁经常用来防止脏写（见“没有脏写”一节）：如果两个事务同时尝试写入同一个对象，锁保证第二个写者必须等待，直到第一个事务完成之后（中止了或者提交了）才能继续。

两阶段锁与锁类似，但是使得锁定要求更强。只要没人尝试写入，好几个事务被允许并发读取同一个对象。但是一旦有人要写入（修改或是删除）对象，互斥性访问就是必须的：

* 如果事务A读取对象而事务B要写入到那个对象，B必须等待直至A提交或者中止之后才可以继续。（这确保了B不能在A不知情地情况下意外地改变对象。）

* 如果事务A写入了对象而事务B想要读取对象，B必须等待直至A提交或是中止之后才可以继续。（读取对象的旧值，如图7-1所示，在2PL下是无法接受的。）

在2PL中，写者不只是阻塞其它写着；它们也阻塞读者，反之亦然。快照隔离中读者从不阻塞写着，而写者也从不阻塞读者（见“实现快照隔离”一节），这抓住了快照隔离与两阶段锁的核心差别。另一方面，因为2PL提供了可串行性，它保护事务免受所有之前讨论的竞争条件，包括丢失更新与写偏。

#### Implementation of two-phase locking

2PL is used by the serializable isolation level in MySQL (InnoDB) and SQL Server, and the repeatable read isolation level in DB2 [23, 36]. 

The blocking of readers and writers is implemented by a having a lock on each object in the database. The lock can either be in shared mode or in exclusive mode. The lock is used as follows: 

* If a transaction wants to read an object, it must first acquire the lock in shared mode. Several transactions are allowed to hold the lock in shared mode simultaneously, but if another transaction already has an exclusive lock on the object, these transactions must wait. 

* If a transaction wants to write to an object, it must first acquire the lock in exclusive mode. No other transaction may hold the lock at the same time (either in shared or in exclusive mode), so if there is any existing lock on the object, the transaction must wait. 

* If a transaction first reads and then writes an object, it may upgrade its shared lock to an exclusive lock. The upgrade works the same as getting an exclusive lock directly. 

* After a transaction has acquired the lock, it must continue to hold the lock until the end of the transaction (commit or abort). This is where the name “two-phase” comes from: the first phase (while the transaction is executing) is when the locks are acquired, and the second phase (at the end of the transaction) is when all the locks are released. 

Since so many locks are in use, it can happen quite easily that transaction A is stuck waiting for transaction B to release its lock, and vice versa. This situation is called deadlock. The database automatically detects deadlocks between transactions and aborts one of them so that the others can make progress. The aborted transaction needs to be retried by the application.

#### Performance of two-phase locking

The big downside of two-phase locking, and the reason why it hasn’t been used by everybody since the 1970s, is performance: transaction throughput and response times of queries are significantly worse under two-phase locking than under weak isolation. 

This is partly due to the overhead of acquiring and releasing all those locks, but more importantly due to reduced concurrency. By design, if two concurrent transactions try to do anything that may in any way result in a race condition, one has to wait for the other to complete. 

Traditional relational databases don’t limit the duration of a transaction, because they are designed for interactive applications that wait for human input. Consequently, when one transaction has to wait on another, there is no limit on how long it may have to wait. Even if you make sure that you keep all your transactions short, a queue may form if several transactions want to access the same object, so a transaction may have to wait for several others to complete before it can do anything. 

For this reason, databases running 2PL can have quite unstable latencies, and they can be very slow at high percentiles (see “Describing Performance”) if there is contention in the workload. It may take just one slow transaction, or one transaction that accesses a lot of data and acquires many locks, to cause the rest of the system to grind to a halt. This instability is problematic when robust operation is required. 

Although deadlocks can happen with the lock-based read committed isolation level, they occur much more frequently under 2PL serializable isolation (depending on the access patterns of your transaction). This can be an additional performance problem: when a transaction is aborted due to deadlock and is retried, it needs to do its work all over again. If deadlocks are frequent, this can mean significant wasted effort.

#### Predicate locks

In the preceding description of locks, we glossed over a subtle but important detail. In “Phantoms causing write skew” we discussed the problem of phantoms — that is, one transaction changing the results of another transaction’s search query. A database with serializable isolation must prevent phantoms. 

In the meeting room booking example this means that if one transaction has searched for existing bookings for a room within a certain time window (see Example   7-2), another transaction is not allowed to concurrently insert or update another booking for the same room and time range. (It’s okay to concurrently insert bookings for other rooms, or for the same room at a different time that doesn’t affect the proposed booking.) 

How do we implement this? Conceptually, we need a predicate lock [3]. It works similarly to the shared/ exclusive lock described earlier, but rather than belonging to a particular object (e.g., one row in a table), it belongs to all objects that match some search condition, such as: 

```SQL
SELECT * FROM bookings WHERE room_id = 123 AND end_time > '2018-01-01 12: 00' AND start_time < '2018-01-01 13: 00';
```

A predicate lock restricts access as follows: 

* If transaction A wants to read objects matching some condition, like in that SELECT query, it must acquire a shared-mode predicate lock on the conditions of the query. If another transaction B currently has an exclusive lock on any object matching those conditions, A must wait until B releases its lock before it is allowed to make its query. 

* If transaction A wants to insert, update, or delete any object, it must first check whether either the old or the new value matches any existing predicate lock. If there is a matching predicate lock held by transaction B, then A must wait until B has committed or aborted before it can continue. 

The key idea here is that a predicate lock applies even to objects that do not yet exist in the database, but which might be added in the future (phantoms). If two-phase locking includes predicate locks, the database prevents all forms of write skew and other race conditions, and so its isolation becomes serializable.

#### Index-range locks

Unfortunately, predicate locks do not perform well: if there are many locks by active transactions, checking for matching locks becomes time-consuming. For that reason, most databases with 2PL actually implement index-range locking (also known as next-key locking), which is a simplified approximation of predicate locking [41, 50]. 

It’s safe to simplify a predicate by making it match a greater set of objects. For example, if you have a predicate lock for bookings of room 123 between noon and 1 p.m., you can approximate it by locking bookings for room 123 at any time, or you can approximate it by locking all rooms (not just room 123) between noon and 1 p.m. This is safe, because any write that matches the original predicate will definitely also match the approximations. 

In the room bookings database you would probably have an index on the room_id column, and/ or indexes on start_time and end_time (otherwise the preceding query would be very slow on a large database): 

* Say your index is on room_id, and the database uses this index to find existing bookings for room 123. Now the database can simply attach a shared lock to this index entry, indicating that a transaction has searched for bookings of room 123. 

* Alternatively, if the database uses a time-based index to find existing bookings, it can attach a shared lock to a range of values in that index, indicating that a transaction has searched for bookings that overlap with the time period of noon to 1 p.m. on January 1, 2018. 

Either way, an approximation of the search condition is attached to one of the indexes. Now, if another transaction wants to insert, update, or delete a booking for the same room and/ or an overlapping time period, it will have to update the same part of the index. In the process of doing so, it will encounter the shared lock, and it will be forced to wait until the lock is released. 

This provides effective protection against phantoms and write skew. Index-range locks are not as precise as predicate locks would be (they may lock a bigger range of objects than is strictly necessary to maintain serializability), but since they have much lower overheads, they are a good compromise. 

If there is no suitable index where a range lock can be attached, the database can fall back to a shared lock on the entire table. This will not be good for performance, since it will stop all other transactions writing to the table, but it’s a safe fallback position.