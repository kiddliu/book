# 第五章 副本

*一个可能出错的事物和一个不可能出错的事物之间的主要区别是，当一个不可能出错的事物出错时，通常没办法用也没办法修了。*

---

道格拉斯·亚当斯，*基本无害*（1992）

副本的意思是保存同一份数据的拷贝在数台通过网络连接的设备上。在第二部分的介绍中我们提到，想要复制数据有下面好几个原因：

* 让数据在物理位置上更靠近你的用户（因而降低了延迟）

* 即使在某些部分失效的情形下允许系统继续工作（因而增加了可用性）

* 增加了服务读取请求的设备数量（因而增加了读取吞吐量）

在这一章我们会假设你的数据集很小以至于单个设备就可以保存整个数据集的拷贝。在第六章我们会放开这个假设条件，讨论对单个设备太大的数据集的*分库*（*分片*）。在更后边的章节我们会讨论可以发生在复制的数据系统的各种不同种类的故障，以及如何处理它们。

如果复制的数据不会随时间变化，那么复制过程就很简单：你只需要把数据拷贝到各个节点，就这样。所有复制过程中的难题都发生在如何处理复制后数据的变化，这一章就是讲这些的。我们将讨论三种流行的节点间复制变更的算法：*单主机*，*多主机*，以及*无主机*复制。几乎所有的分布式数据库都使用这三种方式之一。它们都有各种优缺点，稍后我们会详细的研究。

副本有许多权衡需要考虑：觉个例子，用同步复制还是异步复制，如何处理失败的副本。它们通常是数据库中的可配置选项，尽管每个数据库细节都不一致，但是一般性的原则在众多不同的实现之间还是类似的。我们会在这一章讨论各种选择所带来的后果。

数据库副本是个旧话题——自1970年代被研究之后原则并没有太大的变化，因为网络的基础限制依然没变。然而在研究之外的地方，许多开发者很长一段时间继续假设数据库只包含一个节点。分布式数据库的主流应用是最近的事。由于许多应用开发者是该领域的新手，对类似*最终一致性*这样的问题一致有许多错误理解。在“TODO:副本延迟的问题”一节我们会更精确地了解最终一致性，并且讨论例如*读取你的写入*以及*单调读取保证*这样的问题。

## 领机与从机

每一个储存着数据库拷贝的节点叫做*副本*。有了多个副本，有个问题难免会有：我们如何保证数据拷贝到了所有的副本上？

每次对数据库的写入都需要被每一个副本处理；否则，副本不再包含着同一份数据了。最常见的解决方式叫做*基于领机的复制*（也叫做*主动/被动*或者*主从复制*），如图5-1所示。工作原理如下：

1. 副本之一被指定为领机（也叫做主机）。当客户端要写入数据库时，它们必须发送请求到领机，它首先把新数据写入到本地存储中。

2. 其它副本称为*从机*（*读取副本*，*从机*，*辅机*，或者*热待机*）。每当领机写入新数据到自己的本地存储，它也发送数据变更到它的所有从机，作为复制日志或者变更流的一部分。每个从机从领机获取日志并更新对应的本地数据库拷贝，它按照领机上处理的相同的顺序应用所有的写入操作。

3. 当客户端要读取数据库时，它既可以查询领机也可以查询任意从机。然而，写入只会被领机接受（从客户端的角度看，从机是只读的）。

*图5-1 基于领机的（主从）复制*

这种模式的复制是许多关系型数据库，比如PostgreSQL（自从9.0版本）、MySQL、Oracle Data Guard以及SQL Server的AlwaysOn Availability Groups的内建功能。它也被用在一些非关系型数据库中，包括了MongoDB、RethinkDB以及Espresso。最后，基于领机的复制并不只局限于数据库：分布式的消息代理例如Kafka以及RabbitMQ高可用队列也使用它。一些网络文件系统以及复制块设备比如（DRBD）也很类似。

### 同步复制 vs 异步复制

副本系统的一个重要细节是复制是*同步*发生的还是*异步*发生的。（在关系型数据库中，这通常是一个可配置选项；其它系统通常是硬编码为二者之一。）

考虑一下图5-1中发生了什么，其中的网站用户更新了他的资料图片。在某个时刻，客户端发送了更新请求到领机；稍后，请求被领机接受。再在某个时刻，领机把数据变更转发到从机。最终，领机通知客户端更新成功。

图5-2现实了系统众多组件之间的通信：用户的客户端，领机，以及两个从机。时间从左向右流逝。请求与响应消息是用粗箭头表示的。

*图5-2 基于领机的复制，其中一个同步从机一个异步从机*

在图5-2的势力中，复制到从机1是同步的：领机等待从机1直至从机1确认完成了写入动作之后才向用户报告成功，同时也才让写入对其他客户端可见。复制到从机2是异步的：领机发送消息，但是不会等待来自从机的响应。

图标显示了从机2处理消息之前有足够的延迟。通常，复制是相当快的：大多数数据库系统不到一秒钟就可以应用变更到从机。然而，并不能保证这个动作会花多长时间。有情况显示从机有可能落后于领机几分钟，甚至更多；举个例子，比如从机正在从故障中恢复，比如系统负载几乎为满，比如节点之间网络有问题。

同步复制的优势在于从机保证可以获得最新的、与领机一致的数据。如果领机突然崩溃，我们可以确定从机上的数据依然可用。缺点在于同步从机如果不响应（由于崩溃，或者网络故障，或者任何其他原因），写入将不能继续处理。领机必须阻塞所有的写入，直至同步副本再次可用。

因为这样的原因，那么所有的从机都是同步的就不现实了：任意节点离线都会导致整个系统陷于停顿。实践中，如果为数据库启用了同步复制，这通常意味着只有一个从机是同步的，而其他从机都是异步的。如果同步从机不可用或者变慢，可以把一个异步从机变为同步的。这保证了最新的数据至少出现在两个节点上：领机与一个同步从机。这种配置被叫做*半同步*。

很多时候，基于领机的复制是被配置成完全异步的。在这种情况下，如果领机故障也无法恢复的花，任何没有复制到从机的写入都会丢失。这意味着写入是无法保证的，哪怕客户端已经确认了。然而，完全异步配置也有优势，领机可以一致处理写入请求，哪怕所有的从机都落后于领机。

削弱耐久性听起来像是一个馊主意，但是异步复制仍然被广泛使用，尤其是有许多从机，或者从机分布在不同的地方。我们会在“复制延迟带来问题”一节回到这个问题上。

> 关于复制的研究
>
> 对于异步复制系统来说领机崩溃导致的数据丢失是一个很严重的问题，于是研究者依然在探索不会导致数据丢失但仍然提供良好性能于可用性的复制方法。比如，*链式复制*是同步复制的一个变种，已经成功应用在了类似微软Azure Storage这样的部分系统中了。
>
> 复制的一致性与共识（让好几个节点同意某个值）之间有很强的联系，我们会在第九章更详细地探索这个领域的理论。在这一章我们会集中在哪些实践中数据库上最长用到的简单复制形式上。

### 设置新的从机

时不时，你需要设置新的从机——也许是为了增加副本的数量，或者是替换崩溃的节点。你如何保证新的从机有着与领机一模一样的数据？

只是把数据从一个节点拷贝到另一个节点通常是不够的：客户端在持续地写入数据库，数据一直在变化，那么标准的文件拷贝在不同的时间点会看到数据库的不同部门。这样的结果没有任何意义。

你可以通过锁住数据库（从而无法写入）使磁盘上的文件一致，但是这违背了我们追求高可用性的目标。幸运的是，设置从机通常不会产生离线时间。定义上的流程是这样的：

1. 获取某个时间点的领机数据库的一致性快照——如果可能的话，不锁整个数据库。大多数数据库有这个功能，因为备份也是需要它的。在某些情况下，还需要第三方工具，例如MySQL的*innobakcupex*。

2. 把快照拷贝到新从机节点。

3. 从机连接到主机并请求所有发生在快照截取之后的数据变更。这要求快照绑定到了领机复制日志中的一个精确位置。这个位置有许多个名字：举个例子，PostgreSQL把它叫做*日志序列号*，而MySQL把它叫做*二进制日志坐标*

4. 当从机处理了自快照起积压的数据变更后，我们说它*赶上来*了。这是可以继续并开始处理来自领机的刚刚发生的数据变更。

设置从机的实际步骤由于数据库而非常不同。在某些系统中这个过程是全自动的，而在其它的里边它可以是某种神秘的多步骤工作流，需要管理员手动执行。

## 处理节点离线

系统中的任意节点都可以下线，也是是因为意外的故障，或者只是因为到了事先计划好的维护时间（举个例子，重启设备以安装内核安全补丁）。可以重启独立节点而不造成停机对于运维来说是巨大的优势。因此，我们的目标是保持系统作为一个整体运行良好而不在意个别节点崩溃，以为保持节点离线的影响越小越好。

对于基于领机的复制如何实现高可用性呢？

### 从机故障：追赶恢复

每一台从机在自己的本地磁盘上都保存了来自领机的数据变更的日志。如果从机崩溃然后重启，或者领机与从机之间的网络临时受阻，从机可以相当轻松的恢复：它从自己的日志知道故障发生钱最后一个被处理的事务。因而，从机可以连接到领机并请求自断线以后发生的所有数据变更。当这些变更应用完毕时，从机追赶上了领机并可以如往常一样接受数据变更流。

### 领机故障：故障迁移

处理领机故障更复杂一些：一台从机需要升级为新的领机，客户端需要重新配置才能发送写入请求到新的领机，同时其他从机需要开始处理来自新领机的数据变更。这个过程叫做*故障迁移*。

故障迁移可以手动触发（管理员接到领机故障的通知，采取必要措施构建新的领机）或者完全自动化。自动化故障迁移流程通常包含下面几步：

1. *判断领机故障*。许多事都有可能出错：系统崩溃，停电，网络问题，等等。没有简单明了的方式判断到底哪除了问题，所以绝大部分系统只是用了超时：节点频繁地在彼此之间来回传递消息，如果节点在一段时间内没有响应——比如说，30秒——我们就假设它崩溃了。（如果领机由于计划好的维护而估计下线的，就不适用了。）

2. *选择一个新领机*。这可以通过一个选举过程（领机由余下副本的大多数投票选择）解决，或者由先前指定的*控制器节点*指定新领机。最佳的备选通常是有着来自旧领机最新数据变更（把数据丢失风险降到最低）的副本。使所有节点都同意新领机是一个共识问题，会在第九章具体讨论。

3. *重新配置系统，使用新领机*。客户端现在需要发送它们的写入请求到新领机（会在“路由请求”一节讨论）。如果旧的领机恢复，它有可能还认为自己是领机，没有意识到其它副本强制它卸任了。系统需要保证旧领机变为从机，并且识别出新的领机。

故障迁移也注定有许多可能出错的事情：

* 如果使用了异步复制，新的领机可能在旧领机故障之前并没有接收到所有的写入。如果旧领机在新领机被选举出来之后重新加入到簇，这些写入怎么办呢？同时新领机也有可能收到冲突的写入。最常见的解决方案是旧领机丢弃没有被复制的写入，但是这样也许违背了客户端在持久性方面的预期。

* 如果数据库之外其它的存储系统需要与数据库内容协调的花，丢弃写入是极其危险的。举个例子，在GitHub的一次时间中，一个许久没有更新的从机被提升为领机。数据库使用了一个自增长的计数器给新插入的行分配主键，但是因为新领机的计数落后于旧领机的，于是重用了一些已经被旧领机分配过了的主键。这些主键也被用在了一个Redis存储中，主键的重用导致了MySQL与Redis之间不再保持一致，引发了某些私人数据暴露给了错误的用户。

* 在特定的故障场景（见第八章），两个节点都认为自己是领机的情况是会发生的。这种情况被叫做*脑裂*，这很危险：如果两个领机都接受写入请求，没有流程来处理冲突（见“多领机复制”一节），数据很有可能被丢失或者损坏。某些系统有机制在检测到两个领机后关闭其中一个节点。然而如果这个机制设计不是很细心的话，有可能最终两个节点都会被关闭。

在领机被宣布故障之前正确的超时是多少呢？超时时间更长意味着领机故障需要更长的时间恢复。然而如果超时过短，也可能发生不必要的故障迁移。举个例子，短时间负载高峰也许会让节点的响应时间超时，网络小故障也会导致包延迟。如果系统已经在高负载或是网络问题中苦苦挣扎，不必要的故障迁移极有可能使情况变得更糟，而不是更好。

这些问题都没有简单的解决方案。由于这个原因，某些运营团队更倾向于手动执行故障迁移，哪怕软件本身支持自动故障迁移。

这些问题——节点故障；不可靠的网络；以及副本一致性，持久性，可用性之间的权衡，以及延迟——实际上是分布式系统的基本问题。在第八和第九章我们会深度讨论它们。

### 复制日志的实现

基于领机的复制到底是如何工作的？实践中使用几种不同的复制方法，让我们简要了解一下每一种方法。

#### 基于语句的复制

在最简单的情况，领机记录每一个它执行地写入请求（语句），然后发送语句日志到它地从机。对于关系型数据库，这意味着每一个`INSERT`、`UPDATE`或`DELETE`语句都转发给了从机，每个从机解析并执行这个SQL语句如果它是直接来自客户端的。

虽然这听起来很合理，但是这种方式有许多种失败的可能：

* 任何语句调用非确定性函数，比如`NOW()`以获取当前日期与时间或者`RAND()`以获取随机数，在不同副本很有可能生成不同的值。

* 如果语句用到自增的列，或者是依赖数据库中已有的数据（比如，`UPDATE ... WHERE <some condition>`），他们在每一个副本上必须以完全一致的顺序执行，否则它们也许有不同的结果。当有多个并发执行的事务时，这会是很大的限制。

* 有副作用的语句（比如触发器，存储过程，用户定义函数）会在每一个副本上触发不同的副作用，除非副作用是绝对确定性的。

绕过这些问题是可能的——举个例子，领机可以在记录语句时用固定的返回值替换任何非确定性函数调用，从而所有从机都获得相同的值。然而，因为有许多边缘情况，其它复制方法现在通常更受欢迎。

MySQL在5.1版本之前使用基于语句的复制。今天依然有时在用，由于相当紧凑，但是如果语句中有任何不确定性默认的MySQL会切换到基于行的复制（稍后将讨论到）。VoltDB使用基于语句的复制，通过要求事务是确定性的使之安全。

#### 预写入日志（WAL）发送

在第三章我们讨论了存储引擎如何在磁盘上呈现数据，我们发现通常每一次写入都附加到了日志上：

* 对于日志结构的存储引擎（见“SSTable与LSM树”一节），这个日志是存储的主要位置。日志在后台被压缩并进行垃圾回收。

* 对于会复写独立磁盘块的B树（见“B树”一节），每一次修改首先被写入到预写入日志于是索引可以在崩溃之后恢复到一致的状态。

任一种情况，日志都是一个只附加的字节序列，包含着所有对数据库的写入。我们可以用一模一样的日志在另一个节点上构建副本：除了把日志写入到磁盘，领机也通过网络把它发送给从机。当从机处理这个日志时，它构建了领机上一模一样的数据结构拷贝。

这种复制方法被用在了PostgreSQL以及Oracle等等。主要的缺点是日志在一个非常低的层级描述数据：WAL细致地包含了哪个磁盘区块的哪个字节发生了变化。这使得复制与存储引擎紧密地耦合着。如果数据库把储存格式从一个版本变成了另外一个版本，通常在领机和从机上运行不同的数据库版本是不可能的。

这看起来好像只是一个很小的实现细节，但是运营时会有很大的影响。如果复制协议允许从机比领机更新的软件版本，你可以执行执行零离线时间的数据库软件升级：首先升级从机，然后执行故障迁移使其中一个从机变为领机。如果复制协议不允许这种版本不匹配，由于这是WAL发送常态，这样的升级需要离线时间。

#### 逻辑（基于日志的）日志拷贝

一种替代方案是为复制与存储引擎使用不同的日志格式，这样使得复制日志与存储引擎内部实现解耦。这种复制日志被叫做*逻辑日志*，用以区分存储引擎的（物理的）数据表示方法。

关系型数据库的逻辑日志通常是记录序列，以行的粒度描述对数据表的写入：

* 对于被插入的行，日志包含了所有列的新值。

* 对于被删除的行，日志包含了足够多的信息用来唯一标识被删除的行。通常这个信息是主键，但是如果表里没有主键，所有列的旧值都需要被记录。

* 对于被更新的行，日志包含了足够多的信息用来唯一标识被更新的行，还包括所有列的新值（或者至少是所有变了的行的新值）。 

一个修改了好几行的事务会生成好几个这样的日志记录，紧接着是一个指示事务已提交的记录。MySQL的二进制日志（当被配置为使用基于行的复制时）使用这个方式。

由于逻辑日志与存储引擎内部实现解耦，它很容易的保留了向后兼容性，允许领机与从机运行不同版本的数据库软件，甚至是不同的数据引擎。

逻辑日志格式也很容易被外部应用解析。如果向一个外部系统发送数据库内容，比如向数据仓库发送做离线分析，或者是为了构建自定义索引以及缓存，这一方面都是很有用的。这个技术叫做*数据变更抓取*，我们会在第十一章回到这个话题。

#### 基于触发器的复制

截至目前描述的复制方法都是数据库系统实现的，没有涉及到任何应用程序代码。在许多情况下，这就是你要的——但是也有一些场景需要更大的灵活性。举个例子，如果你只想复制一部分数据，或者从一种数据库复制到另一种数据库，或者你需要解决冲突的逻辑（见“处理写入冲突”一节），那么你也许需要把复制过程向上挪到应用层。

一些工具，例如Oracle GoldenGate，通过读取数据库日志使数据变更对应用程序可用。另外一个备选方案使使用在许多关系型数据库中都有的功能：*触发器*与*存储过程*。

触发器允许你注册自定义的应用程序代码，当数据库系统中数据变化的时候自动执行。触发器有机会把这个变更记录到单独一张表里，外部进程可以读取它。这个外部进程之后应用任意必要的引用逻辑然后把数据变更复制到其它系统。举个例子，针对Oracle的Databus与针对Postgres的Bucardo都是这样工作的。

相比于其它复制方法，基于触发器的复制通常有更大的消耗，相比于数据库内建的复制方法也更容易出问题，更容易受到限制。然而，它仍然因为灵活性而有用。

## 复制延迟带来的问题

可以容忍节点故障只是想要复制的一个原因。正如在第二部分的介绍中提到的，其他原因还包括可扩展性（处理超过单台设备可以应对的请求）以及延迟（把副本放置在地理位置更靠近用户的地方）。

基于领机的复制需要所有的写入请求发送到单个节点，但是只读查询请求可以发送到任何副本。由于工作负载绝大部分是读取而很小一部分是写入（网络上常见的模式），这里有一个吸引人的选项：建立许多从机，并把读取请求分发到这些从机去。这降低了领机的负载并且允许读取请求是由就近的副本服务的。

在这种按读取缩放的架构中，你可以通过只是添加更多从机来增加服务只读请求的容量。然而，这种手段只对异步复制实际有效——如果你尝试同步复制到所有的从机，单个节点故障或者是断网都会是整个系统写入不可用。而且节点越多，一个节点离线的可能性就越大，因而一个完全的同步配置是非常不可靠的。

然而，如果引用程序读自异步从机，如果从机落后的话它也许会看到过期的信息。这会导致数据库中明显的不一致：如果同时在领机与从机上执行一样的查询请求，你也许会得到不一样的结果，因为不是所有的写入请求都反映到了从机。这种不一致只是种临时状态——如果你停止写入到数据库然后等一小会儿，从机最终会赶上，与领机保持一致。由于这个原因，这个效果叫做*最终一致性*。

术语“最终”是故意模糊的：一般地，没有限制副本可以落后多少。在正常地运营下，从写入到领机到反映到从机之间的延迟——复制延迟——也许只是几分之一秒，实践中并不太容易注意到。然而如果系统运行在接近满负载的情况或者网络中有问题，延迟会很轻易地上升到几秒钟甚至几分钟。

当延迟很高的时候，产生的不一致性不再只是理论性的问题而是应用真实的问题。在这一节我们会着重研究三个示例问题，它们在有复制延迟的时候很有可能发生，并且列出解决它们的几个办法。

### 读取自己的写入

许多应用程序允许用户提交一些数据，然后显示他们刚刚提交的。这也许是客户数据库中的记录，讨论主题中的评论，或是其它类似的东西。当新数据被提交以后，它必须被发送到领机，但是当用户查看数据，它可能读自一台从机。如果数据被频繁地查看但是只是偶尔写入地话，这样就特别合适。

对于异步复制来说，这里有个问题，如图5-3所示：如果用户在数据写入后不久即浏览它，那么新数据也许还没有到达副本。对于用户，这看起来好像他们提交的数据丢失了，可以理解他们会不高兴的。

*图5-3 用户写入后紧接着从旧的副本读取。为了避免这样的异常现象，我们需要保持写入后读取一致性*

在这种情况下，我们需要*写入后读取一致性*，也叫做*读取自己的写入一致性*。这是一种保证，如果用户重新加载页面，他们总是可以看到他们自己提交的任意更新。这对其它用户没有任何承诺：其它用户的更新稍后才可见。然而，它向用户保证了它们自己的写入被正确保存了。

在一个基于领机的复制的系统中我们如何实现写入后读取一致性呢？仅举几例：

* 在读取用户可能修改了的数据时，从领机读取；否则，从从机读取。这要求你有办法在不实际查询的情况下知道数据是不是已经被修改了。举个例子，社交网络上的用户信息一般只允许用户自己修改，其它人是不可以的。因而，一个简单的规则是：总是从领机读取用户自己的用户信息，从从机读取任意其它人的用户信息。

* 如果应用程序中的大部分东西都是可以被用户修改的，那种方式就不是很有效了，因为大多数数据就必须从领机读取了（读取缩放的收效将大打折扣）。在这种情况，可以用其它标准决定是否从领机读取。举个例子，你可以追踪最后一次更新的时间，以及在最后一次更新之后的一分钟内都从领机读取。你也可以监控监控从机的复制延迟，拒绝对落后一分钟以上的从机发起查询请求。

* 客户端可以记住最近一次写入的时间戳——之后系统可以保证服务于那个用户的副本会返回截至于那个时间点前的所有更新。如果副本数据不足够新，要么读取请求可以由另外一个副本处理，要么查询请求可以等待副本更到最新。时间戳可以是逻辑时间戳（指示写入顺序的东西，比如日志序列号）或者是实际的系统时钟（在这种情况下时钟的同步问题变得关键起来；见“不可靠的时钟”一节）。

* 如果副本分布在数个数据中心（为了从地理位置上更接近用户，或者是为了高可用性），就更增进了复杂性。任何需要领机服务的请求都必须首先发送到包含领机的数据中心。

当同一个用户从多个设备，比如桌面网页浏览器和移动客户端，访问你的服务时另外一个复杂问题出现了。在这种情况下你需要提供*跨设备的*写入后读取一致性：如果用户在一台设备上输入了某些信息然后再另外一台设备上浏览它，他们应该看到各个输入的信息。

在这种情况下，有几个额外的问题需要考虑：

* 那种需要记住用户最后一次更新的时间戳的方式变得更困难了，因为运行在一个设备上的代码不知道另一台设备上发生了什么样的更新。这个元数据需要中心化。

* 如果你的副本分布在不同的数据中心里，那么没有办法保证来自不同设备的连接会路由到相同的数据中心。（举个例子，如果用户的台式机使用家里的宽带连接而移动设备使用的是移动数据网络，那么设备的网路路由也许是完全不同的。）如果你的方法需要从领机读取，也许你首先需要吧来自用户设备的所有请求路由到同一个数据中心。

### 单调读取

从异步从机读取时可以发生的异常现象第二例，是用户有可能看到时间倒流了。

这种情况可以发生在用户尝试从不同的副本读取数据。举个例子，图5-4显示了用户2345发起两次同样的请求，第一个请求发送到了有少许延迟的从机，之后第二个发送到了有很大延迟的从机。（当用户刷新网页时这个场景很可能发生，每一个请求都发送到了一个随机服务器。）第一个查询请求返回了用户1234最近添加的一条评论，但是第二天查询请求并没有返回任何东西，因为延迟的从机还没有收到那条写入变更。实际上，相比于第一条查询请求第二条查询请求在一个更早的时间点观察系统。如果第一条查询请求没有返回任何东西这看起来不算太差，因为用户2345大概不知道用户1234最近添加了一条评论。然而，如果先看了用户1234的评论，之后又消失了，这会非常让用户2345困惑。

*图 5-4. 用户首先从更新了的副本读取，之后从旧的副本读取。时间看起来倒流了。我们需要单调读取来防止这种异常现象。*

*单调读取*保证了这一类异常情况不会发生。相比于强一致性它是一种弱保证，但是相比于最终一致性这是一种强保证。当读取数据时，你也许会看到一个旧的值；单调读取只意味着如果用户顺序发起了几个请求，他们不会看到时间倒流——也就是说，他们不会在之前读到新数据的情况下再读到旧数据。

实现单调读取的一种方式是保证每个用户都从同一个副本读取（不同的用户可以从不同的副本读取）。举个例子，副本的选择可以基于用户ID的哈希，而不是随机的。然而，如果那台副本故障了，用户的查询请求需要被重新路由到另外一台副本。

### 一致性前缀读取

复制延迟异常现象的第三个例子是关于因果关系违反的。设想下面一段Poons先生与Cake太太的对话

*Poons先生*

Cake太太，对于未来你能看多远？

*Cake太太*

通常大约十秒钟吧，Poons先生。

两句话之间有因果关系：Cake太太听到了Poons先生的问题并做出了回答。

现在，假设第三个通过从机再听这段对话。Cake太太说的话经过了一个有少许延迟的从机，但是Poons先生说的话有一个更长的复制延迟（见图5-5）。观察者会听到下面的对话：

*Cake太太*

通常大约十秒钟吧，Poons先生。

*Poons先生*

Cake太太，对于未来你能看多远？

对于观察者来说这看起来好像Cake太太正在回答Poons先生还没有问出口的问题。这种精神力量给人留下深刻印象，但是也会很困惑。

*图5-5 如果部分分库比其它的复制的慢，观察者会先看到答案再看到问题。*

避免这一类异常现象需要另一种类型的保证：一致性前缀读取。这种保证是说如果一系列写入以某种顺序发生，那么任何读取这些写入的人也会以同样的顺序看到这些写入。

这对于已分区的（已分片的）数据库来说是一个很特别的问题，我们会在第六章讨论它。如果数据库一致以同样的顺序应用写入动作，写动作会一直看到一致性前缀，于是这种异常不会发生。然而，在许多分布式数据库中，不同的分区独立地运行，所以没有写入地全局顺序的：当用户读取数据库时，他们会看到某些部分是旧的状态，而有一些已经是新的状态了。

一种解决方案是保证任何有因果关系的写入都去到了同一个分区——但是在一些应用中没有办法高效地完成。也有算法可以明确地保留因果关系记录，我们将在““之前发生”关系与并发”一节回到这个问题上。

### 复制延迟的解决方案

在使用最终一致系统时，很值得考虑一下当复制延迟上升到几分钟甚至几个小时的时候应用的行为如何。如果你的答案是“没问题”，这很好。然而，如果结果是用户体验很糟糕的话，为系统设计更强的保证，比如写入后的读取，是很重要的。假装复制是同步的但实际上是异步的注定会导致问题。

如前面所讨论的，应用程序相比于底层数据库提供更强的保证是有许多种方式的——举个例子，在领机上执行特定类型的读取。然而在应用程序代码中处理这些问题会很复杂并且很容易出错。

如果应用程序开发者不用考虑细微的复制问题而是相信数据库就可以“做正确的事”，那就更好了。这就是事务存在的原因：这是一种数据库提供更强保证的方式，于是应用变得更简单。

单节点事务已经存在很长时间，然而，在转向分布式（复制了的或者分区了的）数据库时，许多系统放弃了它，声称事务对于性能以及可用性来说都代价太高了，并且断言最终一致性在可扩展系统中是无法避免的。这段表述部分是事实，但是过于简单了，而我们会在本书的余下部分构建更细致入微的观点。我们会在第七、九章回到事务这个主题，而在第三部分讨论一些替代机制。

## 多领机复制

本章截至到目前我们只考虑了使用单个领机的复制架构。虽然那是一种常见的方式，但是也有许多有趣的替代方案。

基于领机的复制有一个重大的缺点：只有一个领机，所有的写入请求都必须通过它完成。如果你因为任何原因无法连接到领机，比如说因为你与领机之间的网络故障，你无法写入到数据库中。

基于领机复制模型的一种自然扩展是允许一个以上的节点接受写入请求。复制过程与先前方式一模一样：每一个处理写入请求的节点都比如转发数据变更到所有其它的节点。我们把这个叫做多领机配置（也叫做主-主或者主动/主动复制）。在这种配置下，每一个领机同时也是其它领机的从机。

### 多领机复制的用例

在单个数据中心内配置多领机几乎没有意义，因为带来的好处抵不过增加了的复杂度。然而，在某些情况下这种配置是合理的。

#### 多数据中心运营

设想你有一个数据库，副本分布在几个不同的数据中心内（也许这样你可以容忍整个数据中心故障，或是为了使它们更靠近用户）。对于普通的基于领机的复制配置来说，领机必须位于其中一个数据中心内，而所有的写入请求都必须通过那个数据中心。

在一个多领机配置中，你可以在每一个数据中心都有一个领机。图5-6显示了这样的架构大概使什么样子的。在每个数据中心中，使用正常的领机-从机复制；在数据中心之间，每个数据中心的领机把自己的变更复制给其它数据中心的领机。

*图5-6 横跨多个数据中心的多领机复制*

让我们比较一下在多数据中心部署环境中单领机与多领机配置的情况：

*性能*

在单领机配置中，每一个写入请求都必须穿过互联网抵达领机所在的数据中心。这会明显增加写入的延迟，也可能与使用多个数据中心的首要目的相抵触。在多领机配置中，每个写入都可以在本地数据中心处理，然后异步复制到其它数据中心去。因而数据中心之间的网络延迟对用户是不可见的，这意味着感受到的性能也许会更好一些。

*数据中心离线忍耐度*

在单领机配置中，如果领机所在的数据中心故障了，故障迁移可以吧另一个数据中心的从机提升为主机。在多领机配置中，每一个数据中心可以独立运行，当故障了的数据中心重新上线时复制过程可以重新赶上来。

*网络问题忍耐度*

数据中心之间的交通通常穿越公共互联网，它相比于数据中心内的本地网络就不太可靠了。单领机的配置对于这个数据中心之间的链路问题非常敏感，因为写入请求是在这个链路上同步发出的。异步复制的多领机配置通常可以更好的忍耐网络问题：临时的网络中断不会妨碍处理写入请求。

一些数据库默认支持多领机配置，但是也经常利用外部工具实现，比如针对MySQL的Tungsten Replicator，针对PostgreSQL的BDR，以及针对Oracle的GoldenGate。

虽然多领机复制有优势，但是他也有一个大缺点：同样的数据也许同时在两个不同的数据中心内处理，这种写入冲突必须要解决（如图5-6中标识为“解决冲突”的）。我们会在“处理写入冲突”一节讨论这个问题。

由于多领机复制在许多数据库中是某种程度上有所改进的功能，经常会有一些细微的配置陷阱以及与其它数据库功能令人意外的交互。举个例子，自增长的键，触发器，以及完整性约束是很有问题的。由于这样的原因，多领机复制经常被认为是需要尽量避开的危险地带。

#### 离线客户端

另外一种多领机复制更合适的情形是你的应用需要在与互联网断开的情况下继续工作。

举个例子，考虑一下在手机上的、笔记本电脑上以及其它设备上的日历应用。你需要在任意时刻都可以看到你的会议安排（发起读取请求）并且可以输入新的会议安排（发起写入请求），不管你的设备当前是否有互联网连接。如果在离线期间做出了任何变更，它们都需要在设备下次上线时与服务器以及你的其它设备同步。

在这种情况下，每个设备都有一个本地数据库作为领机（接受写入请求），并且在所有设备上的日历副本之间有一个异步多领机复制过程（同步）。复制延迟也许有数个小时甚至数日，取决于什么时候你可以有互联网访问。

从架构的角度上看，这种结构本质上与多数据库之间的多领机复制是一样的，极端一些看的话：每个设备是一个“数据中心”，它们之间的网络连接是极不稳定的。鉴于历史上有太多失败的日历同步实现，多领机复制不那么容易搞对的。

也有工具旨在帮助把多领机配置变得简单。举个例子，CouchDB就是为这种运作模式设计的。

#### 协作编辑

实时的协作编辑应用程序允许几个人同时修改一份文档。举个例子，Etherpad与Google Docs允许几个人同时编辑一个文本文件或者电子表格（这种算法将在“自动解决冲突”一节简要讨论）。

我们一般不会把协作编辑视为数据库复制问题，但是它与之前提到的离线编辑使用场景有许多共同之处。但一个用户修改了文件，变更立即应用到了本地副本（网页浏览器或客户端程序中的文档状态）并异步地复制到服务器以及任何其它用户正在修改的同一个文件。

如果要保证没有任何编辑冲突，用户在编辑之前应用必须获得整个文档的锁。如果另外一个用户想要编辑同一个文档，就必须等待第一个用户提交了所有的变更并且释放了锁。这种协作模型等价于领机支持事务的单领机复制。然而，对于更快的协作，大概需要把变更单位变得很小（比如单个按键动作）且避免锁。这种方式允许多个用户同时编辑，但是也带来所有多领机复制的挑战，包括需要解决冲突。

### 解决写入冲突

多领机复制最大的问题是会发生写入冲突，这意味着必须解决冲突。

举个例子，考虑一下一个维基页面同时被两个用户编辑，如图5-7所示。用户1把页面题目从A改为了B，而用户2同时把题目从A改为了C。每个用户的变更都成功地应用到了本地领机。然而，当变更被异步地复制了之后，冲突被检测到了。这个问题不会出现在单领机数据库。

*图5-7 两台领机同时更新同一条记录导致的写入冲突*

#### 同步 vs 异步冲突检测

在单领机数据库中，第二个写者要么会被阻塞然后等待第一个写入完成，要么中止第二次写入事务，强迫用户重新尝试写入。另一方面，在多领机配置环境中，两个写入都可以成功，冲突只会在稍后的一个时间点异步检测到。那个时候，让用户解决冲突可能已经晚了。

原则上，你可以使冲突检测变为同步的——比如，等待写入被复制到所有副本之后才告诉用户写入成功了。然而，这样做，你会失去多领机复制的主要优势：允许每个副本独立接受写入请求。如果你要同步的冲突检测，也许你只需要用单领机复制。

#### 避免冲突

应对冲突最简单的策略就是避免它们发生：如果应用程序可以保证对某个特定条目的写入请求都经过同一台领机，那么冲突是不会发生的。由于许多多领机复制的实现处理冲突的能力相当差，避免冲突发生是经常推荐的方式。

举个例子，在一个用户可以编辑他们自己数据的应用中，你可以保证来自某个特定用户的请求一直发送到同一个数据中心，并用那个数据中心里的领机做读取和写入。不同用户也许有不同的“主”数据中心（也许选择是基于地理位置上更靠近用户考虑的），但是从任何一个用户的角度看这个配置就是单领机的。

然而，优势你也许想要为某个条目改变指定的领机——也许是因为一个数据中心故障了，你需要重新路由流量到另一个数据中心，或者是因为用户移动到了一个不同的位置，如今更靠近另一个数据中心。在这种情况下，避免冲突就是小了，你必须处理不同领机上并行写入的可能性。

#### 趋于一致的状态

单领机数据库是按顺序应用写入请求的：如果同一个字段有好几个更新请求，最后一次写入决定了字段最终的值。

在多领机的配置中，没有定义好的写入顺序，所以最终值应该是多少也不清楚。在图5-7中，在领机1，题目首先更新到了B再是C；在领机2首先更新为了C再是B。没有哪个顺序比其它顺序“更正确”。

如果每一个副本以它们看到的顺序应用写入请求的话，数据库最终会处于不一致的状态：领机1的最终值为C而在领机2为B。这是无法接受的——每个复制方案都必须保证数据在所有副本最终都是一样的。因此，数据库必须以*会聚*的方式处理冲突，它意味着当所有变更都被同步了以后所有的副本都必须有着同样的最终值。

有许多种方式实现会聚的冲突处理：

* 给每个写入请求分配唯一ID（比如时间戳，长随机数，UUID，或者键值对的哈希），选有着最高ID的写入请求作为*胜者*，然后抛弃其它写入请求。如果用时间戳，这种技巧被叫做*最终写入为准*（LWW）。虽然这种方式很流行，但是它很容易导致数据丢失。我们会在本章末尾（“检测并行写入”一节）再详细讨论LWW。

* 给每个副本分配唯一ID，让再大数字副本发起的写入永远优先于小数字副本发起的写入。这种方式也会导致数据丢失。

* 以某种方式把值合并在一起——比如按字母顺序将它们排序，然后连接起来（在图5-7中，合并的题目可以是类似“B/C”的东西）。

* 把冲突记录在一个明确的数据结构中，保留所有的信息，并且实现在稍后某个时间解决冲突的应用代码（也许还要提醒用户）。

#### 自定义的解决冲突的逻辑

由于解决冲突最合适的方式取决于应用程序，大多数多领机复制工具允许你用应用程序代码写出解决冲突的逻辑。这些代码可以在写入或是读取的时候执行：

*写入时*

一旦数据库系统在复制变更的日志中检测到了冲突，它调用冲突处理逻辑。举个例子，Bucardo允许你为这个目的写一段Perl。这个处理逻辑通常不能提示用户——它运行在后台进程而且它必须执行的很快。

*读取时*

当检测到冲突时，所有发生冲突的写入请求被存了起来。下一次数据被读取时，这些不同版本的数据被返回给应用程序。应用也许会提示用户或者自动解决冲突，并且把结果写回数据库。举个例子，CouchDB是这样做的。

值得注意的是解决冲突通常应用在单个行或者单个文档这样的级别，而不是整个事务。因而，如果你有一个事务原子性地发起来几个不同地写入请求（见第七章），从解决冲突的目的看每个写入仍将被视为单独的写入。

> 自动解决冲突
>
> 解决冲突的规则可以很快地变复杂，而自定义代码很容易出错。亚马逊经常由于解决冲突处理机制令人意外的效果而被诟病：有些时候，购物车解决冲突的逻辑会保留添加到购物车的项，但是不保留从购物车删除的项。因而，客户有些时候会看到先前被删除的项又出现了。
>
> 对于由并发的数据修改所导致冲突的自动解决已经有了一些很有意思的研究。值得一提的有：
>
> * *无冲突复制数据类型*（CRDT）是一系列可以同时被多人编辑的数据类型，集合，映射，有序链表，计数器等等，它们以合理的方式自动地解决冲突问题。一些CRDT已经在Riak 2.0版中实现了。
>
> *可以合并的持久性数据结构*显式地记录历史，类似Git版本控制系统，使用三路合并函数（而CRDT使用两路合并）。
>
> *运行转型*是如Etherpad和Google Docs这些协作编辑应用背后的冲突解决算法。它特别为项的有序链表的并发编辑而设计，例如构成文本文件的字符列表。
>
> 这些算法的实现在数据库中依然很年轻，但是在未来它们很有希望被集成到更多的复制数据系统中。自动解决冲突可以使应用程序处理多领机数据同步更加简单。

#### 什么是冲突？

有些类型的冲突是很明显的。在图5-7的示例中，两次写入请求同时修改了同一条记录的同一个字段，设置成了不同的值。毫无疑问这是冲突。

其它类型的冲突可能更不容易检测到。举个例子，设想一下会议室预订系统：它记录了什么人什么时间预定了哪个房间。这个应用需要保证每个房间任何时间都只能被一组人预定（也就是说，同一个房间不能有任何交叉的预定计划）。在这种情况下，如果两个不同的预订计划同一时间预订了同一个房间，冲突就很可能发生了。即使应用程序首先检查了房间是否可用才允许用户发起预订请求，如果两个预订请求发生在两个不同的领机上，依然会有冲突。

这个问题没有现成的答案，但是在后续章节我们会加深对这个问题的理解。在第七章我们会看到更多的冲突示例，而在第十二章我们会讨论在复制系统中检测和解决冲突的可扩展方法。

### 多领机复制的拓扑结构

复制的拓扑结构描述了写入请求从一个节点传到另外一个节点的通信路径。如果你有两台领机，如图5-7一样，那只有一种合理的拓扑结构：领机1必须把所有的写入请求发送给领机2，反过来也一样。如果多于两个领机，许多不同的拓扑结构就有了可能。图5-8展示了一些例子。

*图5-8 多领机复制可以配置的三种示例拓扑结构*

最一般的拓扑结构是*多对多*（图5-8【c】），在这种拓扑结构中每个领机把自己的吸入请求发送给其它所有领机。然而，更受限的拓扑结构也会被用到：举个例子，MySQL默认只支持*环形拓扑结构*，在这种拓扑结构中每个节点从一个节点接收写入请求并把这些写入请求（外加任何自己的写入请求）转发给另一个节点。另外一种受欢迎的拓扑结构是*星形*的：一个制定的根节点转发所有的写入请求给所有其他节点。星形拓扑结构可以一般化为树。

在环形与星形拓扑结构中，写入请求也许需要经过好几个节点才能到达所有的副本。因此，节点需要转发它们从其它节点收到的数据变更。为了防止无限次的复制循环，每个节点被分配了唯一标识符，同时在复制日志中，每个写入请求都标记了所经节点的标识符。当节点收到了标记着自己的标识符的数据变更时，那个数据变更被忽略了，因为节点知道它已经被处理过了。

环形与星形拓扑结构的问题在于如果一个节点故障了，这会终端其他节点之间复制消息的传递，导致它们在节点恢复之前没有办法通信。这时拓扑结构可以被重新配置来绕过故障节点，但是在绝大多数部署中这样的重新配置需要手动完成。连接更密集的拓扑结构对故障的容忍度更高，因为它允许消息从不同的路径传递，避免单节点故障的问题。

另一方面，多对多拓扑结构也是会有问题的。特别是，某些网络链接比其它的要快（比如因为网络拥堵），结果导致某些复制消息”赶超“了其它消息，如图5-9所示。

*图5-9 在多领机复制中，写入请求在某些副本也许会以错误的顺序抵达*

在图5-9中，客户端A向领机1中的表插入一行，而客户端B更新了领机3上的那行。然而，领机2也许是以不同的次序收到了这些写入请求：它有可能首先收到了更新（这种情况从它的角度看来，是在尝试更新数据库中不存在的行）而之后收到了对应的插入请求（它原本应该在更新请求前到达）。

这是一个因果问题，与我们在“一致性前缀读取”一节看到的类似：更新请求依赖先前的插入请求，于是我们需要保证所有节点先处理插入请求，然后再是更新请求。只是为每个写入请求附上时间戳是不够的，因为没有办法相信时钟是完全同步了的，进而在领机2上正地对这些事件排序（见第八章）。

为了正确地为这些事件排序，用到了一种叫做版本向量的技巧，我们会在本章稍后讨论它（见“检测并发写入”一节）。然而，冲突的检测技巧在许多多领机复制系统中实现得很差。举个例子，在本书写成时，PostgreSQL BDR不提供因果关系的写入请求，而针对MySQL的Tungsten Replicator甚至根本不去检测冲突。

如果你在使用多领机复制系统，注意这些问题，仔细地阅读说明文档都是值得的，并且全面地测试你的数据库以保证它确实提供了你认为的必要的保证。

## 无领机复制

本章截至目前我们讨论的复制方法——单领机以及多领机复制——是基于这样一个理念的：客户端发送写入请求到一个节点（领机），然后数据库系统自行复制那个写入请求到其它副本。领机决定了处理写入请求的顺序，而从机以同样的次序应用领机的写入请求。

一些数据存储系统选择了完全不同的方式，放弃了领机的概念而允许任何副本直接接受来自客户端的写入请求。一些早期的复制数据系统是无领机的，但是在关系型数据库的绝对统治时期这个理念被大部分遗忘了。它再一次变成流行的数据库架构实在亚马逊把它应用在了自家的*Dynamo*系统。Riak、Cassandra以及Voldemort都是受Dynamo启发的开源无领机模型数据存储，所以这一类数据库也被叫做*Dynamo风格*。

在某些无领机的实现中，客户端直接发送写入请求到数个副本，而在其它时候，协调节点代替客户端做这些事情。然而，不像领机数据库，协调器不会强制特定的写入顺序。如我们所见的，这种设计上的差异对数据库的使用方式有着深刻的影响。

### 节点下线时写入数据库

设想一个有三个副本的数据库，其中一个副本当前离线了——也许正在重启以安装系统更新。在基于领机的配置环境中，如果想要继续处理写入请求，你也许需要执行故障迁移（见“处理节点离线”一节）。

另一方面，在无领机的配置环境中，故障迁移是不存在的。图5-10展示了会发什么：客户端（用户1234）并行发送写入请求到所有三个副本，两个在线副本接受了写入请求但是离线副本错过了。我们假设三个副本中的两个接受了写入请求已经足够了：在用户1234收到两个*好的*响应时，我们就认为写入成功了。客户端可以忽略副本中的一台错过了写入的事实。

*图5-10. 写入定额，读取定额，以及节点离线后的读取修复*

现在假设离线节点重新上线，而客户端开始读取它。任何发生在离线期间的写入请求对于那个节点来说都错过了。因此，如果要从那个节点读取，会得到旧的（过时的）的值作为响应。

为了解决这个问题，当客户端从数据库读取时，它不只是向一个副本发送请求：读取请求也并行发送到数个节点。客户端也许从不同的节点或的不同的响应；也就是说，从一个节点获得了最新的值而从另一个节点获得了旧的值。版本号码被用来决定哪个值更新（见"检测并发写入"一节）。

#### 读取修复与反熵

复制方案应该保证最终所有数据都拷贝到每个副本。当离线节点重新上线时，它如何追回它错过的所有写入请求呢？

在Dynamo风格的数据存储中常用到两种机制：

*读取恢复*

当客户端并行向几个节点发起读取请求，它可以检测到任何旧的响应。举个例子，在图5-10中，用户2345从副本3获得版本6的值，而从副本1与副本2获得版本7的值。客户端看到副本3的值更老并把较新的值写回。这种方式对于被频繁读取的值很好用。 

*反熵处理*

此外，一些数据存储有后台进程持续地寻找副本间数据地差异，并把缺失地数据从一个副本拷贝到另一个去。与基于领机复制使用的复制日志不同的是，这种反熵处理进程不以任何特定顺序拷贝写入请求，同时在数据被拷贝之前有明显的延迟。

并不是所有的系统都实现的这两种机制；举个例子，Voldemort目前还没有反熵处理。值得注意的是没有反熵处理的话，很少读取的值很可能在某些副本是缺失的，因而降低了耐久性，因为读取恢复只会在应用读取值的时候才执行。

#### 读写的定额

在图5-10的示例中，我们认为即使三个副本中的两个处理了写入请求这也是成功的。那么如果三个副本中只有一个接受了写入请求呢？我们能把这个成功范围扩到多大呢？

如果我们知道每一个成功的写入请求保证出现在三个副本中的两个，那意味着最多只有一个副本会过时。因而如果我们从任意两个副本读取，我们可以确定二者至少有一个是最新的。如果第三个副本离线或是响应地很慢，读取请求仍然可以返回最新地值。

更一般地说，如果有*n*个副本，每个写入必须被*w*个节点确认才被认为成功，而每个读取请求我们必须至少查询*r*个节点。（在我们的例子中，*n* = 3，*w* = 2，*r* = 2。）只要*w* + *r* > *n*，我们就可以在读取的时候获取最新的值，因为我们读取的*r*个节点中至少有一个必然是最新的。读取和写入遵守这个*r*和*w*值被叫做读取和写入*定额*。你可以认为*r*与*w*是读取或者写入合法的最小必要票数。

在Dynamo风格的数据库中，参数*n*，*w*和*r*通常都是可以配置的。一种常见的选择是把*n*设为奇数（通常3或者5）并设*w* = *r* = （*n* + 1） / 2 （四舍五入）。然而，你可以根据需要调整这些数字。举个例子，如果工作量是少量写入与大量读取的话，把*w*设为*n*而*r*设为1是有好处的。这使得读取更快，但是缺点是只要一个节点故障将会导致所有数据库写入都会故障。

> 注意
>
> 簇中也许会有超过*n*个节点，但是值只存储在*n*个节点。这使得数据集可以分区，从而支持比你可以放在一个节点上的数据集更大的数据集。我们会在第六章回到分区这个主题。

定额条件，*w* + *r* > *n*，使得系统对离线节点的容错能力如下：

* 如果*w* < *n*，节点离线我们仍然可以处理写入请求。

* 如果*r* < *n*，节点离线我们仍然可以处理读取请求。

* 如果有*n* = 3，*w* = 2，*r* = 2我们可以容忍一个离线节点。

* 如果有*n* = 5，*w* = 3，*r* = 3我们可以容忍两个离线节点。这种情况如图5-11所示。

一般地，读取请求与写入请求总之并行发送给所有*n*个副本。参数*w*与*r*决定了我们等待多少个节点——也就是*n*个节点中有多少需要报告成功我们才认为读取或写入是成功的。

*图5-11 如果w + r > n，那么读取的r个副本中至少有一个必然看到了最新的成功的写入*

如果可用的节点少于必要的*w*或*r*个节点，写入或者读取将返回错误。节点不可用有许多原因：因为节点离线（崩溃，断电），由于执行操作时的错误（由于磁盘已满而无法写入），由于客户端与节点之间的网络中断，或者是因为任意数量的其它原因。我们只关心节点是否返回了成功的响应，而不需要区分不同种类的故障。

### 定额一致性的局限性

如果你有*n*个副本，你选择的*w*与*r*满足*w* + *r* > *n*，一般你可以期待每一个读取都返回的是某键写入的最新的值。这是因为你写入的节点集合与你读取的节点结合必然有交集。于是，在你读取的节点中必然至少有一个节点有最新的值（如图5-11所示）。

通常，*r*与*w*选取大多数（超过*n*/2）节点，因为这样才能保证*w* + *r* > *n*同时仍可以忍受最多*n*/2节点崩溃的情况。但是定额不一定必须是多数——只要读取和写入操作使用的节点集合在至少一个节点中重叠即可。其它定额赋值是可能的，这使得分布式算法设计中有一些灵活性。

你也可以为*r*与*w*选较小的值，于是有*w* + *r* ≤ *n*（也就是定额条件不满足）。在这种情况下，读取请求与写入请求仍然会被发送到*n*个节点，但是操作成功就需要一定的成功响应数了。

如果*r*与*w*选了较小的值，你就更可能读到旧的值，因为更有可能你的读取请求并没有达到有最新值的节点。好的方面，是这样的配置环境有更低的延迟与更高的可用性：如果有网络中断并且许多副本无法到达，有很大的可能你可以继续处理读取和写入。只有在可到达的副本书分别低于*w*或*r*时才会导致数据库无法用于写入或读取。

然而，即使有*w* + *r* > *n*，仍然会有返回旧值的极端情况。这取决于实现，可能的场景包括：

* 如果用了草率的定额值（见“草率的定额值与提示交换”一节），*w*次写入的节点也许与*r*次读取的节点完全不同，所以没有办法保证*r*个节点与*w*个节点之间一定有交集。

* 如果两个写入请求并发发生，没有办法搞清楚哪个先发生。在这种情况下，最安全的解决方案是合并并发写入（见“处理写入冲突”）。如果是以时间戳为准（以最终写入为准），由于时钟偏移的原因写入会丢失。我们将在“检测并发写入”一节回到这个主题。

* 如果写入与读取并发发生，写入请求有可能只反映在了某些副本上。在这种情况下，没有办法判断读取返回的到底是旧的还是新的值。

* 如果写入请求在某些副本上成功但是在其它副本上失败（比如因为某些节点的磁盘已满），且总体成功的副本数小于*w*，写入不会在这些已成功的副本上回滚。这意味如果某个写入请求返回失败，之后的读取请求有可能会也有可能不会返回之前那次写入的值。 

* 如果存储着新值的节点故障，而后数据自一个存储着旧值的副本恢复，那么存储新值的副本数会少于*w*，从而破坏了定额条件。

* 即使所有都工作正常，还是有极端情况因为时机问题极不走运，我们会在“线性化与定额”一节看到。

因而，即使定额看起来可以保证读取请求总是返回最新写入的值，在时间中却没有那么简单。Dynamo风格的数据库通常为容忍最终一致性的使用场景而优化了。参数*w*与*r*允许你调整读取到旧值的可能性，但是并不把它们当作绝对的保证才是明智的做法。

特别的是，你通常无法得到“复制延迟的问题”一节讨论到的保证（读取自己的写入，单调读取，或者一致性前缀读取），所以之前提到的一场情况都可以在应用中发生。更强的保证一般需要事务或者共识。我们会在第七与第九章回到这个主题。

#### 监测数据老旧程度

从运营的角度上看，检测数据库是否正在返回最新的结果是很重要的。即使你的应用可以接受旧的读取请求，你需要清楚复制过程的老旧程度。如果落后太多，它应该提醒你你需要检查一下原因了（举个例子，网络问题或是某个节点拥堵）。

对于基于领机的复制，数据库一般都会暴露复制延迟的指标数据，可以把它接入监控系统。这是可能的，因为写入请求以同样的顺序应用到领机和从机，每个节点在复制日志（本地应用的写入请求次数）中都有位置信息。从领机当前的位置减去从机当前的位置，你可以测量出复制延迟的确切量。

然而，在无领机复制系统中，是应用写入请求是没有固定顺序的，这使得检测变得困难。此外，如果数据库只是用了读取修复（没有反熵），数据有多老就没有限制了——如果某个值只是偶尔被读取，那么从老旧的副本返回的值也许是已经非常久远的了。

已经有了一些关于无领机复制数据库如何测量副本老旧程度的研究，并预测老旧读取的期待百分比取决于参数*n*，*w*与*r*。不幸的是这些目前还不是常见做法，但是在数据库标准指标数据集中加上数据老旧程度的测量依然是好的。最终一致性是故意地模糊保证，但是对于运营性来说能量化“最终”是很重要的。

### 草率的定额值与提示交换

合理配置定额的数据库可以容忍节点故障而无需故障迁移。它们也可以忍受节点变慢，因为请求不必等待所有*n*个节点响应——有*w*或*r*个节点响应就可以返回了。这种特性使得无领机复制数据库看起来很适合需要高可用性和低延迟的使用场景，并且可以忍受偶尔读到旧数据。

然而，定额（截至目前介绍的）并不像看起来的那样能够容错。网络中断可以轻易的掐断客户端与一大部分数据库节点之间的链接。即使这些节点仍在线，同时其它客户端也许仍可以连上它们，对于从数据库节点断开的客户端来说，它们好像已经崩溃了。在这种情况下，这看起来像是只有不到*w*或*r*个可连接的节点了，于是客户端无法满足定额要求。

在一个大集群（有着比*n*大的多的节点数）很有可能客户端可以在网络中断时仍能链接到某些数据库节点，只是不能凑齐某个特定的定额值。在这种情况下，数据库设计者面领着这样的权衡：

* 如果不能达到*w*或者*r*个节点的定额于是对所有请求都返回错误，这样好么?

* 或者我们应该接受所有的写入请求，并把它们写入到那些可以连接到的节点而不是那些通常存取的*n*个节点？

后者被称为*草率的定额*：写入请求和读取请求仍需要*w*和*r*个成功的响应，但是这些响应可以来自不是值的指定的*n*个“主”节点的节点。打个比方，如果你把你自己锁在自家门外了，你可以去敲邻居家的门，问问可不可以临时睡它们家沙发一晚。

一旦网络中断恢复，一个节点临时代替另一个节点接受的任何写入请求被发送到合适的“主”节点。这叫做*提示交换*。（一旦你找到自家钥匙，邻居有礼貌地让你离开他家的沙发赶紧回家。）

草率的定额在提升写入可用性时特别有用：只要任意*w*节点可用，数据库旧可以接受写入请求。然而，这意味着即使在*w* + *r* > *n*的时候，你不能保证一定读到某个键的最新值，因为最新值也许被临时写到了*n*以外的某个节点。

因而，草率的定额完全不是传统意义上的定额。它只是持久性的保证，即数据保存在*w*个节点中的某处。没有保证*r*个节点的一次读取会看到它，直至提示交换完成。

草率的定额在所有一般的Dynamo实现都是可选支持的。在Riak中它默认被打开，在Cassandra与Voldemort中默认被关闭。

#### 多数据中心操作

我们之前讨论了把跨数据中心的复制作为多领机复制（见“多领机复制”一节）的一种用例。无领机复制也适用于多数据中心操作，毕竟它被设计来容忍冲突的并发写入，网络中断以及延迟峰值问题。

Cassandra与Voldemort在一般的无领机模型中实现了它们多数据库的支持：副本个数*n*包括了在所有数据中心的节点，在配置时你可以指明每个数据中心要有多少个副本。来自客户端的每一个写入请求都被发送到所有的副本，而不在乎是哪一个数据中心，但是一般客户端只等待本地数据中心足够限额节点的响应，这样可以不受数据中心之间链路的延迟与中断的影响。对其它数据中心的高延迟写入请求经常被配置为异步发生，虽然配置中是有一些灵活性的。

Riak保留所有客户端与本地数据库节点的通信，于是*n*描述的是一个数据中心内的副本数量。数据库簇之间的跨数据中心复制在后台异步发生，风格与多领机复制类似。

### 检测并发写入

Dynamo风格的数据库允许数个客户端并发写入到某个键，这意味着即使用了严格的定额还是会发生冲突。这个情况与多领机复制很类似（见“处理写入冲突”一节），虽然在Dynamo风格数据库进行读取修复或者提示交换时也会产生冲突。

问题在于事件在不同节点会以不同的顺序到达，因为网络延迟不同以及部分故障问题。举个例子，图5-12展示了两个客户端，A与B，同时对三个节点的数据存储进行键X的写入：

* 节点1收到了来自A的写入请求，但是由于暂时中断没有收到来自B的写入请求。

* 节点2首先收到来自A的写入请求，然后收到来自B的写入请求。

* 节点3首先收到来自B的写入请求，然后收到来自A的写入请求。

*图5-12 并发写入Dynamo风格的数据存储：没有定义好的顺序*

如果每个节点在收到来自客户端的写入请求时只是覆盖键对应的值，节点之间就会永远无法保持一致了，如图5-12显示的最终读取请求：节点2认为X的最终值为B，而其它节点认为值为A。

为了最终一致性，副本应该趋于同一个值。它们会怎么做呢？一种是希望复制了的数据库可以自动处理这种情况，但是不幸的是绝大部分实现都非常不好：如果要防止丢失数据，你——应用开发者——需要了解许多关于数据库冲突处理的内部知识。

我们在“处理写入冲突”一节简单了解了一些处理冲突的技巧。在本章完结之前，让我们更深入的探索一下这个问题。

#### 以最终写入为准（丢弃并发写入）

实现最终趋紧的一种方式是声明每个副本只需要存储最“近”的值并允许“旧”的值被覆盖和丢弃。这样，只要我们有某种明确决定哪个写入是最“近”的方式然后每个写入请求最终都被拷贝到每个副本，副本就将最终趋近于同一个值。

“近”这个想法实际上相当误导人。在图5-12的示例中，没有一个客户端在发送写入请求到数据库节点时知道对方的存在，所以不清楚哪一个首先发生。实际上，说任何一个“首先”发生都不太合理：我们说写入是*并发的*，所以它们之间的顺序是没有定义的。

即使写入请求没有天然的顺序，我们可以对它们强制加上特定的顺序。举个例子，我们可以为每个写入请求附上时间戳，选择数字最大的时间戳作为最“近”的并丢弃任何有着更早时间戳的写入请求。这种解决冲突的算法叫做*最终写入为准*（LWW），是Cassandra中仅支持的冲突解决办法，而在Riak中是可选功能。

LWW达成了最终趋近的目标，但是牺牲了持久性：如果对同一个键有数个并发写入，哪怕它们向客户端全部报告成功（因为它们被写入到了*w*个副本），只有一个写入能被留下来而其它的将被静静地丢弃。此外，LWW也有可能丢弃非并发地写入请求，我们会在“针对事件排序的时间戳”一节讨论它。

某些情况下，比如缓存机制，此时丢弃写入请求也许是可以接受的。如果丢弃数据是不可接受的，LWW不是一种解决冲突的好选择。

使用带LWW功能的数据库的唯一安全方式是保证键只被写入一次而之后被当作不可变的，因而防止任何对同一个键的并发更新。举个例子，一种使用Cassandra建议的方式是用UUID作为键，因而给了每一个写入操作唯一的键。

#### “在。。。之前发生”关系与并发

我们如何决定两个操作是不是并发的？我们看几个例子来锻炼一下直觉：

* 在图5-9中，两次写入请求不是并发的：A的插入在B的增加之前发生，因为B增加后的值是A插入的值。换句话说，B的操作基于A的操作，所以B的操作必定发生在A的操作之后。我们也说B是因果依赖A的。

* 另一方面，图5-12中的两次写入请求是并发的：当每个客户端开始操作时，它并不知道另一个客户端也在同一个键上执行操作。因而，两个操作之间时没有因果关系的。 

操作A在操作B之前发生指的是B知道A，或者依赖A，或者以某种方式基于A。一个操作是不是发生在另一个操作之前是定义并发含义的关键。事实上，只要没有任何一个操作发生在另一个操作之前我们旧可以说两个操作是并发的（也就是，没有一个知道对方的存在）。

因而，每当有两个操作A和B，就有三种可能：要么A发生在B之前，要么B发生在A之前，要么A与B是并发的。我们需要一个算法来告诉我们两个操作是否是并发的。如果一个操作在另一个操作之前发生，后一个操作应当覆盖前一个操作，但是如果操作是并发的，我们需要解决冲突了。

> **并发，时间与相对性**
>
> 看上去如果两个操作“同时”发生就应该被叫做并发——但是事实上，时间上是否重叠并不重要。由于分布式系统中的时钟问题，事实上很难分辨两件事是否刚刚好发生在同一时刻——这个问题我们会在第八章具体讨论。
>
> 对于定义并发，与准确的时间无关：只要两个操作不知道彼此我们就把它们叫做并发，完全不在乎它们发生的具体时间。人们有时把这个原则联系到物理学中的相对论，它引入了信息传递的速度无法超越光速的概念。所以，发生在相聚不远位置的两个事件，如果事件时间差比光在两地传递的事件还要短的话，是无法影响彼此的。
>
> 在计算机系统中，即使原则上光速允许一个操作影响另外一个操作，两个操作仍是并发的。举个例子，如果网络很慢或是当前中断了，两个操作可以相隔一段事件发生但仍是并发的，因为网络问题阻碍了一个操作知道另外一个操作的可能性。

#### 捕捉“在。。。之前发生”关系

让我们来看一个判断两个操作是否并发，或者一个在另一个之前发生的算法。简单起见，我们从数据库只有一个副本开始。一旦我们明白了在单个副本上是如何做到的，我们可以把这种方式一般化到拥有多个副本的无领机数据库。

图5-13展示了两个客户端并发地添加物品到同一个购物车中。（如果这个例子对于你来说太空洞了，那就假设两个空中交通管制中心同时添加飞行器到它们追踪的空域。）最初，购物车是空的。两个客户端对服务器发起了五次写入请求：

1. 客户端1添加`milk`到购物车。这是对键的首次写入，所以服务器成功保存了它，并且设为版本1.服务器也把之传回给客户端，还有版本号。

2. 客户端2添加`eggs`到购物车，不知道客户端1同时添加了`milk`（客户端以为`eggs`是购物车中唯一的物品）。服务器把这次写入设为版本2，并且把`eggs`和`milk`存储为两个独立的值。然后向客户端返回这两个值，还有版本号2。

3. 客户端1，对客户端2的写入毫不知晓，想要添加`flour`到购物车，并认为当前购物车内的物品有`[milk, flour]`。它发送这个值到服务器，还有之前服务器给客户端1的版本号1.服务器通过版本号知道写入`[milk, flour]`将取代先前的值`[milk]`但是这与`[eggs]`是并发的。因而，服务器设`[milk, flour]`为版本3，覆盖了版本1的值`[milk]`，但是保留了版本2的值`[eggs]`并返回了两个余下的值给客户端。

4. 同时，客户端2想要添加`ham`到购物车，不知道客户端1刚刚添加了`flour`。客户端2从最后一次客户端响应收到了两个值`[milk]`和`[eggs]`，所以客户端现在合并它们并且加入了`ham`形成一个新的值，`[eggs, milk, ham]`。它把这个值发送到服务器，还有之前的版本号2。服务器检测到了版本2覆盖了`[eggs]`但是与`[milk, flour]`是并发的，于是两个剩下的值是版本3`[milk, flour]`，以及版本4`[eggs, milk, ham]`。

5. 最终，客户端1想要添加`bacon`。它之前收到了来自服务器的版本3`[milk, flour]`和`[eggs]`，于是它合并了这些，添加了`bacon`，然后发送了最终值`[milk, flour, eggs, bacon]`到服务器，还有版本号3。它覆盖了`[milk, flour]`（注意`[eggs]`已经在上一步被覆盖了）但是与`[eggs, milk, ham]`并发，所以服务器保留了这两个并发值。

*图5-13 捕捉两个客户端并发编辑购物车时之间的因果关系*

图5-13中操作之间的数据流如图5-14所示。箭头表示哪一个操作在另外哪一个操作之前发生，某种意义上后来的操作知道或者依赖前一个操作。在这个例子中，客户端从来没有与服务器上的数据完全同步过，因为总是有并发的另外一个操作发生。但是旧版本的值最终确实被覆盖了，没有写入请求被丢掉。

*图5-14 图5-13中的因果关系图*

值得注意的是服务器可以通过查看版本号判断两个操作是否并发——它不用去判断值本身（所以值可以是任意数据结构）。算法是这样工作的：

* 服务器为每个键维护一个版本号，每一次键被写入时版本号增加1，并且新的版本号与新写入的值一起保存。

* 当客户端读取键时，服务器返回所有没有被覆盖的值，以及最新的版本号。客户端必须在写入之前先读取键。

* 当客户端写入键时，它必须带上前一次读取的版本号，还必须合并前一次读取到的所有值。（写入请求的响应类似一次读取，它返回所有当前的值，这使得我们可以把几个写入请求链起来，就像购物车的例子一样。）

* 当服务器收到了有特定版本号的写入请求时，他可以覆盖所有有着那个版本号以及更老版本号的值（因为它知道这些值已经被合并入新值了），但是它必须用一个更高的版本号保存所有值（因为这些值与即将到来的写入是并行关系）。

当写入请求包含着前一次读取的版本号时，这告诉了我们写入是基于先前哪个状态的。如果你要发起一个没有版本号的写入请求，它会与所有其它写入请求并发，所以他不会覆盖任何东西——它会被当作后续读取的值之一被返回回来。

#### 合并并发写入的值

这个算法保证了没有数据会被静默地丢弃，但是不幸的是它要求客户端做了一些额外工作：如果几个操作并发发生，客户端需要做一些清理工作，合并并发写入的值。Riak把这些并发的值叫做*siblings*。

合并siblings值本质上于多领机复制中冲突的解决是同一个问题，我们之前讨论过（见“处理写入冲突”）。一种简单的方法是基于版本号或者时间戳选出其中一个值（以最终写入为准），但是这以为着数据丢失。所以，在应用程序代码中你需要做一些更聪明的事。在购物车例子中，一种合并siblings的合理方式是把值联合起来。在图5-14中，两个最终的siblings是`[milk, flour, eggs, bacon]`和`[eggs, milk, ham]`；值得注意的是`milk`和`eggs`同时出现在两个值中，尽管它们只被写入了一次。合并后的值也可能是`[milk, flour, eggs, bacon, ham]`，完全没有重复值。

然而，如果你要允许人们也可以从购物车中删除物品，而不只是添加东西，那么选择联合siblings值就无法得到正确的值了：如果你合并两个siblings购物车而一个物品只从二者之一删除了，那么被删掉的物品会重新出现在siblings联合中。为了防止这样的问题，一个物品被移除时不能简单的从数据库中删除；而应该是在合并siblings时，系统留下一个合适版本号的标记来指示该物品已经被移除了。这样的删除标记被叫做*墓碑*。（我们之前在“哈希索引”一节的日志压缩中见过墓碑）。

由于在应用程序代码中合并siblings很复杂也很容易出错，现在已经设计出了一些可以自动执行合并的数据结构，如“自动解决冲突”一节讨论到的。举个例子，Riak的数据类型支持使用一系列CRDT的数据结构合理地自动合并siblings，包括保留删除。

#### 版本向量

图5-13中的例子只用到了单个副本。当有多个副本但是没有领机的时候它会怎么变化呢？

图5-13shi用了单个版本号来捕捉操作之间的依赖关系，但是如果有多个副本并发接受写入的话就不够了。取而代之的应该是*每个副本*每个键都有版本号。每个副本处理写入请求增加自己的版本号，同时也记录来自其它副本它看到的版本号。这个信息指明哪些值要复写，哪些值要留下作为siblings。

来自所有副本的版本号集合叫做*版本向量*。也有用一些类似的理念，但是最有趣的大概就是TODO: dotted version vector，它被用在Riak 2.0中。我们不会去深入细节，但是它工作的方式与我们在购物车示例中看到的很像。

就像图5-13中的版本号一样，当值被读取时版本向量从数据库副本发送到客户端，而新值稍后被写入时需要再发回到数据库。（Riak把版本向量编码为字符串，叫它*因果上下文*。）版本向量使得数据库可以区别覆盖写入与并发写入。

并且，就像单副本例子中那样，应用程序需要合并siblings。版本向量结构保证了从一个副本读取然后写回到另一个副本时是安全的。这样做会导致创建新的siblings，但是只要正确合并数据是不会丢失的。

> 版本向量与向量时钟
>
> 版本向量有时也被叫做向量时钟，虽然它们并不完全是一回事。区别很细微——细节青查看引用【57，60，61】。简而言之，在比较副本状态的时候，应该使用版本向量。

## 总结

In this chapter we looked at the issue of replication. Replication can serve several purposes: 

High availability 

Keeping the system running, even when one machine (or several machines, or an entire datacenter) goes down 

Disconnected operation 

Allowing an application to continue working when there is a network interruption 

Latency 

Placing data geographically close to users, so that users can interact with it faster 

Scalability 

Being able to handle a higher volume of reads than a single machine could handle, by performing reads on replicas 

Despite being a simple goal — keeping a copy of the same data on several machines — replication turns out to be a remarkably tricky problem. It requires carefully thinking about concurrency and about all the things that can go wrong, and dealing with the consequences of those faults. At a minimum, we need to deal with unavailable nodes and network interruptions (and that’s not even considering the more insidious kinds of fault, such as silent data corruption due to software bugs). 

We discussed three main approaches to replication: 

Single-leader replication 

Clients send all writes to a single node (the leader), which sends a stream of data change events to the other replicas (followers). Reads can be performed on any replica, but reads from followers might be stale. 

Multi-leader replication 

Clients send each write to one of several leader nodes, any of which can accept writes. The leaders send streams of data change events to each other and to any follower nodes. 

Leaderless replication 

Clients send each write to several nodes, and read from several nodes in parallel in order to detect and correct nodes with stale data.

Each approach has advantages and disadvantages. Single-leader replication is popular because it is fairly easy to understand and there is no conflict resolution to worry about. Multi-leader and leaderless replication can be more robust in the presence of faulty nodes, network interruptions, and latency spikes — at the cost of being harder to reason about and providing only very weak consistency guarantees. 

Replication can be synchronous or asynchronous, which has a profound effect on the system behavior when there is a fault. Although asynchronous replication can be fast when the system is running smoothly, it’s important to figure out what happens when replication lag increases and servers fail. If a leader fails and you promote an asynchronously updated follower to be the new leader, recently committed data may be lost. 

We looked at some strange effects that can be caused by replication lag, and we discussed a few consistency models which are helpful for deciding how an application should behave under replication lag: 

Read-after-write consistency 

Users should always see data that they submitted themselves. 

Monotonic reads 

After users have seen the data at one point in time, they shouldn’t later see the data from some earlier point in time. 

Consistent prefix reads 

Users should see the data in a state that makes causal sense: for example, seeing a question and its reply in the correct order. 

Finally, we discussed the concurrency issues that are inherent in multi-leader and leaderless replication approaches: because they allow multiple writes to happen concurrently, conflicts may occur. We examined an algorithm that a database might use to determine whether one operation happened before another, or whether they happened concurrently. We also touched on methods for resolving conflicts by merging together concurrent updates. 

In the next chapter we will continue looking at data that is distributed across multiple machines, through the counterpart of replication: splitting a large dataset into partitions.
