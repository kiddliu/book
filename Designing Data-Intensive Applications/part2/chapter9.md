# 第九章 一致性与共识

*活着但是是错的与正确但是死掉了，哪个更好？*

杰·克莱普斯，*关于Kafka与Jepsen的几个注意事项*（2013）

---

正如第8章所讨论的，分布式系统中许多事情会出错。处理这些故障最简单方法就是直接让整个服务失效，并向用户显示错误消息。如果这个解决方案不可接受，我们需要找到容错的方法——也就是即使某些内部组件出现故障，也可以保持服务正常运行。

在这一章里，我们将讨论构建可容错分布式系统算法与协议的一些例子。我们将假设第8章中的所有问题都会发生：网络中的数据包会丢失，会被重新排序，会重复或任意地延迟; 时钟最多是近似的; 并且节点可以暂停（例如，由于垃圾收集）或者随时崩溃。

构建容错系统的最佳方式是找到一些具有有用保证的通用的抽象概念，实现它们，然后让应用程序依赖这些保证。 这与我们在第7章中用于事务的方法相同：通过使用事务，应用程序可以假装没有崩溃（原子性），没有其他人同时访问数据库（隔离性），并且存储设备是完全可以依赖的（耐用性）。即使发生崩溃，竞态条件和磁盘故障确实会发生，但是事务抽象隐藏了这些问题所以应用程序不需要担心它们。

我们现在将继续沿着同样的路线前进，尝试寻找能够让应用程序忽略分布式系统部分问题的抽象概念。例如，分布式系统最重要的抽象之一就是共识：也就是说，让所有的节点都能达成一致。我们将在本章中看到，尽管存在网络故障和进程故障，可靠地达成一致是个令人惊讶的棘手问题。

一旦实现了协商一致，应用程序可以将其用于各种目的。例如，假设您有一个单主机复制的数据库。如果主机失效而您需要故障迁移到另一个节点，那么其余的数据库节点可以使用协商一致来选出新的主机。正如在“处理节点中断”中所讨论的，重要的是只有一个主机，并且所有节点都同意谁是主机。如果两个节点都认为自己是主机，那么这种情况就叫做裂脑，并且常常导致数据丢失。正确实现协商一致才可以避免此类问题。

在本章的后面，我们将在“分布式事务与协商一致”一节中研究解决协商一致以及相关问题的算法。但是首先，我们需要探索在分布式系统中可以提供的各种保证和抽象概念。

我们需要了解能做什么不能做什么的范围：在某些情况下，系统是可以容忍错误并继续工作的；在其他情况下，这就不可能了。在理论证明和实际实现中已经深入探讨了什么可能什么不可能的极限。我们将在这一章里概述这些基本限制。

分布式系统领域的研究人员几十年来一直在研究这些问题，因此有大量的材料——我们只会接触一些表面问题。在这本书中，我们没有空间去深入到正式模型与证明的细节，所以我们将坚持非正式的直觉。如果你感兴趣的话，这些参考文献提供了足够多的深度讨论。

## 一致性保证

在“复制延迟问题”一节中，我们研究了复制了的数据库中出现的一些计时问题。如果同时查看两个数据库节点，你很可能会在两个节点上看到不同的数据，因为写入请求在不同的时间到达不同的节点。无论数据库使用哪种复制方法（单主机、多主机或无主机复制），都会发生这些不一致的情况。

大多数复制了的数据库至少提供了最终一致性，这意味着如果停止对数据库的写入并等待一段时间，那么最终所有读取请求都会返回相同的值。换句话说，不一致是暂时的，最终是可以自己解决的（假设任何的网络故障也最终被修复了）。最终一致性的一个更好的名称可能是收敛，因为我们期望所有的副本最终收敛到相同的值。

然而，这是一个很弱的保证——它没有提到副本什么时候会收敛。在收敛之前，读取可以返回任何内容或什么都不返回。例如，如果写入一个值然后马上读取它，则无法保证你会看到刚才写入的值，因为读取请求可能被路由到另一个副本（见“读取你自己的写入”一节）。

对于应用程序开发人员来说最终一致性是很难，因为它与普通单线程程序中变量的行为非常不同。如果给变量赋值然后立即读取它，你不会期望读到旧值，或者读取失败。数据库表面上看起来像一个可以读写的变量，但实际上它有更复杂的语义。

当使用只提供弱保证的数据库时，您需要一直意识到它的局限性，而不是意外地假设太多。Bug通常很微妙，很难通过测试找到，因为大部分时间应用程序都能正常工作。最终一致性的边缘情况只有在系统中出现故障（例如网络中断）或高并发时才会变得明显。

在这一章里我们将探讨数据系统会选择提供的更强的一致性模型。它们并不是免费的：与有较弱保证的系统相比，具有更强保证的系统性能会更差，容错性会更低。尽管如此，因为更方便正确地使用，更有力的担保是有吸引力的。一旦您了解了几个不同的一致性模型，您就可以更好地决定哪一个最适合您的需要。

分布式一致性模型与我们前面讨论过的事务隔离级别的层次结构有一些相似之处（见“弱隔离级别”一节）。但是虽然有一些重叠之处，但它们绝大部分是独立的：事务隔离主要是为了避免由于并发执行事务而产生的竞争条件，而分布式一致性则主要是在遇到延迟和错误时协调副本的状态。

这一章涵盖了广泛的主题，但正如我们将看到的，这些领域之间实际上有着深刻的联系：

* 我们会首先研究其中一个最常用的最强一致性模型，线性化模式，并研究其利与弊。

* 然后我们将研究分布式系统中的事件排序问题（“排序保证”），特别是围绕因果关系和完全排序的问题。

* 在第三部分（“分布式事务和协商一致”）中，我们将探讨如何原子性地提交分布式事务，这将最终为协商一致问题找到解决方案。

## 线性化

在最终一致的数据库中，如果同时问两个不同的副本相同的问题，你可能会得到两个不同的答案。这太让人困惑了。如果数据库能给人一种错觉，以为只有一个副本（即只有一个数据副本），那岂不是简单得多吗？这样，每个客户端都将拥有相同的数据视图，而不必担心复制滞后。

这就是线性化（也称为*原子一致性*[7]、*强一致性*、*即时一致性*或*外部一致性*）背后的思想。线性化的确切定义是非常微妙的，我们会在这一节剩下的部分中对其进行探讨。但是基本的思想是让系统看起来好像只有一个数据的副本，并且对它的所有操作都是原子的。有了这个保证，即使现实中会有多个副本，应用程序也不需要担心它们。

在一个可线性化的系统中，一旦一个客户端成功地完成了一次写入，所有正在从数据库读取的客户端都必须能够看到刚刚写入的值。保持单个数据副本的错觉意味着确保读取的值是最近更新的值，而不是来自陈旧的缓存或副本。换句话说，线性化是新近性的保证。为了澄清这个想法，让我们看一个非线性化的系统的例子。

*图9-1 系统是非线性化的，这让球迷非常的困惑*

图9-1展示了一个非线性体育网站的例子。爱丽丝和鲍勃坐在同一个房间里，两人都在手机上查看2014年世界杯决赛的结果。最后的比分刚刚公布以后，爱丽丝刷新了页面，看到宣布了获胜者，兴奋地告诉了鲍勃。鲍勃疑惑地点击了手机上的重新加载按钮，然而他的请求转到了一个滞后的数据库副本，于是他的手机显示比赛仍在进行。

如果爱丽丝和鲍勃同时点击重新加载按钮，得到两个不同的查询结果就不会那么令人惊讶了，因为他们不知道服务器处理各自请求的确切时间。然而，鲍勃知道他在听到爱丽丝惊呼最后的分数后按了重新加载按钮（启动了他的查询），因此他希望他的查询结果至少和爱丽丝的一样新。他的查询返回了一个旧结果的事实违反了线性化。

### 是什么让系统线性化的？

线性化背后的基本思想很简单：使系统看起来好像只有一个数据副本。然而，准确地确定这意味着什么，实际上要很小心。为了更好地理解线性化，让我们看更多的例子。

图9-2展示了三个客户端同时在线性化数据库中读写相同的键*x*。在分布式系统文献中，*x*被称为*寄存器*——在实践中，它可以是键值对存储中的一个键，关系型数据库中的一行，或者文档型数据库中的一个文档。

*图9-2 如果读取请求与写入请求并发，它要么返回新值，要么是旧值。*

为了简单起见，图9-2只展示了客户端角度的请求，而不是数据库的内部。每个条形框是一个客户端发出的请求，条形框的起始位置是发送请求的时间，终止位置是客户端接收响应的时间。由于可变的网络延迟，客户端不知道数据库什么时候会处理它的请求——它只知道发生的时间一定在客户端发送请求之后与接收响应之间。

在本例中，寄存器有两种类型的操作：

* *read(x) ⇒ v*表示客户端请求读取寄存器*x*的值，数据库返回了值*v*。

* *write(x, v) ⇒ r*表示客户端请求将寄存器*x*设置为值*v*，数据库返回响应*r*（可以是*正常*或*错误*）。

在图9-2中，x的值初始为0，客户端C执行写入请求将其设置为1。当这种情况发生时，客户端A和B在反复轮询数据库以读取最新值。A和B在它们的读取请求中可能得到什么响应？

* 客户端A的第一个读取操作在写入开始之前完成，因此它必然返回旧值0。

* 客户端A的最后一次读取是在写入完成后开始的，因此，如果数据库是可线性的，则必须返回新的值1：我们知道，数据库写入必然是在写入操作开始和结束之间的某个时间进行的，读取数据库必然是在读取操作开始和结束之间的某个时间进行的。如果读取是在写入结束后开始的，那么读取的处理必然是在写入之后的，因此它必然看到写入的新值。

* 任何与写入操作重叠的读取操作都可能返回0或1，因为我们不知道在处理读取操作时写入是否已经生效。这些操作与写入是并行的。

但是，这还不足以充分地描述线性化：如果与写入并发的读取操作可以返回旧值也可以返回新值，那么读者在写入过程中可以多次看到旧值和新值之间的来回翻转。这不是我们所期望的、可以模拟“单一数据副本”的系统。

为了使系统可线性化，我们需要添加另一个约束，如图9-3所示。

*图9-3 在任何一个读取操作返回新值之后，之后所有（在相同或其他客户端上的）读取操作也必须返回新值。*

在线性化系统中我们设想必然有某个时间点（在写入操作的开始和结束之间）*x*的值从0原子翻转为1。因此，如果一个客户端的读取操作返回了新的值1，那么所有后续读取操作也必须返回新的值，即使写入操作还没有完成。

图9-3中的箭头展示了这种时间依赖性。客户端A是第一个读到新值的，1。在A的读取请求返回之后，B开始了新的读取操作。由于B的读取操作严格地发生在A的读取操作之后，所以它也必须返回1，即使C的写入仍在进行之中。（这与图9-1中的爱丽丝和鲍勃的情况相同：爱丽丝读取了新值之后，鲍勃也希望读取到新值。）

我们可以进一步细化这个时间图，从而可视化每个发生在某个时间点的，原子性生效的操作。图9-4显示了一个更复杂的例子。

在图9-4中我们添加了读取与写入之外的第三种操作：

* *cas(x, v<sub>old</sub>, v<sub>new</sub>） ⇒ r*意味着客户端请求了原子性的比较后设置操作（见“比较后设置”一节）。如果寄存器x的当前值等于*v<sub>old</sub>*，则应该原子地将其设置为*v<sub>new</sub>*。如果*x ≠ v<sub>old</sub>*，那么操作应该保持寄存器不变，并返回一个错误。*r*是数据库的响应（*正常*或*错误*）。

在我们认为操作被执行的时侯，图9-4中的每个操作都用垂直线（每个操作的条形框内部）标记。这些标记按顺序连接，结果对于寄存器来说必须是有效的读写顺序（每个读取操作必须返回由最近一次写入设置的值）。

线性化的要求是连接操作标记的线总是在时间上向前移动（从左到右），而不是向后移动。这个要求确保了我们前面讨论过的新近性保证：一旦一个新的值被写入或读取，所有后续的读取都会看到被写入的值，直到它再次被覆写。

*图9-4 可视化读写生效的时间点。B的最终读取操作不是线性化的。*

图9-4中一些有趣的细节需要指出：

* 首先客户端B发送读取*x*的请求，然后客户端D发送将*x*设置为0的请求，然后客户端A发送将x设置为1的请求。然而，返回给B的读值是1（由A写的值）。这是正常的：这意味着数据库首先处理了D的写入请求，然后是A的写入请求，而最后是B的读取请求。虽然这不是请求发送的顺序，但是这是一个可接受的顺序，因为这三个请求是并发的。也许B的读取请求在网络中稍微延迟了一些，所以在两次写入完成之后才到达数据库。

* 客户端B的读取请求在客户端A从数据库接收到响应之前返回1，说明值1的写入是成功的。这也是正常的：这并不意味着值是在写入之前读取的，它只是意味着从数据库到客户端A的*正常*响应在网络中稍微延迟了。

* 此模型不假定任何事务隔离：另一个客户端会随时更改值。例如，C首先读取1，然后读取2，因为B在两个读取请求之间更改了值。原子性的比较后设置（CAS）操作可以用于检查没有被另一个客户端更改的值：B和C的*比较后设置*请求成功，但D的*比较后设置*请求失败了（当数据库处理它时，*x*的值不再为0）

* （在阴影条形栏中的）客户端B的最后一次读取操作不是线性化的。这个操作与C的*比较后设置*写入并行，它把*x*从2更新到4。在没有其他请求的情况下，B的读取返回2是正常的。然而，客户端A在B的读取开始之前已经读取了新值4，所以B不允许读取比A更老的值。这与图9-1中的爱丽丝和鲍勃的情况还是一样的。

这就是线性化背后直觉的知识；正式的定义更精确地描述了它。通过记录所有请求和响应的时间，并检查它们是否可以排列成有效的顺序[11]，可以测试系统的行为是不是线性化的（尽管计算的成本很高）。

> 线性化 vs 串行化
>
> 线性化很容易与串行化混淆（参见“串行化”），因为这两个词似乎都意味着“可以按顺序排列”。然而，它们是两种完全不同的保证，必须加以区分：
>
> *串行化*
>
> 串行化是事务的隔离属性，每个事务都可以读写多个对象（行、文档、记录）——见“单对象与多对象操作”一节。它保证事务的行为与它们按照某种串行顺序执行的行为相同（每个事务在下一个事务启动之前执行完毕）。这个串行顺序与实际执行事务的顺序不同是可以的。
>
> *线性化*
>
> 线性化是寄存器（单个对象）读写的新近性保证。它不会将操作分组到事务中，因此不会防止诸如写偏（见“写偏和幻影”一节）等问题，除非采取了其他措施，例如物化冲突（参见“物化冲突”）。
>
> 数据库可以同时提供可串行化和线性化，这种组合被称为*严格串行化*或*强单拷贝串行化*（strong-1SR）[4，13]。基于两阶段锁定（见“两阶段锁定（2PL）”一节）或实际串行执行（参见“实际串行执行”）的串行化实现通常是线性化的。
>
> 然而，串行化快照隔离（见“串行化快照隔离（SSI）”一节）不是线性化的：根据设计，它从一致快照进行读取，以避免读取器和写入器之间的锁定争用。一致快照的全部要点是，它不包括比快照更近期的写入，因此从快照中读取的内容不是线性化的。

### 依赖线性化

在什么情况下线性化是有用的？查看一场体育比赛的最后比分也许是一个轻率的例子：在这种情况下，比赛结果晚了几秒更新不太可能造成任何真正的伤害。然而，在一些领域线性化是使系统正常工作的重要条件。

#### 锁定与主机选举

使用单主机复制的系统需要确保确实只有一个主机，而不是多个主机（裂脑）。选举主机的一种方法是使用锁：每一个节点启动的时候都试图获得锁，成功的节点将成为主机。不管这个锁是如何实现的，它必须是线性化的：所有节点都必须同意哪个节点拥有锁；不然没用。

像Apache ZooKeyer和etcd这样的协调服务通常用于实现分布式的锁与主机选举。他们使用协商一致算法以容错方式实现线性化操作（我们在本章后面的“容错协商一致”一节中讨论了这些算法）。正确实现锁和主机选举仍然有许多微妙的细节（见比如“主机与锁”中的围栏问题），而诸如Apache Curator这样的库，通过提供基于ZooKeeper的高级解决方案起到帮助作用。然而，线性化的存储服务是这些协调任务的基础。

分布式的锁定也用在一些分布式数据库中的更高粒度级别，例如Oracle Real Application Clusters （RAC）。由于多个节点共享对同一磁盘存储系统的访问，RAC在每个磁盘页上都用到锁。由于这些线性化的锁位于事务执行的关键路径上，因此RAC的部署通常有一个专用的集群互连网络，用于数据库节点之间的通信。

#### 约束与唯一性保证

唯一性约束在数据库中是很常见的：例如，用户名或电子邮件地址必须唯一地标识一个用户，而在文件存储服务中，不可能有两个路径和文件名相同的文件。如果要在写入数据时强制执行这个约束（如果两个人试图同时用同样的名字创建用户或者文件，其中一个将被返回错误），你需要线性化。

这种情况实际上与锁类似：当用户注册你的服务时，您可以想象成他们获得了他们选择的用户名上的“锁”。这个操作也非常类似原子性的比较后设置，如果用户名没有被占用，就把这个用户名设为申请该用户名用户的ID。

如果你想确保银行账户余额不会为负数，或者你不会卖出比仓库库存更多的物品，或者两个人不会同时在飞机上或剧院里预订相同的座位，就会出现类似的问题。这些约束都要求有一个所有节点都同意的最新值（帐户余额、库存水平、座位是否占用）。

在实际应用中，宽泛地对待这些约束有时是可以接受的（例如，如果航班超订，你可以把客人转移至不同的航班，并为带来的不便之处向他们进行赔偿）。在这种情况下，线性化也许不需要，我们会在“及时性和完整性”一节讨论这种的被宽泛解读的约束。

但是，硬唯一性约束，例如通常在关系型数据库中找到的那种，需要线性化。其他类型的约束，例如外键或属性约束，不需要线性化的情况下就可以实现。

#### 跨通道时序依赖

注意图9-1中的一个细节：如果爱丽丝没有报出分数，鲍勃就不会知道他的查询结果已经过时了。几秒钟后，他就会再次刷新页面，并最终看到了最后的比分。只有在系统中有另一条通信通道（爱丽丝的声音对到鲍勃的耳朵），才会注意到这种线性化违规。

计算机系统中也可能出现类似的情况。例如，假设你有一个网站，用户可以上传照片，背景进程调整照片大小到较低分辨率以便更快地下载（缩略图）。该系统的架构和数据流如图9-5所示.

需要显式地指示图像编辑器执行调整大小的任务，并且通过消息队列将此指令从Web服务器发送到编辑器（见第11章）。Web服务器不会将整个照片放在队列中，因为大多数消息代理都是专为小消息设计的，而一张照片的大小可能是几兆字节。取而代之的是，照片首先被写到文件存储服务中，一旦写入完成，给编辑器的指令就会放在队列上。

*图9-5 Web服务器和图像编辑器通过文件存储和消息队列进行通信，从而打开了竞争条件的潜在可能性。*

如果文件存储服务是线性化的，那么这个系统应该可以正常工作。如果消息队列不是线性化的，就有存在竞争条件的风险：消息队列（图9-5中的步骤3和步骤4）可能比存储服务中的内部复制的速度更快。在这种情况下，当编辑器获取图像（步骤5）时，它可能会看到图像的旧版本，或者什么也看不到。如果它处理图像的旧版本，则文件存储中的全尺寸的图片和大小调整后的图片就不一致了。

这个问题之所以出现，是因为Web服务器和编辑器之间有两个不同的通信通道：文件存储和消息队列。没有新近性线性化的保证，两个信道之间的竞争条件是可能的。这种情况类似于图9-1，图中两个通信通道之间也存在竞争条件：数据库复制，以及真实世界里爱丽丝嘴巴到鲍勃耳朵之间的音频通道。

线性化并不是避免这种竞赛条件的唯一方法，却是最容易理解的方式。如果你控制着多余的那条通信通道(比如消息队列的情况，但不是爱丽丝和鲍勃的情况)，你可以使用与我们在“读取自己的写操作”中讨论的类似替代方法，代价是额外的复杂性。

### 实现线性化系统

现在既然我们已经看了几个线性化有用的例子，那么让我们考虑一下如何实现提供线性化语义的系统吧。

由于线性化本质上意味着“表现地就像只有一份数据拷贝，而且对它的所有操作都是原子性的”，那么最简单的答案就是真的只用一份数据拷贝。但是，这种方法将没有办法容错：如果保存该副本的节点失效，数据将丢失，或者至少在节点再次上线之前是无法访问的。

使系统可以容错的最常见方法是使用复制。让我们重新回顾第5章中提到的复制方法，然后比较它们是否可以线性化：

*单主机复制（有可能可以线性化）*

在有着单主机复制的系统中（见“主机与从机”一节），主机拥有用于写入的数据的主副本，而从机在其他节点上维护数据的备份副本。如果您从主机或同步更新的从机中读取数据，那么它们有可能是线性化的。然而，并不是每个单主机数据库实际上都是线性化的，要么是因为设计（比如因为它使用快照隔离），要么是由于并发bug。

使用主机完成读取请求依赖于这样一个假设——你确实知道哪个是主机。正如在“真理由多数人定义”一节中所讨论的那样，节点很有可能认为它是主机，而实际上并非如此——如果妄想的主机继续响应请求，就很可能违反了线性化。使用异步复制，故障转移甚至可能会丢失已经提交的写入操作（见“处理节点离线”一节），这既违反了持久性也违反了线性化。

*协商一致算法（线性化的）*

一些我们将在本章稍后讨论到的协商一致算法，与单主机复制相似。然而，协商一致协议包含了防止裂脑和陈旧复本的措施。正是由于这些细节，协商一致算法可以安全地实现线性化存储。这就是例如ZooKeeper以及etcd的工作方式。

*单主机复制（非线性化的）*

具有多主机复制的系统通常是非线性化的，因为它们同时处理多个节点的写入请求，并异步地将它们复制到其他节点。由于这个原因，它们会产生需要解决的冲突写入请求（见“处理写入冲突”一节）。这种冲突是缺少数据单个拷贝的产物。

*无主机复制（大概不可以线性化）*

对于无主机复制的系统（Dynamo风格；见“无主机复制”一节），人们有时会说通过要求仲裁读写（*w + r > n*）你可以获得“强一致性”。取决于仲裁的确切配置，以及如何定义强一致性，这并不完全正确。

基于现世时钟的“以最后一次写入为准”的冲突解决方法（例如在Cassandra中；见“依赖同步时钟”一节）几乎肯定是非线性化的，因为时钟时间戳不能保证与由于时钟偏斜导致的实际事件顺序一致。草率的仲裁(“草率的定额值与提示交换”一节)也破坏了任何实现线性化的机会。即使有严格的仲裁，还是有可能出现非线性化行为，如下一节所示。

#### 线性化和仲裁

从直觉上看，似乎严格的仲裁读写应该在Dynamo风格的模型中是线性化的。然而，当我们有可变的网络延迟时，就有可能有竞争条件，如图9-6所示。

*图9-6 非线性化的执行，尽管使用了严格的仲裁。*

在图9-6中，*x*的初始值是0，并且写入器客户端通过将写入请求发送到所有三个副本（*n*＝3，*w*＝3）把*x*更新到1。同时，客户端A从两个节点的仲裁团中读取（*r* = 2），并且在其中一个节点上看到新值1。同时在写入过程中，客户端B从两个节点构成的另一个仲裁团中读取，并且从这两个节点取回旧值0。

仲裁条件（*w + r > n*）是满足了，但这种执行仍然不是线性化的：B的请求是在A的请求完成后开始的，但是B返回旧值的同时A返回了新值。（这又是图9-1中的爱丽丝和鲍勃的情况。）

有趣的是，可以在牺牲性能的情况下使Dynamo风格的仲裁线性化：在将结果返回到应用程序之前，读取器必须同步执行读修复（见“读取修复与反熵”一节），而编写器必须在发送其写入请求之前读取仲裁团节点的最新状态。然而由于性能损失，Riak不执行同步的读取修复。Cassandra确实在仲裁读取时等待读取修复完成，但是如果对相同的键有多个并发写入，就会失去线性化，因为它使用了以最后一次写入为准的冲突解决方案。

此外，只有线性化的读写操作才能以这种方式实现；线性化的比较后设置操作不能实现，因为它需要协商一致的算法。

总之，最安全的假设是具有Dynamo风格复制的无主机系统不提供线性化。

#### 线性化的代价

正是由于一些复制方法可以提供线性化而另一些不能，因此更深入地探讨线性化的利弊是很有意思的。

我们已经在第5章中讨论了不同复制方法的一些用例；例如，我们看到多主机复制通常是多数据中心复制的一个很好的选择(见“多数据中心操作”一节)。图9-7展示了这样一个部署的例子。

*图9-7 网络中断强制在线性化与可用性之间选择。*

考虑一下如果两个数据中心之间网络中断了会发生什么情况。让我们假设每个数据中心内的网络都在工作，客户端可以访问数据中心，但是数据中心之间不能相互连接。

使用多主机数据库，每个数据中心可以继续正常工作：由于来自一个数据中心的写入请求被异步地复制到另一个数据中心，因此在网络连接恢复的时侯，只需排起队来等待交换写入操作。

另一方面，如果使用单主机复制，那么主机必然位于其中一个数据中心。任何写入和任何可线性读取都必须发送给主机——因此，对于任何连接到从机数据中心的客户端，这些读写请求必须同步地通过网络发送到主机数据中心。

如果数据中心之间的网络在单主机设置下中断了，连接到从机数据中心的客户端无法与主机取得联系，因此他们不能对数据库发起任何写入请求，也不能发起任何线性化读取请求。他们仍然可以向从机发起读取请求，但数据可能是陈旧的（非线性化的）。如果应用程序需要线性化的读写，那么网络中断将导致应用程序在无法与主机取得联系的数据中心中不可用。

如果客户端可以直接连接到主机的数据中心，这并不是一个问题，因为应用程序继续在那里正常工作。但是，只能访问从机数据中心的客户端将经历断线，直到（数据中心之间的）网络链接被修复为止。

#### CAP定理

这个问题不仅仅是单主机和多主机复制的结果：任何线性化数据库都有这个问题，不管它是如何实现的。这个问题也不限于多数据中心部署，它可能发生在任何不可靠的网络上，甚至在一个数据中心内。权衡如下：

* 如果你的应用程序需要线性化，并且一些副本由于网络问题而与其他副本断开了连接，于是有些副本在断开连接时无法处理请求：它们必须要么等待网络问题的解决，要么返回错误（无论哪种方式，它们都变得不可用）。

* 如果你的应用程序不需要线性化，那么可以编写为每个副本都可以独立处理请求，即使它与其他副本（例如多主机）断开连接。在这种情况下，即使面对网络问题应用程序仍然可用，只是它的行为不是线性化的。

因此，不需要线性化的应用程序可以更好地容忍网络问题。这种洞察力被通称为CAP定理，由埃里克·布鲁尔于2000年命名，尽管自20世纪70年代以来分布式数据库的设计者就知道了这种取舍。

CAP最初是作为经验法则提出的，没有准确的定义，目的是为了开始讨论数据库中的取舍问题。当时，许多分布式数据库专注于在具有共享存储的设备集群上提供可线性化的语义[18]，CAP则鼓励数据库工程师探索更广泛的分布式无共享系统的设计空间，这些系统更适合用于实现大规模Web服务。CAP值得称赞的地方是这种文化的转变——见证了自2000年代中期以来新数据库技术的爆发（也就是NoSQL）。

> **The Unhelpful CAP Theorem**
> 
> CAP is sometimes presented as Consistency, Availability, Partition tolerance: pick 2 out of 3. Unfortunately, putting it this way is misleading [32] because network partitions are a kind of fault, so they aren’t something about which you have a choice: they will happen whether you like it or not [38].
>
> At times when the network is working correctly, a system can provide both consistency (linearizability) and total availability. When a network fault occurs, you have to choose between either linearizability or total availability. Thus, a better way of phrasing CAP would be either Consistent or Available when Partitioned [39]. A more reliable network needs to make this choice less often, but at some point the choice is inevitable.
>
> In discussions of CAP there are several contradictory definitions of the term availability, and the formalization as a theorem [30] does not match its usual meaning [40]. Many so-called “highly available” (fault-tolerant) systems actually do not meet CAP’s idiosyncratic definition of availability. All in all, there is a lot of misunderstanding and confusion around CAP, and it does not help us understand systems better, so CAP is best avoided.

The CAP theorem as formally defined [30] is of very narrow scope: it only considers one consistency model (namely linearizability) and one kind of fault (network partitions, vi or nodes that are alive but disconnected from each other). It doesn’t say anything about network delays, dead nodes, or other trade-offs. Thus, although CAP has been historically influential, it has little practical value for designing systems [9, 40].

There are many more interesting impossibility results in distributed systems [41], and CAP has now been superseded by more precise results [2, 42], so it is of mostly historical interest today.

#### Linearizability and network delays

Although linearizability is a useful guarantee, surprisingly few systems are actually linearizable in practice. For example, even RAM on a modern multi-core CPU is not linearizable [43]: if a thread running on one CPU core writes to a memory address, and a thread on another CPU core reads the same address shortly afterward, it is not guaranteed to read the value written by the first thread (unless a memory barrier or fence [44] is used).

The reason for this behavior is that every CPU core has its own memory cache and store buffer. Memory access first goes to the cache by default, and any changes are asynchronously written out to main memory. Since accessing data in the cache is much faster than going to main memory [45], this feature is essential for good performance on modern CPUs. However, there are now several copies of the data (one in main memory, and perhaps several more in various caches), and these copies are asynchronously updated, so linearizability is lost.

Why make this trade-off? It makes no sense to use the CAP theorem to justify the multi-core memory consistency model: within one computer we usually assume reliable communication, and we don’t expect one CPU core to be able to continue operating normally if it is disconnected from the rest of the computer. The reason for dropping linearizability is performance, not fault tolerance.

The same is true of many distributed databases that choose not to provide linearizable guarantees: they do so primarily to increase performance, not so much for fault tolerance [46]. Linearizability is slow — and this is true all the time, not only during a network fault. 

Can’t we maybe find a more efficient implementation of linearizable storage? It seems the answer is no: Attiya and Welch [47] prove that if you want linearizability, the response time of read and write requests is at least proportional to the uncertainty of delays in the network. In a network with highly variable delays, like most computer networks (see “Timeouts and Unbounded Delays”), the response time of linearizable reads and writes is inevitably going to be high. A faster algorithm for linearizability does not exist, but weaker consistency models can be much faster, so this trade-off is important for latency-sensitive systems. In Chapter   12 we will discuss some approaches for avoiding linearizability without sacrificing correctness.