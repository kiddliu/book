# 第三章 存储与获取

*Wer Ordnung hält, ist nur zu faul zum Suchen*

（如果你喜欢保持事物整齐有序，你只是太懒不想去寻找罢了）

德国谚语

---

从最基础的层次来讲，数据库需要做两件事：当你给它某些数据，它应该存储这些数据，并且稍后当你问它这些数据时，它应该把数据给回你。

在第二张中我们讨论了数据模型和查询语言——比如，你（应用开发者）给数据库数据的格式，以及稍后问它再要回来的机制。在这一章我们讨论同样的东西，只是从数据库的角度出发：我们如何存储给定的数据，以及当我们被要求提供它时如何找到它。

为什么你，作为一个应用开发者，应当关心数据库内部是如何处理存储与获取的呢？你大概不会从头开始实现自己的存储引擎，但是你*确实*需要从一众可用选择中挑出适合你应用的存储引擎。为了让存储引擎在你的工作负载上运行出色，你需要对当前工作的存储引擎有一个粗略的概念。

特别的是，**为事务性负载优化的存储引擎与为分析优化的存储引擎是有很大差别的**。稍后在“事物处理还是分析？”一节探索这个差别，而在“面向列的存储”一节我们将讨论为分析优化的一个存储引擎家族。

然而，首先我们将通过讨论我们熟悉的数据库种类所使用的存储引擎开启这一章：传统的关系型数据库，以及大部分的NoSQL数据库。我们会查看储存引擎的两个家族：*日志结构*的存储引擎，以及*面向页*的存储引擎，比如B树。

## 为数据库助力的数据结构
想象一下世界上最简单的数据库，由两个Bash函数实现：

```Shell
#!/bin/bash

db_set () {
    echo "$1,$2" >> database
}

db_get () {
    grep "^$1," database | sed -e "s/^$1,//" | tail -n 1
}
```

这两个函数实现了一个键值对存储。你可以调用`db_set key value`，把`key`与`value`储存在数据库中。键与值（几乎）可以是你想要的任何事物——举个例子，值可以是一个JSON文档。之后你可以调用`db_get key`，它会查找这个键绑定的最新的值并返回它。

而它确实可以工作的：

```Shell
$ db_set 123456 '{"name":"London","attractions":["Big Ben","London Eye"]}'
$ db_set 42 '{"name":"San Francisco","attractions":["Golden Gate Bridge"]}'
$ db_get 42
```

底层存储格式非常简单：一个文本文件，每一行包含一个键值对，用逗号隔开（大致与一个CSV文件类似，除了转义字符问题）。每一个`db_set`调用把值附加在文件结尾处，所以加入你更新某个键对应的值好几次，旧版本的值没有被覆盖——你需要查找文件中键出现的最后一次从而定位到最新的值（也就是`db_get`中的`tail -n 1`）：

```Shell
$ db_set 42 '{"name":"San Francisco","attractions":["Exploratorium"]}'

$ db_get 42
'{"name":"San Francisco","attractions":["Exploratorium"]}'

$ cat database
123456 '{"name":"London","attractions":["Big Ben","London Eye"]}'
42 '{"name":"San Francisco","attractions":["Golden Gate Bridge"]}'
42 '{"name":"San Francisco","attractions":["Exploratorium"]}'
```

我们的`db_set`函数实际上对简单的事物有着相当不错的性能，因为附加到一个文件通常是非常有效率的。与`db_set`的行为类似，许多数据库在内部使用*日志*，一个只能附加的数据文件。真实的数据库有更多的问题要解决（比如并发控制、回收磁盘空间从而日志大小不会永远增长下去、以及处理错误与部分写入的条目），但是基本原则是一样的。日志出人意料的有用，在书的余下部分我们还会遇到它们几次。

> 注意
>
> *日志*这个词经常用于指代应用程序日志，那是应用程序输出描述发生了什么的文本。本书使用了*日志*更一般的含义：一个只追加的条目序列。它不必是人类可读；它也许是二进制的，只是为了让其它程序读取。

另一方面，如果在我们的数据库中有大量条目，我们的`db_get`函数性能会非常差。每一次要查找一个键，`db_get`都要从头到尾扫描整个数据库文件，查找键的出现。用算法术语的话，一次查询的代价是`O(n)`:如果数据库中的条目数`n`翻倍，那么查找也要花掉双倍的时间。这可不妙。

为了有效率地在数据库中查找特定地键，我们需要一个不一样地数据结构：*索引*。在这一章我们将看到一系列的索引结构并比较它们；它们背后的大概概念是另外保留某些额外的元数据作为路标，并帮助你定位到你想要的数据。如果你要用集中不同的方式搜索同一个数据，你也许需要几种由不同部分数据构成的索引。

索引是由主数据衍生出来的*额外的*数据结构。许多数据库允许添加和移除索引，而这并不影响数据库的内容；它只影响查询的性能。维护多余的数据结构导致额外的开销，尤其是在写入的时候。对写入来说，要击败只是附加内容到文件的性能是很难的，因为那就是最简单的写入操作。任何类型的索引通常都会使写入变慢，因为每次数据写入时都需要更新索引。

在数据系统中这是很重要的权衡：精心挑选的索引使读取查询更快，但是每一个索引都会使写入变慢。由于这个原因，数据库默认不为所有数据创建索引，而是要求你——应用开发者或是数据库管理员——手动选择索引，这需要借助你对应用的典型查询模式的理解。这样你可以选择带给应用最大便利的索引，而不必引入不必要的开销。

### 哈希索引

让我们从键值对数据索引开始。当然你不只是能为这类数据建立索引，但是它很常见，同时也是更复杂索引的有用组件。键值存储与大多数编程语言中的*字典*类型非常类似，通常实现为一个哈希映射（哈希表）。哈希映射在许多算法教科书里都有描述，所以我们在这里不会具体讨论它是怎么工作的。既然我们已经为内存内数据结构使用了哈希映射，为什么不用它们为磁盘上的数据建立映射呢？

假如说我们的数据存储只是附加数据到文件，就像之前的例子一样。那么最简单可行的索引策略是这样的：维护一个在内存中的哈希映射，其中每一个键都映射到数据文件的一个字节偏移地址——在这个地址可以找到对应的值，如图3-1所示。一旦一个新键值对被附加到文件，同时更新这个哈希映射从而反映刚刚写入数据的偏移地址（这个策略对插入新键以及更新已有的键都有效）。当你需要查找一个值时，用这个哈希映射找到数据文件中的偏移地址，寻址并读取值。

*图3-1 用类似CSV格式储存键值对日志，并用在内存中的哈希映射建立索引*

这听起来很简单，但确实是一种可行的方法。实际上，Bitcask（Riak的默认存储引擎）就是这么做的。Bitcask提供了高性能的读写，满足了所有键都可以存储在可用内存的需求，因为哈希映射完全储存在内存中。值会用掉比可用内存更多的空间，因为只需要一次寻址就可以从磁盘加载这些值。如果部分数据文件已经在文件系统的缓存中的话，那么读取完全不需要任何磁盘输入输出。

类似Bitcask这样的存储引擎非常适合值经常更新的情况。举个例子，键也许是一个关于猫的视频的URL，而值也许是视频被播放的次数（每当有任点击了播放键就增加）。在这样的负载下，有许多的写入，但是没有太多不同的键——每个键写入次数很多，但是把所有键保存在内存中是可行的。

到目前位置所描述的，都只是附加数据到文件——那么我们如何避免最终耗光磁盘空间呢？一个好的解决方案是把日志拆分为特定大小的段，当段达到特定大小之后关闭这个段，并把后续数据写入到一个新的段文件中。这样我们就可以对这些段文件进行压缩，如图3-2所示。压缩意味着丢弃日志中重复的键，只保留每个键最新的更新。

*图3-2 压缩一个键值对更新日志（记录每一个猫视频的播放次数），只保留每个键最新的更新*

此外，由于压缩经常使段文件变得非常小（假设一个键在一个段中平均被复写了好几次），我们还可以在压缩的同时合并好几个段文件，如图3-3所示。段文件在被写入后不会再被修改，于是合并了的段被写入到一个新文件。合并与压缩被冻结的段可以在后台线程中完成，而同时，我们可以继续如往常一样用老的段文件服务读取和写入请求。当合并过程结束，读取请求从就的段文件切换到到新的合并过的段——在这之后旧的段文件就可以删除了。

*图3-3. 同时执行压缩与合并*

现在每一个段都有了自己的在内存中的哈希表，把键映射到了文件偏移地址。为了找到键对应的值，我们首先检查最新的段的哈希映射；如果键不存在我们检查第二新的段，以此类推。合并使得段的数量很小，所以查找并不需要检查太多哈希映射。

许多细节都把这个简单的想法付诸实践。简而言之，真正的实现中一些重要的问题是：

*文件格式*

对于日志来说CSV并不是最好的格式。使用二进制格式更快也更简单：编码首先是以字节为单位的字符串长度，而后是原始字符串（不需要转义）。

*删除条目*

如果你要删除一个键以及对应的值，你必须附加一个特殊的删除条目到数据文件（有时被叫做*墓碑*）。当日志段被合并的时候，墓碑会告诉合并过程丢掉被删除的键对应的任何之前的值。

*崩溃恢复*

如果数据库被重启，在内存中的哈希映射会丢失。原则上，你可以通过从头到尾读取整个段文件，并在过程中标记每个键最新的值的偏移地址的方法恢复每一个段的哈希映射。然而，如果段文件很大的话这大概会花很长时间，使得服务器重启变得很痛苦。Bitcask通过把每一个段的哈希映射快照储存在磁盘上提高了恢复速度，快照可以非常快速的被加载到内存中。

*部分写入记录*

数据库随时有可能崩溃，其中当然也包括正在附加条目到日志的时候。Bitcask文件包含校验值，从而可以检测、忽略日志受损的部分。

*并发控制*

只允许附加的日志第一眼看起来是很浪费的：为什么不就地更新文件呢，用新的值覆盖旧的值？然而只允许附加的设计结果因为这几个原因被证明是很好的：

* 附加数据与段合并时顺序写入操作，这通常比随机写入要快得多，尤其是传统机械硬盘上。某种程度上相对于基于闪存芯片的*固态硬盘*顺序写入也是更优先的选择。我们会在“比较B树与LSM树”一节深入讨论这个问题。
* 如果段文件是只允许附加的或者不可变的话，并发与崩溃恢复就会更简单。举个例子，你不需要担心当值被复写时崩溃发生了，留下了一个包含部分旧数据和部分新数据的文件。
* 合并旧的段防止了数据文件碎片化的问题。

然而，哈希表索引也有局限：

* 哈希表必须能放到内存中，如果你有很大量的键的话就很不走运了。原则上你可以在磁盘上维护哈希映射，但是不幸的是让位于磁盘上的哈希映射运行良好很困难。它需要大量随机访问输入输出，变满了之后再要增长就会有很高的代价，而哈希散列需要非常繁琐的逻辑。
* 范围查询效率低下。举个例子，扫描所有在`kitty00000`到`kitty99999`之间的键非常不容易——你需要在哈希映射中挨个查找。

下一节我们会看到没有这些限制的索引结构。

### SSTable与LSM树

在图3-3中，每一个日志结构存储段都是一个键值对序列。这些键值对出现的顺序就是写入时的顺序，而日志中稍后出现的值优先于同一个键稍早之前出现的值。除此之外，文件中键值对的顺序完全无关。

现在我们对段文件的格式做一个简单的变化：我们要求键值对的次序是*按键排序的*。乍一看，这个要求好像使得我们无法继续使用顺序写入了，但是我们稍后再讨论这个问题。

我们把这种格式叫做*排序字符串表*，简称SSTable。我们还要求在每个合并后的段文件中每个键只能出现一次（压缩过程已经保证了这一点）。相比于有哈希索引的日志段，SSTable有几个大优势：

1. 合并段既简单效率又高，哪怕段文件比可用内存大。这种方式与*合并排序*算法用到的类似，如图3-4所示：开始的时候并列读取输入文件，取每个文件的第一个键，（根据排序顺序）拷贝顺序最小的的键到输入文件，然后重复这个过程。这样产生了一个新的合并段文件，同样按键排好了序。

    *图3-4 合并几个SSTable段，为每个键只保留了最新的值*

    如果同一个键出现在几个输入段呢？记住，每个段都包含了某个时间段内所有写入数据库的值。这意味着一个输入段中所有的值必然比其它段中所有的值都要新（假如我们一直合并相邻的段的话）。当多个段包含同一个键，我们可以保留最新的段中的值而丢弃在老的段中的值。

2. 为了找到文件中的某个特定键，你不用再在内存中保留所有键的索引了。看图3-5中的一个例子：假设你要找键`handiwork`，但是你不知道这个键再段文件中的偏移地址。然而，你却知道键*handbag*与*handsome*的偏移地址，由于排序你知道*handiwork*必然出现在这二者之间。这意味着你可以先跳到*handbag*的偏移地址然后从那里开始扫描，直到你找到*handiwork*（或者没有，如果这个键没有在这个文件中出现）。
    
    *图3-5 一个有内存索引的SSTable。*

    你仍然需要一个内存索引来告诉你某些键的偏移地址，它可以是很分散的：段文件内每几个KB存一个键就足够了，因为搜索几个KB是非常快的。

3. 因为不管怎样读取请求需要扫描请求范围内的一些键值对，把这些条目分组到一个数据块然后在写入磁盘前压缩它是可能的（图3-5中的暗影区域部分）。这样，每一个分散的内存索引条目都指向了一个压缩块的起始地址。除了节省了磁盘空间以外，压缩也降低了输入输出的带宽使用。

#### 构建与维护SSTable

目前为止还不错——但是首先你如何让数据按键排序？后续的写入可以是任意顺序的。

在磁盘上维护一个排好序的结构是可能的（见“B树”一节），但是在内存中维护会更简单一点。有足够多的已知的树结构可以用，比如红黑树或者AVL树。有这些数据结构，你可以以任何顺序插入键并且按顺序重新读回它们。

现在我们可以让我们的数据引擎按照下边的方式工作：

* 当一个写入发生时，把它添加到一个内存平衡树结构（比如，一个红黑树）中。这个内存树结构有时被叫做*内存表*。
* 当内存表的大小超过阙值时——一般几个MB——把内存表写到磁盘成为一个SSTable文件。因为树结构已经保持键值对按键排序，这个动作可以完成得很有效率。新的SSTable文件成为了数据库最新的段。在SSTable被写到磁盘的同时，写入动作可以继续进行，数据保存到了一个新的内存表实例中。
* 为了服务读取请求，首先尝试在内存表里查找键，然后是在磁盘上最新的段文件，然后是在第二新的段文件，等等。
* 时不时的在后台运行一个合并与压缩过程以合并段文件，并丢弃被复写或删除的值。

这个方案工作的非常出色。只受一个问题的影响：如果数据库崩溃，最近的写入（在内存表中还没有写入到磁盘）会丢失。为了防止这个问题，我们可以在磁盘上维护一个单独的日志，每一次写入都立即附加到这里，就像上一节里一样。这个日志不排序，但是这没关系，因为它的作用就是在崩溃之后恢复内存表。每一次内存表被写入到SSTable，这个对应的日志就可以被丢掉了。

#### 从SSTable生成LSM树

这里描述的算法正是LevelDB和RocksDB在用的，它们是被设计为可以嵌入到其他应用里的键值对存储引擎库。除此以外，LevelDB可以在Riak中作为Bitcask的替代品。类似的存储引擎也有在Cassandra与HBase中，这二者都是受谷歌Bigtable论文（里边引进了数据SSTable与内存表）的启发。

起初这种索引结构是Patrick O’Neil等人提出的，当时的名字是日志结构的合并树（或者LSM树），基于日志结构文件系统的早期成果。基于这种合并压缩排序文件原则的存储引擎也通常被叫做LSM存储引擎。

Lucene，一种Elasticsearch与Solr用到的针对全文搜索的索引引擎，用到了一种类似的方法来储存它的术语词典。全文索引要比键值对索引复杂的多但是基于一个类似的想法：在搜索查询中给定一个词，找到所有提到这个词的文档（网页、产品说明等等）。它是用键值对结构实现的，其中键是这个词（*术语*）而值是由包含这个词的文档的ID构成的列表（张贴清单）。在Lucene里，从术语到张贴清单的这种映射是保存在类似SSTable的排序文件中的，并且根据需要可以在后台合并。

#### 性能优化

如往常一样，许多细节使得存储引擎在实践中表现良好。举个例子，用LSM树算法在查找数据库中查找并不存在的键是很慢的：你必须检查内存表，之后从新往旧检查段文件（也许还需要从磁盘读取每一个文件）直到确认这个键并不存在。为了优化这一类访问，存储引擎经常使用额外的*Bloom过滤器*。（Bloom过滤器是用于近似集合内容的高效内存数据结构。它能告诉你键是不是在数据库中，因此为读取不存在的键节省了许多不必要的磁盘读取。）也有不同的判断如何压缩、合并SSTable的次序与时机的策略。最常见的选择是根据大小分层以及分级压缩。LevelDB与RocksDB使用分级压缩（这也就是为什么叫做LevelDB），HBase使用大小分层，而Cassandra二者都支持。在大小分层压缩中，更新更小的SSTable依次被合并到更老更大的SSTable中。在分级压缩中，键的范围被分成了稍小的SSTable而旧的数据被移到了不同的“层，”使得压缩更递进式的进行，而且磁盘空间用量更少。

虽然有许多微妙的地方，但是LSM树的基本理念——保持级联的SSTable在后台不断合并——是简单有效的。哪怕数据集大小比可用的内存空间大很多也可以运行良好。由于数据以排好的顺序存储，你可以有效地执行范围查询（搜索从某个最小值到某个最大值之间所有的键），同时因为磁盘写入是顺序的，LSM树于是支持相当高的写入吞吐量。

### B树

截至目前我们讨论的日志结构的索引越来越被接受，然而它们不是最常见类型的索引。使用最广泛的索引结构完全是另外一个东西：B树。

1970年推出并在之后不到10年的时间里“无处不在”，B树非常好地经受住了时间的考验。几乎在所有的关系型数据库中它仍然是标准的索引实现，而许多非关系型数据库也用它。

就像SSTable一样，B树保存按键排序的键值对，这使得键值对查找以及范围查询非常有效率。但是相似的地方也就这么多：B树有着完全不一样的设计哲学。

我们之前了解的日志结构的索引把数据库拆分成大小不定的*段*，一般大小在几个MB甚至更多，并且段总是顺序写入的。相比之下，B树把数据库拆分成了固定大小的*块*或者*页*，习惯上大小是4KB（有些时候会更大一些），并且同时只读或写一个页。这种设计更贴近底层硬件，因为磁盘也是以固定大小的块进行排列的。

每一个页用地址或者位置标记，这使得页可以指向其它页——与指针类似，但是是在磁盘上而不是在内存中。我们可以用这些页引用构建页面树，如图3-6所示。

*图3-6 用B树索引查找键*

一个页被指定为B树的*根*；每当在索引中查找键的时候，都是从这里开始的。这个页包含了数个键与指向子页的引用。每一个子页覆盖一个连续区间的键，引用之间的键指明了覆盖区间的上下界。

在图3-6的例子中，我们在查找键251，于是知道我们需要沿着上下界为200与300的页引用查找。它把我们带到了一个相似的页面，它进一步把200到300区间分成了数个子区间。最终我们会下到包含独立键的页（叶子页），在那里要么包含内联键对应的值要么包含可以找到值的页引用。

B树中一个页包含的子页引用数被叫做*分支因子*。举个例子，在图3-6中分支因子为6。在实践中，分支因子受页引用的大小以及范围上下界的影响，但是通常是几百。

如果要更新B树中一个已有键对应的值，那么查找包含那个键的叶子页，修改这个页中值，然后把页写回磁盘中（任何指向这个页的引用依旧有效）。如果要添加一个新键，需要找到包含这个新键的区间所在的页然后把它添加进去。如果页没有足够的空间容纳新的键，那么把它分为两个半满的页，而父页也需要被更新以体现区间的重新划分——见图3-7。

*图3-7 通过分页使B树增长*

整个算法保证了树一直保持*平衡*：有着*n*个键的B树高度总是*O(log n)*。大多数数据库都可以放到一个三或四层高的B树里，所不需要为了找到你要的页访问很多页引用。（一个四层高的、每个页4KB、分支因子为500的树可以储存高达256TB的数据。）

#### 让B树变得可靠

B树的基本底层写入操作是用新数据覆盖磁盘上的页。这是基于复写不会改变页的位置的假设的；比如，所有指向这个页的引用在页被覆盖的时候保持不变。这与日志结构的索引形成鲜明的对比，日志结构索引只是附加数据到文件（并且最终删除废弃的文件）但是从来不会就地修改文件。

你可以把覆盖磁盘上的页当作实际的硬件操作。在一个磁盘上，这意味着移动磁头到正确的位置，等待磁盘旋转到正确的位置，之后用新数据覆盖合适的扇区。在固态硬盘上，发生的情况要更复杂一些，，因为固态硬盘必须一次擦除并重新写入存储芯片中的很大一块。

此外，一些操作需要好几个不同的页被复写。举个例子，如果插入动作导致页面需要被分成两部分，这需要写入两个页，并且重写父页以更新这两个子页的引用。这是一个危险的操作，因为如果数据库在其中一部分页被写入后崩溃，这时的索引就被破坏了（比如出现了一个不是任何页面子页的孤页）。

为了使数据库适应崩溃，通常B树的实现会包含一个额外的数据结构在磁盘上：*预写入日志*（WAL，也被称为*重做日志*）。这是一个只附加文件，每个对B树的改动都必须先被写入到这个文件然后再应用到树中的页上。当数据库从崩溃恢复，这个日志用来把B树恢复到一个一致的状态。

就地更新页带来的另一个复杂问题是如果多个线程同时访问B树就需要谨慎的并发控制——否则线程访问树会得到不一致的结果。一般是通过*锁存器*（轻量锁）保护树的数据结构解决这个问题。基于日志结构的方式再这个方面要简单一些，因为所有的合并动作是在后台完成而不会影响对系统的查询同时不时地原子化交换新旧段。

#### B树的优化

由于B树已经存在了太长时间，这些年来有许多优化方法被开发出来也不奇怪。这里只提几个：

* 相比于覆盖页并出于崩溃恢复的原因维护WAL，某些数据库（比如LMDB）使用了写入时复制方案。被修改的页写入到了另外一个位置，同时在树中创建了一个新的父页并指向它。这种方式对并发控制也很有用，这会出现在“快照隔离与重复读取”一节。
* 可以通过不存储整个键而是键的缩写来节省页内的空间。尤其是在树的上层，键只需要提供足够作为边界的信息就可以了。把更多的键存在同一个页使得树有更大的分支因子，因而有更少的层。
* 一般地，也可以被放在磁盘上地任何位置；并没有要求相邻的键区段在磁盘上的位置也靠近。如果一个查询需要以排序顺序扫描很大一部分键，那么分页布局就很没效率了，因为每个页的读取可能都需要磁盘重新寻址。许多B树的实现因此都尝试在布局时让叶子页以顺序次序出现在磁盘上。然而随着树的增长维护这个次序会变难。相比之下，LSM树在合并时只重写存储段一次，对它们来说保持顺序的键在磁盘上的位置也靠近会更简单一些。
* 额外的指针被添加到了树中。举个例子，每个叶子页可以有指向左右兄弟页的引用，使得按顺序扫描键不需要再跳回父页。
* B树的变种，比如*分形树*，为了减少磁盘寻址借鉴了某些日志结构的理念（而这与分形无关）。

### 比较B树与LSM树

虽然B树的实现一般比LSM树的实现更成熟，然而由于性能特点LSM树仍然很有趣。根据经验，LSM树一般写入更快，而B树被认为是读取更快的。在LSM树上读取一般会更慢是因为它们需要在不同的压缩阶段检查好几个不同的数据结构以及SSTable。

然而，基础测试经常是没有定论的，并且对具体的工作量很敏感。为了得到合理的比较结果，你需要用特定的工作量来测试系统。在这一节中我们将简要讨论在测试存储引擎性能时值得考虑的几件事。

#### LSM树的优势

B树索引每一份数据的写入都至少发生两次：一次写入到预写入日志，另一次写入到树中的页本身（如果需要分页的话那就再多一次）。每次都要写入一整页也产生开销，哪怕只是页中的几个字节改变了。某些存储引擎甚至会复写同一个页两次从而避免在类似断电的情况下得到一个部分被更新的页。

日志结构的索引也会因为重复的压缩合并SSTable而重写好几次数据。这个效应——在整个数据库生命期内每一次写入导致了数次的磁盘写入——被称为*写入放大*。在固态硬盘上是一个特别值得关注的问题，因为每一个区块只有有限的复写次数。

在写入繁重的应用中，性能瓶颈也许是数据库写入磁盘的速率。在这种情况下，写入放大有直接的性能代价：存储引擎写磁盘的次数越多，在可用的磁盘带宽下每秒能处理的写入次数就越少。

此外，相比于B树，LSM树通常可以维持高写入吞吐量，部分是因为有些时候它们有着较低的写入放大系数（然而这取决于存储引擎配置与工作量），部分是因为它们顺序写入压缩的SSTable文件而不是复写书中的部分页。在机械硬盘上这个差别特别重要，因为顺序写入比随机写入快多了。

LSM树可以被压缩得更好，因而相比于B树常常在磁盘上生成更小的文件。B树搜索引擎由于碎片会留下一些未用空间：当页面拆分或者一个行不能在已有页放下时，页中的某些空间没有被占用。因为LSM树不是基于页的而且定期重写SSTable以移除碎片，所以有着较低的存储代价，尤其是在用了分层压缩之后。

在许多固态硬盘中，固件内部使用了日志结构算法把随机写入变成底层存储芯片的顺序写入，于是存储引擎写入模式的影响不太明显。然而，较低的写入放大以及较少的碎片仍然是固态硬盘的优势：以更紧凑的方式表示数据使得在可用的输入/输出带宽内允许有更多的读取和写入请求。

#### LSM树的缺点

日志结构存储的一个缺点是压缩过程有时影响正在进行的读取和写入的性能。虽然存储引擎尝试执行增量压缩并且不影响并行访问，但是磁盘的资源有限，于是很容易发生请求需要等待磁盘完成代价昂贵的压缩操作。对吞吐量以及平均响应时间的影响一般比较小，但是在高百分位（见“描述性能”一节）查询日志结构存储引擎的相应时间有些时候会相当得高，而换作B树的话就更好预测一些。

另一个压缩的问题伴着高写入吞吐量产生：磁盘有限的的写入带宽需要在初始写入（记日志以及清空内存表到磁盘上）与后台运行的压缩线程之间共享。当写入一个空的数据库时，所有的磁盘带宽都可以被用来进行初始写入，但是随着数据库越来越大，更多的带宽被用于压缩。

如果写入吞吐量很高而且压缩配置不仔细的话，是会发生压缩跟不上写入速率的情况的。在这样的情况下，磁盘上没有被合并的段数量持续增长，直至磁盘空间耗尽，而读取也会因为需要检查更多的文件而被拖慢。通常，基于SSTable地存储引擎不会限制写入地速率，哪怕压缩无法跟上，所以你需要明确地监控以检测这种情况。

B树的一个优势是在索引中每个键只存在于一个位置，而日志结构的存储引擎可以在多个不同段中有数个同一个键的拷贝。这一方面使得B树在想要提供强大的事务语义的数据库中很有吸引力：在血多关系型数据库中，事务隔离是通过在键的区间上使用锁实现的，而在B树索引中，这些锁可以直接用于树。在第七章中我们会更详细地讨论这一点。

在数据库架构中B树已经相当根深蒂固了，并且针对多种级别工作量提供了一致好的性能，所以短时间内它不可能消失。在新的数据存储中，日志结构索引变得越来越流行了。没有快速简便的法则来判断那种存储引擎更适合你的场景，所以这值得凭经验测试一下。

### 其它索引结构

截至目前我们只讨论了键值对索引，它与关系型模型中的*主键*索引。主键唯一标识了关系型表中的一行，或者是文档型数据库中的一篇文档，或者是图数据库中的一个顶点。数据库中的其它条目可以通过这个主键（或者ID）引用那一行/一篇文档/一个顶点，而索引是用来解析这样的引用的。

*副键*也是很常见的。在关系型数据库中，你可以通过命令`CREATE INDEX`在同一张表中创建数个副键，为了使连接执行得有效率它们通常是必要的。举个例子，在第二章中的图2-1中你很有可能为`user_id`一列创建副键从而可以在每张表中找到所有属于同一个用户的行。

副键是很容易从键值索引中构建的。主要的差别在于键不唯一；即有可能许多行（文档，顶点）有着相同的键。有两种方式可以解决这个问题：要么让所以索引中的每一个值都是一个匹配行标识符的列表（类似全文索引中的TODO: posting list），要么通过附加行标识符使每个键都是唯一的。任意一种方式，B树与日志结构的索引都可以被用作副索引。

#### 在索引中储存值

索引中的键是查询搜索的目标，但是值可以是下面二者之一：它可以是查询的实际的行（文档，顶点），或者是指向存在别处的行的引用。在后一种情况中，存储行的地方被称为堆文件，它在储存数据时没有特定的顺序（也许是只附加式的，也许保留着已删除行的记录方便稍后用新数据覆盖）。使用堆文件的方式很常见，因为它避免了多个副索引出现时数据重复的问题：每个索引只是引用了堆文件中的位置，而实际数据保存在另一个地方。

在不改变键的情况下更新值时，使用堆文件的方式是很有效率的：只要在尺寸上新的值不大于旧的值，条目可以就地复写。如果新的值在尺寸上更大的话情况就变得复杂了，因为有可能需要移到堆内一个有足够空间的新的位置。在这种情况下，要么所有的索引都需要更新以指向条目在堆中的新位置，要么需要在堆中旧的位置留下一个转发指针。

在某些情况里，从索引到堆文件的这个额外的动作对于读取来说是性能损失，所以把被索引指向的行直接存在索引中是可取的。这被称为*聚合索引*。举个例子，在MySQL的InnoDB存储引擎中，表的主键总是一个聚合索引，而副索引指向主键（而不是堆文件位置）。在SQL Server中，你可以为每张表指明一个聚合索引。

聚合索引（把所有数据储存在索引中）与非聚合索引（只是把数据的引用储存在索引中）的一个折中方案
是*覆盖索引*或是*带有包含列的索引*，它在索引中储存表的部分列。这使得响应某些查询时只需要访问索引（在这种情况下，我们说索引覆盖了查询）。

由于数据重复，聚合索引与覆盖索引都可以提高读取的速度，然而它们也需要额外的存储空间并且增加写入的消耗。数据库也需要额外的处理以确保事务的正确，因为应用不应该由于数据重复得到不一致的结果。

#### 多列索引

截至目前我们讨论的索引只是把一个键映射到值上。如果需要查询表内多个列（或者是文档内多个字段）的话，这样是很没有效率的。

最常见的多列索引类型被称为*串联索引*，它只是通过把一列添加到另一列（索引的定义指明字段串联的顺序）简单地把几个字段结合为一个键。这很像旧式的纸质电话簿，就是提供了从（*lastname, firstname*）到电话号码的索引的那种。由于排序顺序，索引可以被用来找出所有有着特定姓的人，或者所有有着特定姓-名组合的人。然而，如果要找出所有有着特定名的人这个索引就没用了。

多维度索引是一种一次性查询多个列的更一般的方式，对地理空间数据来说这一点特别重要。举个例子，搜索餐馆的网站也许有一个包含了每个餐馆经纬度的数据库。当用户在地图上寻找餐馆的时候，网站需要寻找所有用户正在查看的地图方格内的餐馆。这需要类似下面的二维范围查询：

```SQL
SELECT * FROM restaurants 
WHERE latitude > 51.4946 AND latitude < 51.5079 
      AND longitude > -0.1162 AND longitude < -0.1004;
```

标准的B树索引或者LSM树索引没办法有效率地响应这一类查询：它可以给你的要么是在某个维度范围（但任意经度的）之内的所有餐馆，要么是某个经度范围（但任意维度的）之内的所有餐馆，但两者不能同时提供。

有趣的是多维索引并不只用于地理位置。举个例子，电子商务网站可以为维度（*红，黄，绿*）用三维索引搜索特定颜色范围内的产品，而气象观测数据库可以为（*日期，温度*）创建二维索引从而有效地搜索所有2013年、温度在25度至30度之间的观测记录。如果只有一维索引，就必须要么搜索所有2013年（而不管温度）的记录然后通过温度过滤它们，要么反过来。二维索引可以同时缩小时间与温度的范围。这个技术被用在了HyperDex上。

#### 全文搜索与模糊索引

截至目前我们所有讨论到的索引都假设你有精确的数据并且允许你查询键的精确值，或者是排好序的一些列键值。它们没有办法让你搜索类似的键，比如拼写错了的词语。这样的模糊搜索需要不一样的技术。

举个例子，全文搜索引擎一般允许在搜索单个词时扩展到把同义词包含在内，从而忽略词语的语法变化，并且根据全文语言分析支持许多其他的功能。为了解决文档或是查询语句中的笔误，Lucene可以对在特定编辑距离（编辑距离1意味着两个单词之间有一个字母的差别，包括添加，移除，替换）内的词语的搜索。

在“从SSTable生成LSM树”一节我们提到过，Lucene为它的术语字典使用了类似SSTable的结构。这种结构需要一个很小的内存索引从而告诉查询语句需要在排序后的文件的什么偏移位置寻找键。在LevelDB中，这个内存索引是一些键的松散集合，而在Lucene中，内存索引是一个键内出现字符的有限状态自动机，类似trie。这个自动机可以转换成Levenshtein自动机，它支持在给定的编辑距离内有效地搜索单词。

其他模糊搜索引擎被归为文档分类和机器学习方向。更多的详细信息，请参阅信息检索学科的教科书。

#### 把所有东西放在内存中

这一张截至目前讨论的数据结构都是针对磁盘限制问题的。相比于内存，磁盘处理起来很笨拙。同时对于机械硬盘与固态硬盘来说，为了良好的读写性能磁盘上的数据需要仔细布局。然而之所以我们可以忍受这样的笨拙是因为磁盘有两个巨大的优势：持久性（内容在电源关闭时不会丢失），同时相比于内存有着更低的存储成本。

随着内存越来越便宜，成本优势在慢慢消失。许多数据集并不大，所以把它们整个放在内存里是可行的，或许还可以分布在几台设备上。这导致了内存服务器的开发。

某些内存内的键值对存储，比如Memcached，只是用于缓存，设备重启时的数据丢失是可以接受的。但是其他内存数据库解决了持久性问题，可以通过特别的硬件（比如电池供电的内存）、把变更日志写入磁盘、把周期性的快照写入磁盘，或者把内存状态复制到其他设备实现。内存数据库重启时，它需要重新加载状态，（除非使用了特殊的硬件）要么从磁盘要么从网络上的备份恢复。除了写入磁盘，这还是一个内存数据库，因为磁盘只是被用作了解决耐久性问题的只追加日志，而读取完完全全是从内存提供的。写到磁盘还有运营方面的好处，磁盘上的文件很容易被外部工具备份、检查和分析。

类似VoltDB，MemSQL以及Oracle TimesTen这样的产品是有着关系型模型的内存数据库，厂商声称产品通过移除了所有与管理盘上数据结构相关的消耗带来了巨大的性能提升。RAMCloud是一个开源的内存键值对存储，有持久性（用到了日志结构的方式处理内存与磁盘上的数据）。Redis与Couchbase则通过异步写入磁盘提供了弱的持久性。

与直觉相反的是，内存数据库的性能优势并不是因为它们不需要从磁盘读取数据。只要你有足够的内存，甚至基于磁盘的存储引擎也有可能永远不需要从磁盘读数据，因为操作系统无论如何都把最近用到的磁盘块缓存在内存里。然而，它们更快是因为避免了把内存中的数据结构编码成可以被写入磁盘的形式所带来的消耗。

除了性能，内存数据库另一个有趣的领域是提供了基于磁盘的索引难以实现的数据模型。举个例子，Redis为多种数据结构，比如优先队列与集合，提供了一个类似数据库的接口。因为它把所有的数据放在内存中，它的实现就相对简单一些。

最近的研究表明，内存数据库架构可以通过扩展支持大小超过可用内存的数据集，同时没有以磁盘为核心的架构带来的消耗。这种被称为*反缓存*的方式通过在内存紧张时把最近最少用到的数据从内存转移到磁盘，而在被访问到的时候从磁盘重新加载回内存实现的。这个与操作系统处理虚拟内存与分页类似，但是相比于操作系统，数据库因为可以处理单独条目而不是整个页从而可以更有效率地管理内存。然而这种方式仍然要求索引都存在内存中（类似本章开始时Bitcask的例子）。

如果非易失性内存技术大范围应用的话，我们也许需要对存储引擎做进一步变更。目前，这是一块新的研究领域，但是在未来这是值得关注的。

## 事务处理，还是分析？

在商务数据处理的早期，数据库的一次写入通常对应发生了一次*商业交易*：销售，向供应商下单，支付员工薪水，等等。随着数据库扩展到了并不涉及资金转手的领域，术语*事务*仍然存在，指的是构成逻辑单元的一组读写。

> ##### 注意
>
> 事务不是必须有ACID（原子性、一致性、隔离性、持久性）特性。*事务处理*只是意味着允许客户端进行低延迟的读写——与之相反的是*批处理*任务，它只允许周期性地（比如，一天一次）运行。我们会在第七章讨论ACID特性，而在第十章讨论批处理。

虽然数据库开始被用于许多不同种类的数据——博客文章的评论，游戏中的动作，通讯录中的联系人，等等——基本的访问模式依旧与处理商务事务类似。一个应用通常借助索引查找某个键对应的一小部分条目。根据用户的输入插入或是更新条目。由于这些应用都是互动型的，访问模式被人称为*联机交易处理*（OLTP）。

然而，数据库也开始大量用于数据分析，他有着非常不同的访问模式。通常一个分析式的查询需要扫描大量条目，每条记录只读几行，并计算聚合统计数据（比如数量，总和，或是平均数）而不是返回原始数据给用户。举个例子，假如数据是一张销售事务表，那么分析型查许大概是这样的：

* 一月我们每个商店的总营收是多少？
* 最新的促销活动中我们比平时多卖出去了多少香蕉？
* 什么牌子的婴儿食品最经常与什么牌子的尿片一起被购买？

这些查询语句经常是商业分析师写的，并提供帮助公司管理层做出更好决策（商业智能）的报告。为了区分这种使用数据库的方式与商务处理的方式，它被称为*联机分析处理*（OLAP）。OLTP与OLAP的差别并不总是清楚的，但是某些典型特点被列在了表3-1中：

*表3-1 比较事务处理系统与分析系统的特点*

| 特性 | 交易处理系统 (OLTP) | 分析系统 (OLAP) |
| ---  |:----------------- |:--------------- |
| 主读取模式 | 每个查询中返回少量条目，通过键获得 | 聚合大量条目 |
| 主写入模式 | 来自用户的随机访问、低延迟写入 | 批量导入（ETL）或是事件流 |
| 主要使用者 | 终端用户/客户，通过网页应用 | 内容分析式，为了决策支持 |
| 数据代表了什么 | 数据的最新状态（当前时间点） | 随着时间推移发生的事件历史 |
| 数据集大小 | GB到TB级 | TB到PB级 |

起初，同一个数据库可以既用来做事务处理也可以做分析查询。SQL证明了在这方面是很灵活的：对OLTP和OLAP类型的查询都工作得很好。然而，在1980年代末1990年代初，企业有停止使用OLTP系统做分析的趋势，取而代之的是在单独的数据库上运行分析。这个单独的数据库被叫做*数据仓库*。

### 数据仓库

企业也许有数十个不同的事务处理系统：面向客户网站的，实体店控制销售（结账）系统的，记录仓库库存的，计划车辆路线的，管理供应商的，管理员工的，等等。每一个系统都很复杂，需要一个团队的人来维护它，于是系统最终大多数是彼此独立运行的。

这些OLTP系统通常期望有很高的可用性，同时处理事务时有低的延迟，毕竟它们对商业运营常常很关键。数据库管理员因此密切关注OLTP数据库。通常它们不情愿让商业分析师在OLTP数据库上运行临时的分析查询，因为这些查询常常代价很高，会扫描大部分数据集，从而危害并行执行的事务的性能。

相比之下，*数据仓库*是一个独立的数据库，分析师可以查询深度内容而不影响OLTP操作。数据仓库包含了企业内所有不同OLTP系统数据的只读备份。数据从OLTP数据库中被抽取出来（要么周期性的数据转储，要么持续不断的更新），转成对分析友好的模式定义，再清理，然后被加载到数据仓库。这个把数据存入数据仓库的过程叫做*抽取-转换-加载*（ETL），如图3-8所示。

*图3-8 ETL到数据仓库的简化概要*

数据仓库现在几乎再所有的大企业里都存在，但是在小公司他们基本没听说过。这大概是因为小公司没有那么多不同的OLTP系统，而大多数小公司只有很小量的数据——小到足以用传统数据库查询，甚至是用电子表格分析。在大公司，要做一些在小公司很简单的事情需要大量繁重的工作。

为分析用途使用独立的数据仓库，而不是直接查询OLTP系统的一个大优势，是数据仓库可以为分析访问模式做优化。事实证明本章前半部分讨论的建立索引算法对OLTP工作良好，但是响应分析查询就不是那么好了。在本章的剩余部分我们将看到那些为分析目的优化的存储引擎。

#### OLTP数据库与数据仓库的差异

数据仓库的数据模型通常大部分是关系型的，因为SQL通常适合分析查询。有许多图形化的数据分析工具可以生成SQL查询语句，可视化结果，并且允许分析师探索数据（通过诸如*向下钻取*、*切片及切块*）。

表面上，数据仓库与关系型OLTP数据库看起来很像，因为他们都有SQL查询接口。然而系统内部看起来差别很大，因为它们分别为了非常不同的查询模式优化了。许多数据库厂商现在专注于支持事务处理或是分析工作，但不是二者都支持。

某些数据库，比如微软SQL Server和SAP HANA，在同一个产品中同时支持事务处理和数据仓库。然而，它们正逐渐变成两个独立的存储和查询引擎，只是恰好可以通过通用的SQL接口访问。

数据仓库厂商比如Teradata、Vertica、SAP HANA以及ParAccel一般使用昂贵的商业许可销售他们的系统。Amazon RedShift是ParAccel的托管版本。最近，出现了大量的开源SQL-on-Hadoop项目；它们年轻但意欲与商业数据仓库系统竞争。它们包括了Apache Hive、Spark SQL、Cloudera Impala、Facebook Presto、Apache Tajo和Apache Drill。它们中的一些是基于谷歌Dremel项目的理念的。

### 星型与雪花型：分析的模式

如第二章讨论的，根据应用的需求，交易处理领域用到了许多不同的数据模型。然而，在分析领域，数据模型就没有那么的多样性。许多数据仓库都以相当公式化的风格使用，称为*星型模式*（也被叫做*维度建模*）。

图3-9中的示例模式展示了在杂货零售商那可能找到的数据仓库。模式定义的核心被称为*事实表*（在这个例子中，它叫做`fact_sales`）。事实表的每一行代表一个发生在特定时间的事件（这里，每一行代表客户购买产品）。假如我们是在分析网站流量而不是零售销售，那么每一行则代表一次页面浏览或是一次用户点击。

*图3-9 用于数据仓库的星型模式示例。*
通常，事实被记录为单个事件，因为这可以稍后进行最大限度的分析。然而，这意味着事实表可以变得极其大。类似苹果、沃尔玛或者易贝这样得大企业也许有数十PB的交易记录在数据仓库中，其中绝大部分都在事实表中。

事实表中的某些列是属性，比如产品售出时的价格以及从供应商处买进的花费（使得可以计算利润）。事实表中的其他列是指向其他表格的外键，叫做*维度表*。事实表的每一行代表了一个事件，那么维度就代表了事件中的*什么人，什么事，什么地方，什么事件，怎么做*以及*为什么*。

举个例子，在图3-9中，维度之一事售出的产品。`dim_product`表中的每一行代表一种出售的产品，包括库存单位（SKU），描述，商标名，类别，脂肪含量，包装大小等等。`fact_sales`表中的每一行用一个外键指明特定事务中售出了哪个产品。（简单起见，如果客户一次购买了好几个不同的产品，他们会被表示为事实表中不同的行）。

甚至日期与事件也经常用维度表表示，因为这样允许编入关于日期的额外信息（比如公众假日），从而允许查询语句区分假日与非假日的销售。

“星型模式”的名称来自表间关系可视化后的结果，事实表在中间，被它的维度表包围；到这些表的连接就好像星星发出的光线。

这个模板的一个变种被叫做*雪花型模式*，它把维度进一步拆解成子维度。举个例子，可以把商标和产品类别分成不同的表，`dim_product`表中的每一行可以引用商标和类别作为外键，而不是把他们存为`dim_product`表中的字符串。雪花型模式相比于星型模式更规范，但是星型模式往往是首选，因为它们对于分析师来说更简单。

在一个典型的数据仓库中，表通常都很宽：事实表经常超过100列，有时是上百列。维度表也可以非常宽，毕竟包含了所有分析相关的元数据——举个例子，`dim_store`可以包含每个商店所提供服务的详细信息，比如是不是有面包房，商店面积，商店开业日期，最后一次装修，距离最近的高速公路有多远等等。

## 面向列的存储

如果你的事实表里有以兆计的行和PB级别的数据，高效地储存、查询它们变成了一个挑战性的问题。通常维度表会更小一些（以百万计的行），那么在这一节我们主要关注事实的存储。

虽然事实表经常超过100列宽，然而通常的数据仓库查询只会一次访问4或5个列（在分析过程中很少需要“`SELECT *`”查询）。拿示例3-1中的查询举例：虽然它访问了大量的行（2013年内发生的每一次某人购买了水果或者糖果），但是它只需要访问`fact_sales`表中的三个列：`date_key`、`product_sk`、`quantity`。其余所有的行都被忽略的了。

*示例3-1 分析人们购买水果或者糖果的倾向是否取决于一周中的哪一天*

```SQL
SELECT
    dim_date.weekday, dim_product.category,
    SUM( fact_sales.quantity) AS quantity_sold
FROM fact_sales
    JOIN dim_date ON fact_sales.date_key = dim_date.date_key
    JOIN dim_product ON fact_sales.product_sk = dim_product.product_sk
WHERE
    dim_date.year = 2013 AND
    dim_product.category IN ('Fresh fruit', 'Candy')
GROUP BY 
    dim_date.weekday, dim_product.category;
```

这个查询如何高效地执行?

大多数OLTP数据库，储存空间的布局是面向行的：一张表中行的所有值是相互挨着的。文件型数据库是类似的：整个文档一般储存为连续的字节序列。你可以在图3-1中的CSV示例中看到这样的存储。

为了处理类似示例3-1的查询，可以在`fact_sales.date_key`以及`fact_sales.product_sk`列建立索引，告诉存储引擎哪里可以找到特定日期，或者特定产品的所有销售数据。然而，面向行的存储引擎仍然需要从磁盘加载所有这些行（每一行包含超过100个属性）到内存，解析它们，然后过滤掉那些不符合条件的。这会花很长时间。

面向列的理念很简单：不要把行的所有的值储存在一起，而是把每一列所有的数据储存在一起。如果每一列储存在单独的文件里，查询只需要读取和分析查询里用到的行，这会节省大量的工作。这个原理如图3-10所示。

> 注意
>
> 列存储在关系型数据模型中是最容易理解的，但是它也同等地应用于非关系型数据。举个例子，Parquet是一种支持文档数据类型的列存储格式，基于谷歌Dremel。

*图3-10 用列而不是行储存关系型数据。*

面向列的存储布局以来每一个的列文件以同样的次序包含所有的行。因而如果需要重新组织成一整行，那么从每个列文件中取第23条记录，然后把它们放在一起构成表的第23行。

### 列压缩

除了只需要从磁盘加载那些查询需要的列，我们可以通过压缩数据进一步降低对磁盘吞吐量的需求。幸运的是，面向列的存储通常非常适合压缩。

看一下图3-10中每一列的值序列：它们经常看起来很重复，这是压缩的好预兆。取决于列中的数据，我们可以用不同的压缩技术。其中在数据仓库中特别有效的一种叫做*位图编码*，如图3-11中所示。

*图3-11 单列的压缩后的位图索引存储*

经常性的，一列中不同值的数量比行的数量要小（举个例子，零售商也许有以亿计的销售记录，但是只有100000种不同的产品）。现在我们可以把一个有*n*个不同值的列转成*n*个不同的位图：每个位图对应一个值，每个位对应一个行。如果这一行有这个值，那么位为1，否则为0。

如果*n*非常小（比如，*国家*一列只有大约200个不同的值），这些位图可以每行存储一位。但是如果*n*很大，（假设数据分布很松散）那在大部分位图中将有许多0。这种情况下，位图可以进一步进行游程编码，如图3-11底部所示。这使得列编码更加紧凑。

这样的位图索引非常适合数据仓库中常见的查询。比如：

`WHERE product_sk IN (30, 68, 69)`：

加载`product_sk = 30`、`product_sk = 68`与`product_sk = 69`对应的三张位图，然后计算三张位图按位*或*的结果，这个过程可以非常高效地完成。

`WHERE product_sk = 31 AND store_sk = 3`：

加载`product_sk = 30`与`store_sk = 3`对应的位图，然后计算位图按位*与*的结果。这样做是可行的是因为每一列以相同地顺序包含每一行，所以一个列位图中第*k*个比特与另一列位图中第*k*个比特对应是同一行。

当然还有针对不同种类数据的许多其他压缩方法，但是我们不会一一详细讨论它们——可以在（TODO这里）找到一个概览

> 面向列的存储与列族
>
> Cassandra与HBase有*列族*的概念，是从Bigtable继承来的。然而把它们叫做面向列的是很有误导性的：在每个列族中，它们把来自同一行的所有列以及行的键存在一起，而且并不进行列压缩。因而Bigtable模型大部分还算是面向行的。

#### 内存带宽与向量化处理

对于需要扫描上百万行的数据仓库查询，瓶颈在于从磁盘获取数据到内存的带宽。然而，这不是唯一的瓶颈。分析型数据库的开发者也担心是否高效地使用了从内存到CPU缓存的带宽，是否避免了CPU指令处理管道中的分支预测错误和气泡，以及是否用到了现代CPU的单指令多数据（SIMD）指令。

除了降低需要从磁盘加载的数据量，面向列的存储布局也可以很好地利用CPU周期。举个例子，查询引擎可以取出一块压缩了的列数据刚刚好放入CPU的一级缓存，然后在一个紧密循环（这样的循环内没有函数调用）遍历它。CPU执行这样的循环比为每一条被处理的条目都需要执行许多函数调用和条件判断要快多了。列压缩使得一列中更多的行数据可以放在同样大小的一级缓存中。而之前描述的按位*与或*操作符可以被设计为直接在这样的压缩列数据块上执行。这个技术被叫做*向量化处理*。

### 列存储中的排列次序

在列存储中，行是以什么样的顺序并不是很重要。以它们被插入的顺序储存是最简单得，毕竟之后插入一个新行就意味着附加到每个列文件上。然而，我们可以强加一个次序，类似之前我们对SSTable的做法，并把它作为建立索引的机制。

需要注意的是每列独立排序是没有意义的，因为那样我们就不知道列中的哪些项属于同一行。我们可以重建一个行是因为我们知道一列中的第*k*项与另一列中第*k*项是属于同一列的。

然而，数据需要一次排列整行，即使是按列储存的。数据库管理员可以通过他们对日常查询的了解，选择表按哪些行排序。举个例子，如果查询经常定位日期范围，比如上个月，也许把`date_key`作为首排序键。之后查询优化器可以只扫描上个月的行，这比扫描所有行可快多了。

当行在首列中有同样的值时，可以用次列决定排序次序。举个例子，在图3-10中如果`date_key`是首排序键，把`product_sk`作为次排序键就有意义了，因为同一产品同一天的所有销售在存储中被分在同一组中。这对特定日期范围内按产品分组或者过滤销售记录是有帮助的。

排序的另外一个好处在于它帮助压缩列。如果首派序列没有许多不同的值话，排序之后，行中会有很长一段相同值的序列。一个简单的游程编码，就像我们在图3-11的位图里用到的，可以压缩整列到几个KB——即使整张表有几十亿行。

在首排序键上压缩效果是最好的，次排序键与第三排序键会更乱一些，因而没有那么多连续的重复值。更低排序优先级的列看起来基本上是随机次序的，所以它们不会压缩得太好。但是前几列排好序后整体来看仍然是成功的。

#### 几种不同的排列次序

这个想法的一种很聪明的扩展被引入了C-Store并且被商业数据仓库Vertica采用。不同的查询受益于不同的排列次序，那么为什么不把同一份数据以集中不同的方式排序呢？总之数据需要被复制到几台设备上，从而在一台设备失效时数据不会丢失。你也许还会存储以不同方式排序的冗余数据，以便在处理查询时使用最适合查询模式的那个版本。

在面向列的存储中有多个排列次序就好像在面向行的存储中有多个副索引。但是最大的区别在于面向行的存储把每一行储存在一个地方（堆文件或者是聚合索引），而副索引只包含指向匹配行的指针。在列存储中，通常没有任何指向其他地方数据的指针，只有包含数据的列。

### 列存储的写入

这些优化在数据仓库里是有意义的，因为绝大部分负载来自分析师执行的大量只读查询。面向列的存储、压缩以及排序都使得读取查询更快了。然而，它们也有使写入变得困难的缺点。

类似B树使用的就地更新的方式，对压缩列来说是不可能的。如果要在排序了的表中插入一行，基本上必须重写所有的列文件。由于行是通过所在列的位置标识的，插入就必须同时更新所有的列。

幸运的是，在这一章前面提到了一个好的解决方案：LSM树。所有的写入先去到内存存储，在那里它们被添加到排了序的数据结构以待写入到磁盘。这个内存存储无所谓是面向行的还是面向列的。当积累了足够的写入，它们被合并到磁盘上的列文件并批量写入到新的文件里。这正是Vertica怎么做的。

查询需要同时检查磁盘上的列数据以及内存中最新的写入，合并二者的结果。然而查询优化器对用户隐藏了这个差别。从分析师的角度看，数据由于插入、更新或者删除导致的修改立即反应到了之后的查询中。

### 聚合：数据立方体与物化视图

并不是所有的数据仓库都必须是列存储的：传统的面向行的数据库以及一些其他架构也在用。然而，列存储处理临时的分析型查询相当快，因而变得越来越受欢迎。

数据仓库另一个值得简要介绍的方面是*物化聚合*。前面讨论到，数据仓库查询经常涉及聚合函数，比如SQL中的`COUNT`、`SUM`、`AVG`、`MIN`或`MAX`函数。如果许多不同的查询用到了同样的聚合函数，每一次都处理原始数据是很浪费资源的。为什么不缓存一些常用查询用到的聚合结果呢？

构建这样缓存的一张方式是*物化视图*。在关系型模型中，它的定义类似标准的（虚拟）视图：类似表的对象，内容是某个查询的结果。区别在于物化视图是查询结果的真实拷贝，写入到了磁盘，而虚拟视图只是书写查询语句的快捷方式。读取虚拟视图时，SQL引擎实时把它扩展为视图底层的查询，然后处理扩展后的查询。

当底层数据变化时，物化视图需要更新，因为它是数据的非规范化拷贝。数据库可以自动完成它，但是这样的更新使写入代价变得更高，这正是物化视图在OLTP数据库中不经常使用的原因。在读取压力大的数据仓库里它更有意义（到底有没有改善读取性能取决于具体个例）。

物化视图的一个常见特例被称为*数据立方体*或者*OLAP立方体*。它是按不同的维度分组的聚合结果网格。图3-12给出了一个例子。

*图3-12 某数据立方体的两个维度，通过求和聚合数据*

暂时假设每个事实只有指向二维表的外键——在图3-12中，它们是*日期*和*产品*。这样可以画出一个二维表，其中一个坐标是日期，另一个是产品。每个格子包含的是有日期-产品组合的所有事实的属性（比如`net_price`）聚合（比如`SUM`）结果。然后对每一行或每一列应用相同的聚合函数，获得少一维度的结果（销售要么按产品分类而不论日期，要么按日期分类而不论产品）。

一般地，事实的维度常常超过两个。在图3-9中有五个维度：日期，产品，商店，促销活动以及客户。很难想象一个五维超立方体会是什么样子的，但是原则是不变的：每个格子包含的是一个特定的日期-产品-商店-促销活动-客户组合的销售记录。这些值之后可以重复的在每一个维度上总结结果。

物化数据立方体的优势在于特定的查询变得非常快，因为它们实际上已经被计算好了。举个例子，如果想知道昨天每个商店的总销售，你只需要查一下对应维度的总和——不用再扫描数以百万计的行了。

而劣势在于数据立方体没有直接查询原始数据的灵活性。举个例子，没有办法计算哪部分销售来自超过100美元的商品，因为价格不是其中的一个维度。绝大部分数据仓库因此试图保留尽可能多的原始数据，而类似数据立方体这样的聚合只是被用来提升特定查询的性能。

## 总结

在这一章中我们尝试搞清楚数据库是如何处理储存和获取的。在数据库中存储数据的时候发生了什么？稍后再查询这个数据的时候数据库都做了什么？

在高层次上，我们看到存储引擎分成了两大类：一类为交易处理优化（OLTP），一类为分析优化（OLAP）。两者的访问模式有很大的不同：

* OLTP系统通常是面向用户的，这意味着它们会接收到海量的请求。为了处理负载问题，应用通常在每个查询中只访问少量记录。应用通过某种键请求记录，而存储引擎使用索引寻找被请求的键对应的数据。在这里磁盘寻址时间经常是瓶颈。
* 数据仓库和类似的分析系统不太知名，因为它们主要用在商业分析，而不是最终用户。相比于OLTP系统它们处理的查询量要小得多，但是每个查询通常都非常苛刻，要求在很短的时间内扫描数以百万计的记录。在这里磁盘带宽（而不是寻址时间）经常是瓶颈，对这类的工作负载，面向列的存储是一种越来越受欢迎的解决方案。

在OLTP方面，我们看到两大主流派别的存储引擎：

* 日志结构派，它只允许附加数据到文件然后删除废弃的文件，但是从来不更新已经写入的文件。Bitcask、SSTables、LSM树、LevelDB、Cassandra、HBase、Lucene等等是属于这个派别的。
* 就地更新派，它把磁盘看作一组可以被复写的固定大小页集合。B树是这种哲学的最典型例子，被用在所有主要的关系型数据库，以及许多非关系型数据库。

日志结构的存储引擎是相对比较新的发展品。它的核心理念是系统性地把随机写入变为磁盘上地顺序写入，从而由于硬盘与固态硬盘的性能特性获得了更高的写入吞吐量。

介绍完了OLTP，我们简单环顾了一些更复杂的索引结构，以及为把所有数据放在内存的而优化的数据库。

然后我们从存储引擎内部实现离开，了解了一下一般数据仓库的高级架构。这个背景知识展示了为什么分析型工作负载与OLTP是那么得不同：当查询需要顺序地扫描大量行时，与索引就没有多少关系了。相反的是，紧凑地对数据编码变得非常重要，以最大限度地减少查询需要从磁盘读取的数据量。我们讨论了面向列的存储是如何帮助达成这个目标的。

作为应用开发者，如果你有了存储引擎的内部知识，你可以更好的理解哪种工具是最适合你的应用的。如果你需要调整数据库的参数，这种理解使得你可以想象不同的值带来的可能的影响。

虽然这一章的只是不足以让你成为微调任何一种特定存储引擎的专家，但是希望它让你有了足够的知识与想法，使得你可以理解你选择的数据库的说明文档。