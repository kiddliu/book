# 第十章 流处理

*一个切实可行的复杂系统势必是从一个切实可行的简单系统发展而来的。从头开始设计的复杂系统根本不切实可行，无法修修补补让它切实可行。你必须由一个切实可行的简单系统重新开始。*

约翰·盖尔，*系统学*（1975）

---

在第10章中，我们讨论了批处理——这是读取一组文件作为输入并生成一组输出文件的技术。输出是*衍生数据*的一种形式；也就是说，如果有必要，可以通过再次运行批处理过程重新创建的数据集。我们看到了如何使用这个简单而强大的想法来创建搜索索引、推荐系统、分析等等。

然而一个大前提贯穿整个第10章：即，输入是有界的——即已知且有限的大小——因此批处理过程知道它什么时候完成了对输入的读取。例如，位于MapReduce核心位置的排序操作必须先读取整个输入，然后才能开始产生输出：可能会发生最后一个输入记录按键排序值最小的情况，因此它就是是第一条输出记录，所以不可能提早启动输出。

实际上许多数据是没有限制的，因为它是随着时间的推移逐渐到达的：你的用户昨天和今天产生了数据，他们明天会继续生成更多的数据。除非你退出业务，否则这个过程永远没有尽头，因此数据集永远不会以任何有意义的方式“完成”。因此，批处理程序必须人为地把数据按固定时间间隔划分成块：例如，在每天结束时处理一天的数据值，或在每小时结束时处理这一小时的数据。

日常批处理遇到的问题是，输入的更改只会在一天之后的输出中反映出来，这对于许多不耐烦的用户来说太慢了。为了减少延迟，我们可以更频繁地运行处理——比如说，在每秒结束时处理一秒钟的数据——甚至是连续地处理，完全放弃固定的时间片而只处理发生的每一个事件。这就是*流处理*背后的理念。

一般来说，“流”指的是随着时间的推移逐渐可用的数据。这个概念出现在许多地方：在Unix的`stdin`和`stdout`中，编程语言（延迟加载列表）、文件系统API（例如Java的`FileInputStream`）、TCP连接、通过互联网发送音频视频等等。

在本章中，我们将把*事件流*看作一种数据管理机制：与我们在上一章中看到的批处理数据进行无界、增量处理的对应。我们首先会讨论如何在网络上表示、存储和传输流。在“数据库和流”一节中，我们将研究流和数据库之间的关系。最后，在“流的处理”一节中，我们会探索持续处理这些流的方法和工具，以及它们可用于构建应用程序的方法。

## 事件流的发送

在批处理的世界里，作业的输入和输出是文件（或许是在分布式文件系统上）。与流等价的东西是什么样子的？

当输入是文件（字节序列）的时侯，第一个处理步骤通常是把它解析为一系列的记录。在流处理中，记录更常见的名字是*事件*，但本质上是一个东西：一个小的、独立的、不可变的对象，包含了在某个时间点发生的事情的细节。事件通常包含一个时间戳，指的是事件发生时的现世时钟时间（见“单调时钟与现世时钟”）。

例如，发生的事情可能是用户采取的一个操作，例如查看页面或购买。它也可能来自于设备，比如温度传感器的周期性测量，或者是CPU利用率指标。在“使用Unix工具进行批处理”一节的示例中，Web服务器日志的每一行都是一个事件。

如第4章所述，事件可能被编码为文本字符串，或者JSON，或者以某种二进制形式编码。这种编码使得你可以存储事件，比如说可以把它附加到文件，插入到关系性表格，亦或是写入文档型数据库。它还允许你通过网络把事件发送到另一个节点处理。

在批处理中，文件被写入一次，然后会被多个作业读取。类似地，在流的术语中，事件由*生产者*（也称为*发布者*或*发件人*）生成一次，然后可能被多个*消费者*（*订阅者*或*收件人*）处理。在文件系统中，文件名标识一组相关的记录；在流系统中，相关事件通常被分组成为一个*主题*或是*流*。

原则上，文件或是数据库足以连接生产者和消费者：生产者把它生成的每个事件都写入数据存储，而每个消费者定期轮询数据存储以检查自上次运行以来出现的新事件。这基本上是批处理过程在每天结束时处理一整天产生的数据时所做的事情。

然而，向低延迟连续处理发展的时侯，如果数据存储不是为了这一类使用而设计的，轮询的代价就会变得非常高。轮询次数越多，请求返回新事件的百分比就越低，开销因此也就越高。相反地，在新事件出现的时侯通知消费者就好很多。

数据库向来对这种通知机制支持得不是很好：关系型数据库通常有*触发器*，可以对更改作出反应（比如新的一行插入到表中），但它们所能做的非常有限，是数据库设计中之后才考虑到的。取而代之的是，已经有了专门的工具来传递事件通知。

### 消息系统

通知消费者新事件的一种常见方法是使用*消息传递系统*：生产者发送包含这个事件的消息，之后把这个事件推送给消费者。我们之前曾经在“用于消息传递的数据流”一节中讨论过这些系统，现在我们会更详细地讨论这些系统。

生产者和消费者之间直接通信的通道，比如Unix管道或者是TCP连接，是实现消息系统的一种简单方法。然而，大多数消息系统都是在这个基本模型上扩展的。特别的是，Unix管道与TCP都只连接一个发件人与一个收件人，而消息系统则允许多个生产者节点向同一主题发送消息，也允许多个消费者节点在主题中接收消息。

在这个*发布/订阅模型*中，不同的系统采用了各种各样的方法，也没有一个正确的方法可以满足所有的目的。为了区分这些系统，问下面两个问题特别有帮助：

1. *如果生产者发送消息的速度超过了消费者处理的速度，会发生什么？*广义地说有三种选择：系统可以丢弃消息、在队列中缓冲消息或应用*反向压力*（也称为*流量控制*；即，阻止生产者发送更多消息)。例如，Unix管道和TCP使用反向压力：它们有一小块固定大小的缓冲区，如果它被填满，发送方会被阻塞直到接收方从缓冲区中取出数据（请参阅“网络拥塞和排队”）。

    如果消息缓冲在队列中，那么了解队列增长时会发生什么是很重要的。如果内存放不下队列系统会崩溃么，还是它会把消息写入磁盘？如果会写入磁盘，那么这样做是如何影响消息传递系统的性能的？

2. *如果节点崩溃或是暂时离线，会发生什么？有丢失任何消息吗？*与数据库一样，耐久性会要求要么写入磁盘，要么复制到其它设备，或者二者都有（见“复制与耐久性”的边栏），这都是有代价的。如果你可以接受有时消息会丢失，那么在同样的硬件上你会获得更高的吞吐量和更低的延迟。

消息丢失是否可以接受，很大程度上取决于应用程序。例如，对于周期性传输的传感器读数和指标，偶尔丢失数据可能并无大碍，因为不管怎样更新的值晚些就会发出。然而需要注意的是，如果大量消息被丢弃，有可能没有办法立即发现这些指标是不正确的。如果你正在对事件计数，那么可靠地传递是更重要的，因为每一条丢失的消息都意味着不正确的计数值。

我们在第10章中研究的批处理系统有一个优点，那就是它们提供了很强的可靠性保证：失败的任务自动重试，失败任务的部分输出自动丢弃。这意味着如果没有发生错误，输出都是一样的，这有助于简化编程模型。在这一章稍后，我们将研究如何在流中提供类似的保证。

#### 生产者直接向消费者传递信息

许多消息传递系统使用生产者和消费者之间的直接网络通信，而不通过中间节点：

* UDP组播在金融行业中被广泛应用于诸如股票市场提要这样的流中，在这些流中低延迟是非常重要的。尽管UDP本身是不可靠的，但应用程序级别的协议可以恢复丢失的数据包（生产者必须记住它发送的数据包，以便按需重新传输它们）。

* 诸如ZeroMQ和nanomsg这样的无代理消息库采取了类似的方法，通过TCP或IP多播实现发布/订阅消息传递。

* StatsD与Brubeck使用不可靠的UDP消息来收集网络上所有设备的指标，并监视它们。（在StatsD协议中，只有在接收到所有消息时计数器值才是正确的；使用UDP使得指标只是最好的近似值。也请参阅“TCP与UDP”一节）

* 如果消费者在网络上公开了一个服务，生产者可以直接发出HTTP或RPC请求（见“透过服务的数据流：REST与RPC”一节）把消息推送给消费者。这就是webhook背后的理念，在这种模式中，一个服务的回调URL被注册到另一个服务中，每当发生事件时，它都会向该URL发出请求。

尽管这些直接消息传递系统在所设计的情况下运行良好，但它们通常要求应用程序代码注意消息丢失的可能性。他们所能容忍的错误是非常有限的：即使协议检测并重新传输网络中丢失的数据包，它们通常也假定生产者和消费者一直在线。

如果消费者离线，它可能会错过消息无法送达时发出的消息。有些协议允许生产者重发失败的消息，但是如果生产者崩溃，这种方法也就失效了，也失去所有缓冲区内的消息。

#### 消息代理

一种广泛使用的替代方案是通过*消息代理*（也称为消息队列）发送消息，它本质上是一种为处理消息流而优化的数据库。它作为服务器运行，生产者和消费者作为客户端连接到它。生产者将消息写入代理，消费者通过从代理读取消息来接收消息。

通过把数据集中在代理中，这些系统可以更容易地容忍来来去去的客户机（连接、断开和崩溃），而且持久性的问题也被转移到代理上。一些消息代理只把消息保存在内存中，而另一些消息代理（取决于配置）把它们写入磁盘，以便在代理崩溃时不会弄丢它们。面对缓慢的消费者，他们通常允许无限制排队（而不是丢弃消息或是反向压力），尽管这种选择也会取决于配置。

排队的结果也导致消费者通常是*异步的*：当生产者发送消息时，它通常只等待代理确认它已经缓存了消息，而不会等待消息被消费者处理。对消费者的交付将发生在未来的某个时间点——通常在一秒内，但如果存在队列积压，有时会明显更久一些。

#### 消息代理与数据库的比较

一些消息代理甚至可以参与到使用XA或JTA的两阶段提交协议（见“实践中的分布式事务”一节）。这一特性使它们在性质上与数据库非常相似，尽管消息代理和数据库之间仍然存在着重要的实际差异：

* 数据库通常会保存数据，直至它被显式地删除，而大多数消息代理在消息成功传递给消费者后会自动删除它。这样的消息代理不适合长期的数据存储.

* 由于会很快地删除消息，因此大多数消息代理都假定它们的工作集相当小——即队列很短。如果因为消费者速度缓慢（也许是因为内存无法容纳更多地消息，导致消息溢出到磁盘）代理需要缓冲大量消息，那么每个消息需要更长的时间来处理，导致总体吞吐量会下降。

* 数据库通常支持次级索引以及各种数据搜索方式，而消息代理通常支持订阅与某种模式匹配的主题子集的某种方式。这些机制是不同的，但本质上这两种方法都是客户端选择它想知道的数据部分的方法。

* 在查询数据库时，结果通常基于数据某个时间点的快照；如果另一个客户端随后向数据库写入了改变了查询结果的内容，第一个客户端不会发现先前的结果现在已经过时（除非它重复查询，或者轮询变更）。相比之下，消息代理不支持任意查询，但是它们确实在数据更改时通知客户端（即，新消息可用时）。

这是对消息代理的传统视角，它被封装在JMS和AMQP等标准中，并在诸如RabbitMQ、ActiveMQ、HornetQ、Qpid、TIBCO企业消息服务、IBM MQ、Azure服务总线和Google Cloud Pub/Sub等软件中实现。

#### 多个消费者

当多个消费者读取同一主题中的消息时，主要使用两种消息传递模式，如图11-1所示：

*负载均衡*

每条消息都被传递给*一*个消费者，这样消费者就可以在主题中共享处理消息的工作。代理可以把消息指定给任意消费者。当消息处理成本很高时这个模式非常有用，因此你希望能够添加使用者来并行处理。（在AMQP中，你可以通过让多个客户端从同一个队列消费来实现负载平衡，而在JMS中它被称为*共享订阅*。）

*散出*

每条消息都被传递给*所有*的消费者。散出允许多个独立的消费者“收听”到相同的消息广播，而互不影响——这种流相当于有几个不同的批处理作业读取相同的输入文件。（JMS中的主题订阅和AMQP中的交换绑定提供了这个特性。）

Each message is delivered to *all* of the consumers. Fan-out allows several independent consumers to each “tune in” to the same broadcast of messages, without affecting each other — the streaming equivalent of having several different batch jobs that read the same input file. (This feature is provided by topic subscriptions in JMS, and exchange bindings in AMQP.)

*图11-1. (a) 负载平衡: 把消费主题的工作在消费者中共享； (b) 散出: 把每个消息发送给多个消费者。*

这两种模式可以组合在一起：例如，两组不同的使用者可以订阅一个主题，这样每个组可以集体接收所有消息，但是在每个组中只有一个节点接收到每个消息。

#### 应答与重发

消费者可能随时崩溃，因此可能发生这样的情况：代理消息传递给消费者，但消费者没有处理它，或者在崩溃之前只对其进行了部分的处理。为了确保消息不丢失，消息代理使用*应答消息*：客户端在处理消息完成后必须显式地告诉代理，以便代理可以把它从队列中删除。

如果到客户端的连接关闭或超时，而代理没有收到应答，它会假定消息没有被处理，于是它会再次把消息传递给另一个消费者。（请注意，消息实际上*已经*被完全处理但是应答消息在网络中丢失了，这是会发生的。处理这种情况需要原子提交协议，如“实践中的分布式事务”一节中所讨论的那样。）

与负载平衡相结合的时候，这种重发行为对消息排序的影响很有趣。在图11-2中，消费者通常按照生产者发送消息的顺序处理消息。然而，消费者2在处理消息*m3*时崩溃，同时消费者1正在处理消息*m4*。随后把未经确认的消息*m3*重新传递给消费者1，结果消费者1按照*m4*、*m3*、*m5*的顺序处理消息。因此，*m3*和*m4*的交付顺序与生产者1发送的顺序不同。

*图11-2. 消费者2在处理m3的时候崩溃，于是稍后m3被重新发送到消费者1。*

即使消息代理尝试保护消息的顺序（根据JMS和AMQP标准的要求），负载平衡与重发的组合不可避免地导致消息被重新排序。为了避免这个问题，你可以为每个消费者使用独立的队列（即，不使用负载平衡特性）。如果消息完全相互独立，消息重新排序不是问题，但是如果消息之间存在因果依赖关系这就很重要了，这一点我们将在这一章后面看到。

### 分区日志

通过网络发送数据包，或是向网络服务发出请求通常是暂时的操作，不会留下永久的跟踪记录。虽然是可以永久记录它（使用数据包捕获和日志记录），但我们通常不会这样想。即使是那些把消息固化到磁盘的消息代理，在消息传递给消费者之后也会很快再次删除它们，因为这些都是建立在短暂的消息传递思维基础上的。

数据库和文件系统采取相反的方法：写入数据库或文件的所有内容通常都会被永久记录，至少直到有人显式地选择再次删除为止。

这种思维方式的差异对衍生数据的构建方式有很大的影响。正如第10章中所讨论的，批处理过程的一个关键特性是你可以重复运行它们，试验处理步骤，而没有损坏输入的风险（因为输入是只读的）。AMQP/JMS风格的通信不是这样的：如果应答导致消息从代理中删除，那么接收消息就是破坏性的，所以你不能再次运行相同的消费者并期望得到相同的结果。

如果添加新的消费者到消息传递系统，它通常只在注册之后才可以接收消息；任何之前的消息都已经不在了，也无法恢复。与文件和数据库相比，你可以在任何时候添加一个新客户端，并且它可以读取过去任意写入的数据（只要它还没有被应用程序显式覆盖或删除）。

为什么我们不能把数据库的持久存储方法与消息传递的低延迟通知功能结合起来呢？这就是*基于日志的消息代理*背后的理念。

#### 使用日志进行消息存储

日志就是磁盘上只可附加的记录序列。我们先前在第3章中讨论日志结构存储引擎与预写入日志时讨论过日志，在第5章中讨论复制的时候也有涉及。

同样的结构也可以用来实现消息代理：生产者通过把消息附加到日志末尾发送消息，而消费者通过顺序读取日志接收消息。如果消费者到达日志的末尾，它会等待新消息被写入的通知。Unix工具`tail-f`本质上说就是这样工作的，它监视被追加的数据的文件。

为了扩展到一个磁盘所能提供的更高的吞吐量，日志可以被*分区*（在第6章的意义上）。然后，可以把不同的分区托管在不同的设备上，从而使每个分区成为一个独立的、可以互相读写的日志。之后，可以把主题定义为一组分区，这些分区都携带相同类型的消息。这种方法如图11-3所示。

在每个分区中，代理为每个消息分配一个单调递增的序列号，或*偏移量*（在图11-3中，方框中的数字是消息偏移量）。这样的序列号是有意义的，因为分区只能附加，所以分区中的消息是全序的。而在不同的分区之间则没有顺序保证。

*图11-3. 生产者通过把消息添加到主题分区文件来发送消息，而消费者按顺序读取这些文件。*

Apache Kafka、Amazon Kinsis Streams，以及Twitter的DistributedLog都是这样基于日志的消息代理。Google Cloud Pub/Sub在体系结构上是相似的，但公开的是JMS风格的API，而不是抽象日志。即使这些消息代理把所有消息写入磁盘，它们也能够通过跨多台机器进行分区实现每秒数百万条消息的吞吐量，同时通过复制消息实现容错机制。

#### 与传统消息传递相比的日志

基于日志的方法很少支持散出的消息传递，因为几个消费者可以独立读取日志而不会相互影响——读取消息并不会把它从日志中删除。为了在一组消费者之间实现负载平衡，代理可以把整个分区指定给消费者群组中的节点，而不是把单个消息分配给消费者客户端。

每个客户端于是使用它所指定分区中的*所有*消息。通常，当消费者被指定了一个日志分区后，它会以简单的单线程方式顺序地读取分区中的消息。这种粗粒度的负载平衡方法有一些缺点：

* 分享使用主题的节点数量最多可以是该主题中的日志分区数，因为同一分区内的消息被发送到了相同的节点。

* 如果单个消息的处理速度较慢，那么它会阻塞分区中后续消息的处理（队首阻塞的一种形式；参见“描述性能”一节）。

因此，如果消息的处理成本很高，并且你希望在逐条处理消息的基础上做并行处理，或者消息排序并不那么重要，那么最好采用JMS/AMQP风格的消息代理。另一方面，在消息吞吐量高、每条消息处理速度快、消息排序重要的情况下，基于日志的方式工作得非常好。

#### 消费者偏移量

按顺序消费一个分区可以很容易地判断哪些消息已经被处理了：所有偏移量小于消费者当前偏移量的消息都已经被处理，而所有偏移量较大的消息都是没有处理的。因此，代理不需要跟踪每条消息的应答——它只需要定期记录消费者偏移量。在这种方法中，减少的簿记开销以及有机会使用批处理和流水线有助于提高基于日志的系统吞吐量。

这个偏移量实际上非常类似于单主机数据库复制中的*日志序列号*，我们在“设置新从机”一节中讨论过这个问题。在数据库复制中，日志序列号使得从机在断开与主机的连接之后重新连接，并在不跳过任何写入的情况下继续复制。这里使用了完全相同的原则：消息代理的行为类似于主机数据库，而消费者就像从机一样。

如果消费者节点失效，消费者组中的另一个节点被指定到失败的消费者分区，并在最后记录的偏移量处开始处理消息。如果消费者已经处理了后续消息但没有记录其偏移量，这些消息会在重新启动之后再次被处理。我们会在本章稍后讨论处理这个问题的方法。

#### 磁盘空间的使用

如果你一直添加日志，最终会耗尽磁盘空间。为了回收磁盘空间，日志实际上被分成几段，并不时地删除或移动旧的段到存档空间。（稍后我们会讨论一种更复杂的释放磁盘空间的方法。）

这意味着，如果一个慢的消费者跟不上消息到达的速率，并且它远远落后，它的消费者偏移量指向了一个已经删除了的段，那么它会错过一些消息。实际上，日志实现了一个有界的缓冲区，当它装满的时侯会丢弃旧消息，也被叫做*循环缓冲区*或*环形缓冲区*。但是由于缓冲区位于磁盘上，所以它可以相当大。

让我们做一个粗略的计算。在编写这本书的时候，常见的大容量硬盘容量为6 TB，顺序写入吞吐量为150 MB/s。如果你正在以最快的速度写入消息，那么你需要11个小时才能塞满这个硬盘。因此，磁盘可以缓冲11个小时的消息，之后会开始覆盖旧的消息。即使你使用许多硬盘与设备，这一比率都保持不变。实际上，部署的应用很少使用所有的磁盘写入带宽，因此日志通常可以保存几天甚至几周的消息缓冲。

不管消息保留多长时间，日志的吞吐量都或多或少保持不变，因为每条消息反正都写入磁盘。这种行为与默认情况下把消息保存在内存中，并且只在队列增长过大时才把消息写入磁盘的消息系统形成了对比：这种系统在队列较短时速度快，在开始写入磁盘之后就变慢很多，因此吞吐量取决于保留的历史记录数量。

#### 当消费者跟不上生产者的时候

在“消息系统”一节开始的时侯，我们讨论了如果消费者跟不上生产者发送消息的速度，可以做的三种选择：丢弃消息、缓冲或是应用反向压力。在这个分类法中，基于日志的方法是一种具有很大但是固定大小缓冲区的缓冲形式（受可用磁盘空间的限制）。

如果消费者远远落后，它需要的消息比磁盘上保留的消息还要旧，那么它将无法读取这些消息——因为代理实际上丢弃了比缓冲区大小还要远的旧消息。你可以监视消费者落后日志头部的距离，并在明显落后时发出警报。由于缓冲区很大，有足够的时间让人工操作员修复缓慢的使用者，使它在开始错过消息之前赶上。

即使消费者确实落后得太多并开始错过消息，也只有这个消费者会受到影响；它不会破坏针对其他消费者的服务。这是一个很大的运营优势：你可以为开发、测试或调试目的尝试消费生产日志，而无须担心会中断生产服务。当一个消费者被关闭或是崩溃了，它停止消费资源——唯一剩下的就是它的消费者偏移量。

这种行为也与传统的消息代理形成了鲜明对比，在传统的消息代理中，你需要小心地删除任何其消费者已关闭的队列——不然它们会继续不必要地累积消息，并且占据那些仍处于活动状态的消费者的内存。

#### 重播旧消息

我们前面注意到，使用AMQP和JMS风格的消息代理，处理和确认消息是一种破坏性操作，因为它会导致消息在代理上被删除。另一方面，在基于日志的消息代理中，消费消息更像是从文件中读取：它是一个不改变日志的只读操作。

除了任何消费者的输出之外，处理的唯一副作用是消费者偏移量向前移动。但是偏移量是由消费者控制的，如果需要的话它可以很容易地操纵：例如，你可以使用昨天的偏移值启动一个消费者的副本，并将输出写到一个不同的位置，以便重新处理前一天的所有消息。你可以多次重复此操作，更改处理代码。

这一方面使得基于日志的消息更像最后一章的批处理，其中的衍生数据通过一个可重复的转换过程与输入数据分离。它允许更多的实验，更容易地从错误和错误中恢复，使它成为组织内数据流集成的好工具。

## 数据库与流

我们对消息代理和数据库进行了一些比较。尽管传统上它们被认为是不同类别的工具，但我们看到，基于日志的消息代理已经成功地从数据库中获取了一些想法，并把它们应用于消息传递。我们也可以反过来：从消息传递和流中获取想法，并把它们应用到数据库中。

我们之前说过，事件是对某个时间发生的事情的记录。发生的事情可以是用户操作（例如，键入搜索查询语句），也可以是传感器读取，但也可以是*对数据库的写入*。把某些内容写入数据库是个可以捕获、存储和处理的事件。这一观察表明，数据库和流之间的连接比磁盘上日志的物理存储更深入——这是非常基本的。

实际上，复制日志（见“复制日志的实现”一节）是数据库写入事件的流，由主机在处理事务时生成。从机把这种写入流应用到它们自己的数据库副本中，从而得到包含相同数据的精确副本。复制日志中的事件描述了发生的数据变更。

我们还在“全序广播”一节中遇到了*状态机复制*原则，其中规定：如果每个事件代表对数据库的一次写入，并且每个副本以相同的顺序处理相同的事件，那么副本最终都将处于相同的状态。（把处理事件假定为确定性操作。）这只是又一个事件流的例子！

在本节中，我们首先将研究在异构数据系统中出现的问题，然后探讨如何通过把事件流的思想引入数据库来解决它。

### 保持系统之间同步

正如我们在本书中所看到的，没有一个系统能够同时满足所有的数据存储、查询和处理需求。实践中，大多数重要的应用程序需要结合几种不同的技术才能满足它们的需求：例如，使用OLTP数据库来服务用户请求，使用缓存来加速常见的请求，使用全文索引来处理搜索查询，以及使用数据仓库进行分析。每个系统都有自己的数据副本，使用为自己的目标优化过的表现形式存储。

由于相同或类似的数据出现在了几个不同的地方，它们需要彼此保持同步：如果某一项在数据库中更新了，那么它也需要在缓存、搜索索引和数据仓库中更新。对于数据仓库来说，这种同步通常由ETL进程执行（参见“数据仓库”一节），通常通过获取数据库的完整副本、转换它，并把它批量加载到数据仓库中——换句话说，是一个批处理过程。类似地，我们在“批处理工作流的输出”一节中看到了如何使用批处理过程创建搜索索引、推荐系统以及其他衍生数据系统。

如果周期性导出完整数据库太慢，有时用到的一个备选方案是*双写*，应用程序代码在数据变化时显式地写入每个系统：例如，首先写入数据库，然后更新搜索索引，之后使缓存条目无效（或者是并发执行这些写入）。

然而，双写有一些严重的问题，其中之一是如图11-4所示的一个竞赛条件。在这个例子中，两个客户端同时希望更新一个项X：客户端1希望将值设置为A，客户端2希望将其设置为B。两个客户端首先把新值写入数据库，然后把它写入搜索索引。由于时机不佳，请求交织在一起：数据库首先看到来自客户端1写入值A，然后来自客户端2的写入把值设置为B，所以数据库中的最终值是B。搜索索引首先看到客户端2的写入，然后看到客户机1，于是搜索索引中的最终值是A。即使没有发生错误，但是现在彼此之间永久不一致了。

*图11-4. 在数据库中，X首先设为了A然后是B，而在搜索索引中写入到达的顺序恰好相反。*

除非有一些额外的并发检测机制，比如我们在“检测并发写入”一节中讨论过的版本向量，否则你甚至不会注意到并发写入的发生——一个值就这样悄悄地覆盖了另一个值。

双写的另一个问题，是其中一个写入会失败而另一个成功了。这是一个容错问题而不是并发问题，但是它也会使这两个系统变得不一致。确保它们都成功或都失败是原子提交问题的一个例子，解决这个问题的代价很高（见“原子提交和两阶段提交（2pc）”一节）。

如果你只有单个复制数据库和单个主机，那么这台主机决定写入的顺序，因此状态机复制方法在数据库的副本之间有用。然而在图11-4中不是只有单个主机：数据库可能有一个主机，搜索索引可能有一个主机，但两者互不为主从关系，因此可能会发生冲突（见“多主机复制”一节）。

如果真的只有一个主机——比如，数据库的——以及如果我们能够使搜索索引成为数据库的从机，情况会更好。但这在实践中可行吗？

#### 变更数据捕获

大多数数据库复制日志都有的问题是，它们长期以来一直被认为是数据库的内部实现细节，而不是公共API。客户端应该通过数据库的数据模型和查询语言来查询数据库，而不是解析复制日志然后尝试从其中提取数据。

数十年来，许多数据库根本没有记录在案的方法来获取写入它们的更改日志。因此，很难把数据库中的所有变更复制到其它的存储技术中，比如搜索索引、缓存，或是数据仓库。

最近，人们对*变更数据捕获*（CDC）越来越感兴趣，它是观察写入数据库的所有数据变更以及提取可以复制到其他系统形式的过程。如果变更可以在写入的时候作为流立即可用，CDC就更有趣了。

比如说，你可以抓取数据库中的更改，然后不间断地把相同的变更应用在搜索索引上。如果以相同的顺序应用更改日志，那么可以期望搜索索引中的数据与数据库中的数据匹配。搜索索引以及任何其他衍生数据系统只是变更流的消费者，如图11-5所示。

*图11-5. 以数据被写入的顺序获取它，并以相同的顺序应用变更到其它系统中。*

#### 实现变更数据捕获

我们把日志消费者叫做*衍生数据系统*，正如在第三部分的介绍中谈到的：存储在搜索索引和数据仓库中的数据只是记录系统中数据的另一种视图。变更数据捕获是一种机制，用于确保对记录系统所做的所有变更也反映在衍生数据系统中，从而使衍生系统有数据的精确副本。

本质上，更改数据捕获使一个数据库成为主机（从其中捕获变更的数据库），并把其他数据库变为从机。基于日志的消息代理非常适合从源数据库传输变更事件，因为它保留了消息的顺序（避免了图11-2中重新排序的问题）。

数据库触发器可以通过注册触发器来实现变更数据捕获（见“基于触发器的复制”一节），这些触发器可以观察数据表的所有更改，并把相应的条目添加到变更日志表中。然而它们往往很脆弱，并且有明显的性能消耗。解析复制日志可以是一种更健壮的方法，尽管它也会带来挑战，比如处理模式定义的变更。

领英的Databus、Facebook的Wormhole和雅虎的Sherpa都大规模使用了这个理念。Bottled Water实现PostgreSQL的CDC的时候使用到了一个对预写入日志进行解码的API，Maxwell和Debezium通过解析binlog为MySQL做类似的事情，Mongoriver读取MongoDB的oplog，GoldenGate为Oracle提供了类似的工具。

与消息代理一样，变更数据捕获通常是异步的：记录数据库系统在提交更改之前不会等待变更应用于使用者。这种设计的操作优势是增加一个缓慢的消费者不会对记录系统产生太大的影响，但它的缺点是所有复制滞后的问题都适用（见“复制滞后的相关问题”一节）。

#### 初始快照

如果有记录对数据库所做的所有变更的日志，那么可以通过重新执行日志内的变更来重构数据库的状态。然而在许多情况下，永远保存所有变更会需要太多的磁盘空间，而重放执行则需要太长时间，因此日志需要被截断。

比如说，构建一个新的全文索引需要整个数据库的完整副本，只是应用包含最近变更的日志是不够的，因为它没有包含最近没有更新的项。因此，如果你没有完整的日志历史记录，你需要从一个一致的快照开始，正如前面在“设置新的从机”一节中所讨论的那样。

数据库的快照必须与变更日志中已知的位置或偏移量相对应，这样你就可以知道从哪个点开始应用快照生成后的更改。一些CDC工具集成了这种快照工具，而其它的则把它作为手动操作。

#### 日志压缩

如果只能保留有限数量的日志历史记录，那么每次添加新的衍生数据系统时，都需要经历快照处理。但是，*日志压缩*提供了另一个很好的选择。

我们在前面的“哈希索引”一节，讨论日志结构存储引擎时聊到了日志压缩（见图3-2的例子）。原理很简单：存储引擎定期查找具有相同键的日志记录，丢弃任何重复项，只保存每个键的最近的更新。这个压缩与合并的过程运行在后台。

在日志结构的存储引擎中，更新特殊的空值（一个*墓碑*）表示键被删除，并且导致它在日志压缩期间被删除。但是只要键没有被覆盖或删除，它就一直留在日志中。压缩日志所需的磁盘空间仅取决于数据库的当前内容，而不是数据库中发生的写入次数。如果相同的键经常被覆写，以前的值最终会被垃圾回收，只有最新的值会被留下。

同样的理念也适用于基于日志的消息代理与变更数据捕获。如果CDC系统被设置为每个变更都有一个主键，而每一次对键的更新都替换了这个键的前一个值，那么只保留对键的最新写入就足够了。

现在，每当你想要重建衍生数据系统，比如搜索索引的时候，就可以从日志压缩主题的偏移量0开始启用新的消费者，并对日志中的所有消息进行顺序扫描。日志可以保证包含数据库中每个键的最新值（或许还有一些旧的值）——换句话说，你可以用它获得数据库内容的完整副本，而无需对CDC源数据库获取另一次快照。

Apache Kafka支持这个日志压缩特性。正如我们将在本章后面看到的，它允许使用消息代理进行持久性存储，而不仅仅是用于临时地消息传递。

#### 变更流的API支持

越来越多的数据库开始把变更流作为第一等接口，而不是典型改造以及逆向工程后的CDC。举个例子，RethinkDB允许查询语句订阅查询结果变更的通知，Firebase与CouchDB提供基于变更源的数据同步，变更源也同时对应用程序可用，Meteor使用MongoDB的操作日志来订阅数据变更，从而更新用户界面。

VoltDB允许事务以流的形式从数据库持续导出数据。数据库把关系数据模型中的输出流表示为事务可以插入元组但不能查询的表。这样，流包含了提交事务写入这个特殊表的元组日志，它们是按照提交的顺序排列的。外部用户可以异步地消费这个日志，并用它更新衍生数据系统。

Kafka Connect是一个杰作，它把广泛用于各种数据库系统的变更数据捕获工具与Kafka集成起来。一旦变更事件流在Kafka中，就可以用来更新衍生数据系统，比如搜索索引，也可以像本章后面讨论的那样输入到流处理系统。

### 事件溯源

我们在这里讨论的想法与*事件源*，一种在领域驱动设计（DDD）社区开发的技术之间有一些相似之处。我们将简要讨论事件溯源，因为它包含了一些对流系统有用的相关想法。

与变更数据捕获类似，事件溯源也涉及到把对应用程序状态做的所有变更存储为变更事件的日志。最大的区别在于，事件溯源在另一个的抽象级别上应用了这种想法：

* 在变更数据捕获中，应用程序以可变的方式使用数据库，随时更新和删除记录。变更日志在一个较低层次从数据库中提取出来（比如通过解析复制日志），这确保了从数据库中提取写入的顺序与实际写入的顺序相匹配，从而避免了图11-4中的竞争条件。写入数据库的应用程序不需要知道CDC正在发生。

* 在事件溯源中，应用程序逻辑显式基于写入事件日志的不可变事件。在这种情况下，事件的存储是只允许附加的，并且不鼓励甚至禁止更新或删除。事件是被设计用来反映应用程序级别上发生的事情，而不是低级别的状态变化。

事件溯源是种强大的数据建模技术：从应用程序的角度来看，把用户的操作记录为不可变事件更有意义，而不是记录这些操作对可变数据库的影响。事件溯源可以使应用程序随着时间的推移更容易发展，有助于调试，因为它可以在事情发生后更容易理解，并防止应用程序的bug（见“不可变事件的优点”一节）。
  
例如，存储“学生取消了他们的课程注册”事件以中立的方式清楚表达了单个操作的意图，而副作用“从注册表中删除了一个条目，并在学生反馈表中添加了一个取消原因”嵌入了许多关于数据使用方式的假设。如果引入了一个新的应用功能——例如，“这个位置提供给下一个等候名单上的人”——事件溯源方法可以轻松地把新的副作用与现有事件联系起来。

事件溯源类似于编年史数据模型，并且事件日志与星型模式的事实表也有相似之处（见“星星和雪花：分析用的模式”一节）。

类似Event Store这样的专业化数据库被开发出来，支持使用事件溯源的应用程序，但是通常这种方法不依赖于任何特定的工具。传统的数据库，或者基于日志的消息代理也可以用于构建这种类型的应用程序。

#### 从事件日志衍生当前状态

事件日志本身并不十分有用，因为用户通常希望看到系统的当前状态，而不是修改的历史。例如，在购物网站上，用户希望能够看到他们购物车的当前内容，而不是他们对购物车所做所有变更的列表。

因此，使用事件溯源的应用程序需要获取事件日志（表示*写入*系统的数据），然后把它们转换为适合向用户显示的应用程序状态（从系统中*读取*数据的方式）。这种转换可以使用任意的逻辑，但是它应该是确定性的，这样你就可以再次运行它并从事件日志中衍生出相同的应用程序状态。

与变更数据捕获一样，重现事件日志使得你可以重新构建系统当前的状态。但是，这时需要以不同的方式处理日志压缩：

* 用于更新记录的CDC事件通常包含记录的整个新版本，因此主键的当前值完全由该主键的最新事件决定，日志压缩可以丢弃同一键之前的所有事件。

* 另一方面，通过事件溯源，事件是在更高级别上建模的：事件通常表示用户操作的意图，而不是由于操作而发生的状态更新的机制。在这种情况下，以后的事件通常不会覆盖以前的事件，因此你需要完整的事件历史记录来重构最终状态。日志压缩不可能以相同的方式进行。

使用事件溯源的应用程序通常有一些机制来存储从事件日志衍生的当前状态快照，因此它们不需要重复地重新处理完整的日志。但是，这只是性能优化，提高了读取以及从崩溃状态恢复的速度；它的目的是系统能够永远存储所有原始事件，并在需要的时候重新处理完整的事件日志。我们在“不变性的局限性”一节中讨论了这一假设。

#### 命令与事件

事件溯源的哲学仔细区分了*事件*与*命令*。当用户的请求第一次到达时，它最初是一个命令：此时它仍然可能失败，比如说，因为违反了某些完整性条件。应用程序必须首先验证它是否能够执行命令。如果验证成功并且接受了命令，它变成了一个事件，它是持久的和不可变的。

例如，用户尝试注册特定的用户名，或是预定飞机上或剧院里的座位，那么应用程序需要检查这个用户名或是座位是否已被占用。（我们之前在“可容错的协商一致”一节中讨论过这个例子。）当这个检查成功完成之后，应用程序可以生成一个事件，指明特定的用户名已经被特定的用户ID注册，或者是某个特定的座位已经为特定的客户保留。

当事件生成的时间点，它变成了一个*事实*。即使客户稍后决定更改或是取消预订，事实仍然为真：他们之前预订了一个特定座位，而更改或是取消是一个单独的事件，稍后才添加进来的。

不允许事件流的消费者拒绝事件：当消费者看到事件时，它已经是日志不可变的一部分了，而且其他使用者可能已经看到了。因此，命令的任何验证都要在它成为事件之前同步进行——比如使用可串行化的事务，原子性地验证命令并发布事件

或者，用户预订座位的请求可以拆分为两个事件：首先是意向性预订，一旦验证完毕之后是一个单独的确认事件（就像“使用全序广播实现线性化存储”中讨论的那样）。这样的拆分允许验证以异步的方式进行。

### 状态、流与不可变性

我们在第10章中看到，批处理获益于其输入文件的不可变性，因此你可以在现有的输入文件上运行实验性质的处理作业，而不必担心会损坏它们。这种不可变的原则也使得事件源和变更数据捕获变得如此强大。

我们通常认为数据库存储的是应用程序的当前状态——这种表示方式为读取做了优化，用于查询的服务也是最方便的。状态的本质是它是变化的，因此数据库支持更新、删除以及插入数据。这如何与不变性相适应的？

每当你有可变的状态时，这个状态就是随着时间推移改变了它的事件的结果。比如说，当前可用的座位列表是所有处理过的预订结果，当前帐户余额是帐户上信用与借记的结果，Web服务器的响应时间图是所有已发生的Web请求的响应时间的汇总。

无论状态如何变化，总是由一系列事件导致这些变化。即使是事情完成之后又回滚了，这些事件确实发生了的事实仍然为真。核心的理念是可变的状态与仅可以添加的不可变事件日志彼此并不矛盾：它们是同一枚硬币的两面。所有更改的日志，即变更日志，表现了随着时间变化的状态演进。

如果你偏好数学，你会说应用程序状态是随着时间变化集成事件流得到的，而更改流是你按时间区分状态得到的结果，如图11-6所示。这种类比有局限性（比如，状态的二阶导数似乎没有什么含义），但是，它是一个思考数据有用的起点。

*图11-6. 当前应用状态与事件流之间的关系。*

如果你长久地存储变更日志，这只会使状态可以复现。如果你认为事件日志就是你的记录系统，并且任何可变状态都是由它衍生出来的，那么对系统中数据的流动进行推理就变得更加容易了。正如帕特·赫兰所说的：

    事务日志记录了所有对数据库所做的更改。高速添加是唯一更改日志的方式。从这个角度来看，数据库的内容保存了日志中最新记录值的缓存。日志就是真相。数据库是日志子集的缓存。这个缓存了的子集恰好是日志中每个记录和索引值的最新值。

如“日志压缩”一节中所讨论的，日志压缩是弥合日志和数据库状态之间区别的方法之一：它只保留了每条记录的最新版本，而丢弃了被复写的版本。

#### 不可变事件的优势

数据库中的不可变性是一种古老的观念。比如几个世纪以来，会计在财务记录中一直使用不可变性。当一笔交易发生时，它被记录在一个只可以添加的*分类账*中，它本质上是在描述经手的金钱、货物或者服务的事件日志。这些账户，比如亏损与收益，或是资产负债表，是通过把分类账中的交易相加而得的。

如果犯了错，会计师不会删除或是更改分类账中不正确的交易——相反，他们会增加另一笔交易用于补偿错误，比如说返还不正确的收费。不正确的交易仍然一直留存在于分类账中，因为它对于审计来说也许是重要的。如果从不正确的分类账中得出的不正确的数字已经公布，那么下一个会计期间的数字会包括一个修正值。这个过程在会计业中是很普通的。

尽管这种可审计性在金融系统中特别重要，但是对许多不受这种严格监管的其他系统来说也是有益的。正如“批处理输出的哲学”一节中所讨论的那样，如果你意外地部署了写入错误数据到数据库的错误代码，如果代码能够破坏性地覆盖数据，那么恢复就会困难得多。有了只能添加的不可变事件日志，诊断到底发生了什么，并且从问题中回复就容易很多了。

相比于只有当前的状态，不可变事件还捕获了更多的信息。比如说在购物网站上，客户可能会先添加一个物品到他的购物车，然后再删掉。虽然从订单履行的角度上看第二个事件取消了第一个事件，但是出于分析的目的，了解客户原本正在考虑某个特定物品，但随后决定不要了可能会更有用。也许他们将来会选择买它，或者他们找到了一个替代品。事件日志记录了这个信息，但是如果信息是在数据库中，物品从购物车中删除的那一刻它就丢了。

#### 从同一个事件日志衍生几个视图

此外，通过把可变状态与不可变事件日志分离，你可以从同一个的事件日志中衍生出好几种不同的面向读取的表示形式。这就像有多个流消费者一样（图11-5）：比如说分析数据库Druid使用这种方法直接从Kafka获取，Pistachio是一种分布式的键值存储，它使用Kafka作为提交日志，而Kafka Connect接收器可以把数据从Kafka导出到各种不同的数据库和索引。对于许多其他存储和索引系统，比如搜索服务器来说，类似地从分布式日志中获取它们的输入也是有意义的（见“保持系统同步”一节）。

有一个从事件日志到数据库的显式转换步骤可以使你的应用程序随着时间推移更容易地发展：如果你想引入一个以某种新的方式显示现有数据的新特性，你可以使用事件日志为新功能构建一个独立的读取优化视图，并且与现有系统并行运行而无需修改它们。在现有系统中并行运行新旧系统通常比执行复杂的模式迁移更容易。一旦不再需要旧系统，你只需要关闭它然后回收分配给它的资源。

如果你不用担心数据是如何查询和访问的，那么存储数据通常是非常简单的；模式设计、索引和存储引擎的许多复杂性都是由于希望支持特定的查询和访问模式（见第3章）。由于这个原因，你可以通过将数据写入的形式与读取的形式分开，并且允许有几个不同的读取视图，从而获得更大的灵活性。这种理念被称为*命令查询职责分离*（CQRS）。

数据库与模式设计的传统方法基于这样一种谬误，即数据必须以与查询相同的形式写入。如果你可以把数据从写入优化的事件日志转换为读取优化的应用程序状态，那么关于规范化和反规范化的争论（见“多对一与多对多关系”一节）很大程度上就变得无关紧要了：在读取优化的视图中去反规范化数据是完全合理的，因为转换过程为你提供了一种机制，使其与事件日志保持一致。

在“描述负载”一节中，我们讨论了Twitter的主页时间线，这是用户关注的人们最近写入的推文的缓存（好像邮箱一样）。这是另一个阅读优化状态的例子：主页时间线高度非正规化，因为你的推文在所有关注你的人的时间线中都复制了一份。但是，扇出服务使这种复制状态与新的tweet和新的关注关系保持同步，从而使复制易于管理。

#### 并发控制

事件溯源与变更数据捕获的最大缺点是事件日志的消费通常是异步的，因此有这样的可能性：用户对日志发起一次写入，然后从日志衍生的视图中读取，发现他们的写入还没有反映在这个读取视图中。我们先前在“读取自己的写入”中讨论过这个问题，还有可能的解决方案。

一种解决方案是随着事件添加到日志，同步执行读取视图的更新。这需要一个事务把写入操作组合到一个原子单元中，所以你要么把事件日志和读取视图保存在同一个存储系统中，或者你需要一个跨越不同系统的分布式事务。或者，你可以使用我们在“使用全序广播实现可线性存储”一节中讨论的方法。

另一方面，从事件日志衍生出当前状态同时简化了并发控制的某些方面。大部分对于多对象事务（见“单对象与多对象操作”一节）的需求来自于一个需要在几个不同的地方更改数据的操作。有了事件溯源，你可以设计一个事件，使其成为用户操作的独立描述。然后，用户操作只需要在一个地方进行一次写入——也就是把事件添加到日志——这很容易被原子化。

如果以相同的方式对事件日志以及应用程序状态进行分区（例如，为分区3中的客户处理事件只需要更新应用程序状态的分区3），那么简单的单线程日志消费者不需要对写入进行并发控制——根据构造，它一次只处理一个事件（也参考“实际串行执行”一节）。日志通过在分区中定义事件的顺序来消除并发的不确定性。如果一个事件涉及多个状态分区，那么需要做更多的工作，我们会在第12章中讨论这一点。

#### 不可变性的限制

然而许多不使用事件源模型的系统也依赖于不可变性：不同的数据库内部使用不变的数据结构或多版本数据来支持时间点快照（见“索引与快照隔离”一节）。版本控制系统，比如Git、Mercurial以及Fossil，也依赖于不可变的数据来保存文件的版本历史。

在多大程度上可以永久保存所有变更的不可变历史？答案取决于数据集中的变动程度。某些种类工作主要是添加数据，而很少更新或是删除；它们很容易变得不可变。其他类型的工作在相对较小的数据集上具有较高的更新和删除率；在这些情况下，不可变历史的增长会令人望而却步，碎片会是一个问题，而压缩以及垃圾收集的性能对于操作的健壮性至关重要。

除了性能原因以外，还有一些情况因为管理的原因需要删除数据，而无视了所有的不可更改性。举个例子，隐私监管会要求个人信息在用户关闭帐户后必须被删除，对数据保护的立法会要求删除错误信息，或者会需要控制敏感信息的意外泄露。

在这些情况下，仅仅把另一个事件添加到日志末尾表示其之前的数据应当被认为是已删除的，这是远远不够的——实际上你希望重写历史，然后假装数据从一开始就没有写入过。例如，Datomic把这个功能叫做为*切除*[62]，而Fossil版本控制系统有一个类似的概念，称为*刻意避开*[63]。

真正地删除数据非常困难，因为副本可以存在于许多地方：例如，存储引擎、文件系统和SSD通常会写到一个新的位置，而不是就地覆，而备份为了防止意外地删除或损坏，常常是故意设置为不可更改的。删除更多的是“使获取数据变得更困难”，而不是实际上“使检索数据变得不可能”。然而，有时你必须尝试，正如我们将在“立法与自我管制”一节中所看到的那样。

## 流的处理

到目前为止，我们已经讨论了流的来源（用户活动事件、传感器以及对数据库的写入），以及流是如何传输的（通过直接消息传递、消息代理以及事件日志）。

剩下的就是讨论一旦有了流你可以做什么——也就是说，你可以处理它。大体上，有三种选择：

1. 你可以把事件中的数据写入数据库、缓存、搜索索引，或是类似的存储系统，然后在那里其他客户端可以进行查询。如图11-5所示，这是数据库保持与系统其他部分中发生的变更同步的好方法——尤其是如果流消费者是唯一写入数据库的客户端。流，在写入存储系统方面与我们在“批处理工作流的输出”一节中讨论的是等价的。

2. 你可以用某种方式把事件推送给用户，比如通过发送电子邮件警告或是推送通知，或者是把事件流导到实时面板上，把事件可视化。在这种情况下，人类是流的最终消费者。

3. 你可以处理一或多个输入流以生成一或多个输出流。流可能会经过由几个这样的处理阶段组成的管道，然后才来到输出位置（选项1或2）。

在本章的剩余部分中，我们将讨论选项3：处理流来生成其他衍生的流。这样处理流的代码称为*操作符*或是*作业*。它与我们在第10章中讨论的Unix进程以及MapReduce作业密切相关，数据流的模式是类似的：流处理器以只读方式消费输入流，然后把它的输出以只添加的方式写到另外一个位置。

流处理器中分区与并行化的模式也非常类似于那些在MapReduce里的流处理器，以及我们在第10章中看到的数据流引擎，因此我们不会在这里重复这些。基本的映射操作，比如记录的转换与过滤的工作方式也是一样的。

与批处理作业的一个关键区别在于流永远不会结束。这种差异有许多含义：正如本章开头所讨论的，排序在没有边界的数据集中是没有意义的，因此不能使用归并连接算法（见“归纳端的连接与分组”一节）。容错机制也必须改变：对于一个已经运行了几分钟的批处理作业，一个失败的任务可以从头开始重新启动，但是对于一个已经运行了几年的流作业，崩溃之后从头开始重新启动根本不是一个可行的方案。

### 流处理的使用

流处理长期以来一直被用于监视目的，当某些事情发生时组织希望收到警告。举个例子：

* 欺诈检测系统需要判断信用卡的使用模式是否发生了意外的变化，如果信用卡可能被盗，就阻止信用卡的使用。

* 交易系统需要检查金融市场的价格变化，并按照特定的规则进行交易。

* 制造系统需要监视工厂中设备的状态，并且在出现故障的时候快速定位问题。

* 军事与情报系统需要跟踪潜在入侵者的活动，并且在有攻击迹象的时候发出警报。

这些类型的应用需要非常复杂的模式匹配以及相互关系信息。然而，随着时间的推移，流处理的其他用途也出现了。在这一节中，我们将简要地比较和对比一些这样的应用程序。

#### 复杂事件处理

*复杂事件处理*（CEP）是20世纪90年代发展起来的一种分析事件流的方法，特别是针对需要搜索特定事件模式的应用程序。就好像正则表达式允许你搜索字符串中特定字符模式，CEP允许你指定规则来搜索流中的某种事件模式。

CEP系统通常使用像SQL这样的高级声明性查询语言，或是图形界面来描述应当检测的事件模式。这些查询提交给消费输入流的处理引擎，而在内部维护执行需要的匹配的状态机。当找到匹配时，引擎发出一个*复杂事件*（名称就是这样得来的），其中包含了检测到的事件模式的详细信息[67]。

在这些系统中，查询和数据之间的关系与普通的数据库相比是相反的。通常，数据库持久地存储数据而把查询视为临时的：收到查询请求之后，数据库搜索与查询匹配的数据，完成之后就把查询忘记了。CEP引擎反转这些角色：查询被长期存储，而来自输入流的事件持续通过它们，从而搜索与事件模式匹配的查询。

CEP的实现包括Esper、IBM InfoSphere Streams、Apama、TIBCO Streambase以及SQLstream。像Samza这样的分布式流处理器也获得了针对流上声明性查询的SQL支持。

#### 流的分析

使用流处理的另一个领域是流的*分析*。CEP和流的分析的界限很模糊，但作为一般规则，分析往往对查找特定的事件序列不太感兴趣，更倾向于对大量事件的聚合以及统计指标——比如：

* 测量某类型事件的发生频率（一段时间内它发生的频率）

* 计算一段时间内某个值的滚动平均值

* 把当前的统计数据与前一个时间间隔内指标进行比较（比如，检测指标趋势，或是对与上周同一时间相比有异常的指标发出警告）

这些统计数据通常是按固定的周期计算——比如，你也许想知道在过去5分钟内每一秒对服务进行查询的平均数量，以及它们在这段时间内的第99百分位数的响应时间。对几分钟之内的数据进行平均处理，可以消除秒与秒之间不相关的波动，同时还可以及时地显示流量模式的任何变化。执行聚合操作的周期被称为*窗口*，我们将在“关于时间的推理”一节中更详细地研究窗口。

流的分析系统有时使用概率算法，例如用于集合成员关系的Bloom过滤器（我们在“性能优化”一节中遇到过），用于基数估计的HyperLoglog，以及各种百分位数估计算法（见“事件中的百分位数”一节)。概率算法只能生成近似的结果，但它的优点在于与精确算法相比，它在流处理器中需要的内存要少得多。使用这种近似算法有时会让人们觉得流处理系统总是有损且不精确，但这是错误的：流处理本质上没有近似性，而概率算法只是一种优化。

许多开源分布式流处理框架在设计时都考虑了分析的需求：例如，Apache Storm, Spark Streaming, Flink, Concord, Samza, 还有Kafka Streams。托管服务还包括Google Cloud Dataflow和Azure Stream Analytics。

#### 物化视图的维护

我们在“数据库与流”一节中看到，对数据库进行更改的流可以用来使衍生的数据系统，比如缓存、搜索索引以及数据仓库，与源数据库保持同步。我们可以把这些示例视为维护*物化视图*（见“聚合：数据立方体与物化视图”一节）的具体情况：把另一个视图派生到某个数据集上从而能够高效地查询，并且在底层数据变化时更新这个视图。

类似地在事件溯源中，应用程序状态是通过应用事件日志来维护的；在这里，应用程序状态也是一种物化视图。与流的分析场景不同，通常只考虑某个时间窗口内的事件是不够的：构建物化视图可能需要任意时间段内*所有*的事件，除了由于日志压缩可能丢弃的任何过时事件（见“日志压缩”一节）。事实上，你需要一个可以一直延伸到起始时刻的窗口。

原则上，任何流处理器都可以用于物化视图的维护，尽管永远维护事件的需要违背了一些面向分析的框架的假设，这些框架大多在有限的时间窗口内运行。Samza与Kafka Streams支持这种用法，基于Kafka对日志压缩的支持。

#### 在流上进行搜索

除了CEP允许搜索由多个事件组成的模式之外，有时还需要根据复杂的标准搜索单个事件，比如全文搜索查询。

例如，媒体监控服务订阅来自媒体的新闻文章和广播的提要，并搜索任何提及公司、产品或是感兴趣的主题的新闻。这是通过预先制定一个搜索查询来完成的，然后继续把新闻条目流与这个查询进行匹配。一些网站也有类似的功能：例如，房地产网站的用户可以提出要求，如果市场上出现了新的符合其搜索标准的房产时，他们会收到通知。ElasticSearch的过滤器特性是实现这种流搜索的一种选择。

传统搜索引擎首先为文档建立索引，然后在索引上进行查询。相比之下，搜索流完全改变了处理的方式：查询被存储，而文档通过每个查询语句，就好像CEP里一样。在最简单的情况下，你可以对每一个文档测试每个查询，尽管这样在有大量查询的情况下会变得很慢。为了优化该过程，可以对查询以及文档都建立索引，从而缩小可能与查询集合匹配的范围。

### 时间的推理

流处理器常常需要处理时间，尤其是在用于分析目的的时侯，经常会用到时间窗口，比如“过去五分钟的平均值”。“最后五分钟”的含义应该没有歧义、很清楚，但不幸的是这种说法非常复杂。

在批处理过程中，负责处理的任务快速地处理大量历史事件。如果需要按时间划分，批处理过程需要检查嵌入到每个事件中的时间戳。这时是没有必要查看执行批处理过程设备的系统时钟的，因为执行这个处理的时间与事件实际发生的时间一点关系都没有。

批处理过程可以在几分钟之内就读取一整年的历史事件；大多数情况下，有兴趣的时间线是这些历史的年份，而不是处理过程的这几分钟。此外，在事件中使用时间戳使得处理过程变得有确定性：在相同的输入上再次执行相同的处理可以产生相同的结果（见“容错”一节）。

另一方面，许多流处理框架使用执行处理的设备的本地系统时钟（*处理时间*）来确定窗口。这种方法的优点在于简单，如果创建时间与处理时间之间的延迟较短，那么这个方法也很合理。但是，如果存在明显的处理滞后——也就是说如果处理时间明显晚于事件实际发生的时间，那么这种方法就失效了。

#### 事件时间 vs 处理时间

处理发生延迟的原因有很多：排队、网络故障（见“不可靠的网络”一节）、导致消息代理或消息处理器中争用的性能问题、流消费者重启、或是从错误恢复时或是修复了代码中的bug之后重新处理过去的事件（见“重播旧消息”一节）。

此外，消息延迟还可能导致意外的消息顺序。例如，假设用户首先发出一个Web请求（由Web服务器A处理），然后发出第二个请求（由服务器B处理）。A和B发出描述了它们处理的请求的事件，但是B的事件在A的事件之前到达消息代理。这样流处理器首先看到的是B事件，然后是A事件，尽管实际上它们是以相反的顺序发生的。

打个比方，考虑一下“星球大战*系列电影：第四集在1977年上映，第五集在1980年，第六集在1983年，然后是1999年、2002年和2005年的第一、第二和第三集，以及2015年的第七集。如果你按照电影上映的顺序看电影，你处理这些电影的顺序与它们的叙述顺序不一致。（剧集号就像事件时间戳，而看电影的日期就是处理时间。）作为人类，我们能够处理这种不连续性，但是需要专门编写特定的流处理算法来适应这样的时序和排序问题。

混淆事件发生的时间与处理的时间会导致错误的数据。举个例子，假设你有一个流处理器来测量请求的速率（对每秒钟内请求的数量进行计数）。如果你重新部署流处理器，它可能会离线一分钟，并在恢复时处理积攒下来的事件。如果你根据处理事件的时间来衡量速率，那么在处理积攒下来的事件过程中，似乎出现了请求异常激增的情况，而实际上请求的实际速率是稳定的（图11-7）。

*图11-7. 按处理事件的时间划分窗口会由于处理速率的变化而引入错误。*

#### 知道什么时候准备好了

在按事件发生的时间定义窗口时，一个棘手的问题是你永远无法确定什么时候收到了特定窗口内的所有事件，是不是还会有一些事件要发生。

举个例子，假设你把事件按一分钟的窗口进行分组，这样就可以计算每分钟请求数。你已经对一些时间戳归为第三十七分钟的事件进行了计数，时间继续向前；现在大部分进来的事件被归到了第三十八分钟和第三十九分钟的窗口内。那么你什么时候宣布已经完成了第三十七分钟的窗口，并输出它的计数器值？

您可以在一段时间没有看到任何新事件之后暂停，并且宣布窗口完成，但是仍然可能发生某些事件由于网络中断而被缓冲到另一台机器上的情况。您要能够处理这些在窗口声明完成之后到达的*散乱*事件。总的来说，你有两个选择：

1. 忽略那些散乱的事件，因为在正常情况下它们只是事件流的一小部分。您可以把丢弃的事件数量作为追踪指标，并且在开始大量丢弃数据的时候发出警报。

2. 发布一个*修正值*，这是一个包含散乱事件的窗口更新值。你也许还需要撤回前一次的输出。

在某些情况下，可以使用一个特殊的消息来表示“从现在起将不再会有比*t*更早时间戳的消息”，它可以被消费者用来触发窗口。然而，如果不同设备上的几个生产者正在生成事件，每个事件都有它们自己的最小时间戳阈值，那么消费者需要单独跟踪每个生产者。在这种情况下，添加删除生产者都更为棘手。

#### 你到底在用哪个时钟？

当事件可以在系统中好几个点缓冲时，为事件分配时间戳就更加困难了。举个例子，假设一个移动应用，它向服务器报告使用情况的事件。这个应用可以在设备离线的时侯使用，在这种情况下它会在设备上缓冲事件，并且在下一次互联网可用的时候（可能是几个小时甚至几天以后）把它们发送到服务器。对于任何这个流的消费者来说，这些事件看起来就像是延迟许久的散乱事件。

在这种情况下，事件上的时间戳就应该是用户交互发生时的时间，以移动设备本地时钟为准。然而，通常用户控制设备上的时钟通常不能被信任，因为它可能被意外地或故意地设置为错误的时间（见“时钟同步与准确性”一节）。服务器收到事件的时间（以服务器的时钟为准）更可能是准确的，因为服务器在你的控制下，但是在描述用户交互的方面没有意义。

为了调整不正确的设备时钟，一种方法是记录三个时间戳

* 事件的发生时间，以设备的时钟为准

* 事件向服务器发送的时间，以设备的时钟为准

* 服务器收到事件的时间，以服务器的时钟为准

通过从第三个时间戳中减去第二个时间戳，就可以估计设备时钟与服务器时钟之间的偏差（这里假设与所需的时间戳精度相比，网络延迟是可以忽略不计的）。然后把这个偏差应用于事件发生的时间戳，从而可以估计事件实际发生的真实时间（假设设备时钟的偏差在事件发生时间与发送到服务器的时间之间没有变化）。

这个问题并不是流处理所独有的——批处理也面临着完全相同的时间推理问题。它只是在流的环境中更明显，我们更加清楚时间的流逝。

#### 窗口的类型

一旦知道了事件的时间戳应该如何确定，下一步就是决定如何定义时间窗口。之后，窗口可以用于执行聚合操作，比如对事件进行计数，或是计算窗口内的平均值。常用的窗口有这样几种类型：

*翻滚窗口*

翻滚窗口具有固定长度，而每个事件都只属于一个窗口。举个例子，如果有一个长度为1分钟的翻滚窗口，那么所有时间戳在10:03:00到10:03:59之间的事件被划分在一个窗口中，在10:04:00到10:04:59之间的事件被划分在下一个窗口，以此类推。实现长度为1分钟的滚动窗口，可以通过对每个事件的时间戳舍去秒数，从而确定它属于哪个窗口。

*跳跃窗口*

跳跃窗口也有固定的长度，但是允许窗口重叠以提供某种平滑处理。例如，一个跳跃长度为1分钟的5分钟窗口将包含10:03:00到10:07:59之间的事件，然后下一个窗口将包含10:04:00到10:08:59之间的事件，以此类推。实现这种跳跃窗口，你可以首先计算所有1分钟的翻滚窗口，然后再聚合几个相邻的窗口。

*滑动窗口*

滑动窗口包含任意两个时间点之间发生的所有事件。例如，一个5分钟的滑动窗口将包含10:03:39与10:08:12的事件，因为它们相隔不到5分钟（注意，因为使用固定的边界，翻滚和跳跃的5分钟窗口不会把这两个事件放在同一个窗口中）。通过保留按时间排序的事件缓冲区，并且当旧事件窗口过期时从中删除，就实现了滚动窗口。

*会话窗口*

与其他窗口类型都不同，会话窗口没有固定的时间范围。相反，它是通过把同一用户在时间上发生的所有事件组合在一起来定义的，当用户不活动一段时间之后（比如30分钟之内没有任何事件），窗口就会关闭。会话化是网站分析的一项常见要求（见“按组分类”一节）。