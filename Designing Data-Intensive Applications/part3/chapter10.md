# 第十章 批处理

*如果系统受到一个人的强烈影响，那它是不可能成功的。一旦初始设计完成并且相当得健壮，真正的测试就开始了——拥有许多不同观点的人们开始进行他们自己的实验。*

高德纳

---

在本书的前两部分中，我们讨论了许多关于请求、查询以及对应的响应或结果的内容。这种风格的数据处理在许多现代数据系统中都假设：你请求什么，或者发送一条指令，稍后系统（但愿）给你一个回答。数据库、缓存、搜索索引、Web服务器以及许多其他系统都是以这样的方式工作的。

在这样的在线系统中，无论是请求页面的Web浏览器还是调用远程API的服务，我们通常假设请求是由用户触发的，并且这个用户正在等待响应。他们没有必要等待太久，所以我们非常关注这些系统的响应时间（见“描述性能”一节）。

互联网，以及越来越多的基于HTTP/REST的API使得请求/响应风格的交互变得如此普遍，以至于很容易认为这些是理所当然的。但我们应该记住，这不是构建系统的唯一方式，而且其它方法也有各自的优点。让我们区分三种不同类型的系统：

*服务（在线系统）*

服务等待来自客户端的请求或指令的到来。当接收到一条消息时，服务尝试尽可能快地处理它，并回复一个响应。响应时间通常是衡量服务性能的主要指标，而可用性通常也非常重要（如果客户端无法访问服务，用户可能会收到一条错误信息）。

*批处理系统（离线系统）*

批处理系统接受大量的输入数据，启动一个任务来处理它，然后产生一些输出数据。任务通常需要花费一段时间（从几分钟到几天不等），因此通常不会有用户等待任务完成。取而代之的是，批处理任务通常定期运行（比如一天一次）。批处理任务的主要性能衡量指标通常是吞吐量（处理特定大小输入数据集所花费的时间）。在这一章里，我们将讨论批处理。

*流处理系统（近似实时系统）*

流处理介于在线处理和离线/批处理之间（因此有时被称为近似实时或接近线的处理）。与批处理系统类似，流处理器消耗输入并产生输出（而不是响应请求）。然而，流任务在事件发生之后不久就会运行，而批处理任务则在固定的输入数据集上操作。这种差异使得流处理系统比等效的批处理系统具有更低的延迟时间。由于流处理构建在批处理的基础上，我们将在第11章中讨论它。

正如我们将在本章中看到的，批处理是构建可靠的、可扩展的和可维护的应用程序的重要组成部分。举个例子，2004年发布的批处理算法MapReduce，被称为“使谷歌如此大规模可扩展的算法”。之后在各种开放源码数据系统中都有实现，包括Hadoop、CouchDB和MongoDB。

与许多年前为数据仓库开发的并行处理系统相比，MapReduce是一个相当低层级的编程模型，但是它在商业性硬件上可以实现的处理规模方面是向前迈进了一大步。尽管现在MapReduce的重要性正在下降，但仍然值得去理解它，因为它清楚地说明了批处理为什么以及如何有用。

事实上，批处理是一种非常古老的计算形式。早在可编程数字计算机被发明之前，穿孔卡片制表机——如1890年美国人口普查中使用的何乐礼机——实现了一种半机械化的批处理形式，用以从大量的输入计算总的统计数据。并且MapReduce与二十世纪四五十年代广泛用于商业数据处理的机电式IBM卡片分类机有着惊人的相似之处。如往常一样，历史有重复自己的倾向。

在本章中，我们将研究MapReduce和其他几个批处理算法和框架，并探讨它们是如何在现代数据系统中使用的。但是首先，我们先使用标准Unix工具看看数据处理。即使你已经很熟悉它们了，还是要提醒你一下Unix哲学还是很有价值的，因为Unix的思想和经验传递给了大规模、异构的分布式数据系统。

## 用Unix工具进行批处理

让我们从一个简单的例子开始。假设你有一个Web服务器，每次服务了请求之后都会在日志文件添加一行。举个例子，使用nginx默认访问日志格式，其中一行可能如下所示：

Let’s start with a simple example. Say you have a web server that appends a line to a log file every time it serves a request. For example, using the nginx default access log format, one line of the log might look like this:

```TEXT
216.58.210.78 - - [27/Feb/2015: 17:55:11 + 0000] "GET /css/typography.css HTTP/1.1" 200 3377 "http://martin.kleppmann.com/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/40.0.2214.115 Safari/537.36"
```

(这实际上是一行；这里只是为了便于阅读才将它分解成多行。)这一行有很多信息。为了解读它，你需要查看日志格式的定义，如下所示：

```PHP  
$remote_addr - $remote_user [$time_local] "$request"
$status $body_bytes_sent "$http_referer" "$http_user_agent"
```

因此日志的这一行表示，在UTC时间2015年2月27日17时55分11秒，服务器收到了来自客户端IP地址为216.58.210.78对文件/css/typography.css的请求。用户没有经过身份验证，因此$Remote_user被设置为连字符（-）。响应状态为200（即请求成功），响应大小为3377字节。Web浏览器是Chrome 40，它加载该文件是因为它在网址http://martin.kleppmann.com/ 的页面中被引用了。

### 简单的日志分析

各种工具可以读取这些日志文件并生成关于你的网站流量的漂亮报告，但为了便于练习，让我们使用基本的Unix工具构建自己的日志文件。例如，假设你想在你的网站上找到最受欢迎的五个页面。你可以在Unix命令行中这样做，如下所示：

```BASH
cat /var/log/nginx/access.log | ➊
    awk '{print $7}'    | ➋
    sort                | ➌
    uniq -c             | ➍
    sort -r -n          | ➎
    head -n 5             ➏
```

➊ 读取日志文件。

➋ 用空格把每一行分割成许多字段，然后只输出每一行中的第七个字段，这是请求的URL。在我们的例子中，这个请求URL是*/css/typography.css*。

➌ 按字母顺序对请求的URL列表进行排序。如果某个URL被请求了*n*次，那么在排序之后，该文件在每一行包含了重复*n*次的相同URL。

➍ `uniq`命令通过检查相邻的两个行是否相同来过滤输入中的重复行。`-c`选项告诉命令还要输出计数器值：对于每个不同的URL，它都会报告该URL在输入中出现的次数。

➎ 第二次排序是根据每行开头的数字（`-n`）排序，即请求URL的次数。然后它以反向（`-r`）顺序返回结果，即最大值排在前面。

➏ 最后，`head`只输出输入的前五行（-n 5），而其余的被丢掉了。

这一系列命令的输出结果如下：

```BASH
4189 /favicon.ico
3631 /2013/05/24/improving-security-of-ssh-private-keys.html
2124 /2012/12/05/schema-evolution-in-avro-protocol-buffers-thrift.html
1369 /
915 /css/typography.css
```

如果不熟悉Unix工具，虽然前面的命令行可能看起来有点生涩，却是非常得强大。它可以在数秒钟内处理千兆字节的日志文件，而且可以轻松地修改分析过程以满足需要。比如，如果要在报告中省略CSS文件，可以把`awk`的参数更改为`‘$7 !~ /\.css$/ {print$7}’`。如果要计算访问次数最多的客户端IP地址而不是访问次数最多的页面，可以把`awk`参数更改为`‘{print $1}’`。等等。

在这本书中我们没有篇幅来详细探讨Unix工具，但它们非常值得学习。令人惊讶的是，使用`awk`、`sed`、`grep`、`sort`、`uniq`和`xargs`的组合可以在几分钟内完成许多数据分析，而它们的性能出人意料地好。

#### 命令链 vs 定制程序

不使用Unix命令链，你还可以写一个简单的程序来完成同样的任务。举个例子，用Ruby这个程序大概是这个样子的：

```RUBY
counts = Hash.new(0) ➊

File.open('/var/log/nginx/access.log') do |file|
    file.each do |line|
        url = line.split[6] ➋
        counts[url] += 1    ➌
    end
end

top5 = counts.map{|url, count| [count, url] }.sort.reverse[0...5]   ➍
top5.each{|count, url| puts "#{count} #{url}" } ➎
```

➊ `counts` 一个哈希表，它为我们看到每个URL的次数计数。默认情况下，计数为零。

➋ 从日志的每一行中，我们将URL作为第七个按空格分隔的字段（这里的数组索引为6，因为Ruby的数组是以零为起始索引的）。

➌ 为日志当前行中URL的计数加一。

➍ 按计数器值（降序）对哈希表内容进行排序，并取前五项。

➎ 打印出前五项。

这个程序不像Unix管道链那样简洁，但是它的可读性相当强，而两者之中更偏好哪一个在一定程度上取决于你的喜好。但是除了这两者之间的表面语法差异之外，在执行流程上也有很大的差异，这一点在一个大文件上执行分析的时候就很明显了。

#### 排序 vs 内存内聚合

Ruby脚本保存了URL的内存哈希表，其中每个URL都映射到它被浏览的次数。Unix管道示例没有这样的哈希表而是依赖于排序URL列表，列表中同一个URL多次重复。

哪种方法更好？这取决于你有多少个不同的URL。对于绝大多数中小型网站，你大概可以把所有不同的URL，以及为每个URL设置的计数器，放在（例如）1GB内存中。在这个例子中，任务的工作集（任务需要随机访问的内存量）只取决于URL的数量：如果单个URL有一百万个日志条目，那么哈希表中所需的空间仍然是一个URL的长度加上计数器的大小。如果这个工作集足够小，那么内存哈希表就可以工作得很好了——哪怕是在一台笔记本电脑上。

另一方面，如果任务的工作集大于可用内存，则排序方法的优点是可以有效地利用磁盘。这与我们在“SSTable与LSM树”一节中讨论的原则相同：数据块可以在内存中排序并作为段文件写入磁盘，之后数个排好序的段可以合并到一个更大的排好序的文件中。合并排序的顺序访问模式在磁盘上表现良好。（请记住，对顺序I/O的优化是第3章中反复出现的主题。同样的模式在这里再次出现。）

GNU Coreutils (Linux)中的`sort`程序通过溢出到磁盘自动处理大于内存的数据集，并自动跨多个CPU核心进行并行排序。这意味着我们前面看到的简单的Unix命令链很容易扩展到大型数据集上，而不会耗尽内存。瓶颈很可能是从磁盘读取输入文件的速度。

### Unix哲学

我们能够很容易地使用前面示例中的一系列命令来分析日志文件，这并不是巧合：实际上，这是Unix的关键设计思想之一，今天它仍然具有惊人的相关性。让我们更深入地研究它，方便我们可以从Unix中借鉴一些点子。

道格拉斯·麦克罗伊，Unix管道的发明者，在1964年第一次这样描述它们：“我们应该有一些连接程序的方法，就像花园里浇水的软管一样——当必须以另一种方式修改数据的时候就再接入另一段（软管）。I/O也是同样的方式。”水管的比喻非常成功，用管道连接程序的理念成为了现在被称为Unix哲学的一部分——一套在Unix开发人员和用户中流行的设计原则。这一哲学在1978年被描述如下：

1. 让每个程序都做好一件事。要做一项新的工作，构建一个新程序而不是通过添加新的“功能”使旧程序复杂化。

2. 期望每一个程序的输出都可以成为另一个，也许未知，的程序的输入。不要杂乱地输出不相关的信息。避免严格的列式或二进制输入格式。不坚持使用交互式输入。

3. 设计和构建软件甚至是操作系统，都要尽早适用，最好是在几周之内就这样做。在扔掉和重建笨拙的组件时不要犹豫。

4. 优先使用工具而非不熟练的帮助来减轻编程任务，哪怕你必须专门去构建工具，并且在使用完这些工具后会扔掉其中一些。

这种方法——自动化、快速原型化、增量迭代、对实验友好，以及将大型项目分解成可管理的块——听起来非常像今天的敏捷和DevOps运动。令人惊讶的是，这四十年来几乎没有什么变化。

`sort`工具是很好的程序只做好一件事的例子。可以说它是一种比大多数编程语言在其标准库中实现的更好的排序实现（它不会溢出到磁盘，也不会使用多个线程，即使这样做是有益的）。然而，单单只用`sort`几乎没什么用。它只有与其他Unix工具，比如`uniq`，结合时才会变得强大。

Unix命令行，比如`bash`，让我们很容易将这些小程序*编写*成令人惊讶的强大的数据处理任务。虽然这些程序中有很多是由不同的人们编写的，但是它们可以灵活地结合在一起。实现这种可组合性Unix都做了些什么？

#### 统一的界面

如果你期望一个程序的输出变成另一个程序的输入，那意味着那些程序必须使用相同的数据格式——换句话说，一个兼容的接口。如果你想将任何程序的输出连接到任何程序的输入，这意味着所有的程序都必须使用相同的输入/输出接口。

在Unix中，这个接口是文件（或者更准确地说，是文件描述符）。文件只是一个有序的字节序列。因为这是一个简单的接口，所以可以使用相同的接口来表示许多不同的东西：文件系统上实际的文件、到另一个进程（Unix套接字、`stdin`、`stdout`）的通信通道、设备驱动程序（例如`/dev/audio`或`/dev/lp0`）、表示TCP连接的套接字，等等。我们很容易认为这是理所当然的，但事实上，这些非常不同的东西可以共享一个统一的接口是非常值得注意的，因此它们可以很容易地连接在一起。

按照惯例，许多（但不是所有）Unix程序将这个字节序列作为ASCII文本来处理。我们的日志分析示例利用了这个特点：`awk`、`sort`、`uniq`和`head`都将它们的输入文件视为由`\n`（新行，ASCII码`0x0A`）字符分隔的记录列表。选择`\n`是任意的——按理说，ASCII记录分隔符`0x1E`本来是更好的选择，因为它就是为此目的而设计的——但无论如何，所有这些程序在使用相同的记录分隔符的这一刻就标准化了，这使得它们可以进行相互操作。

对每条记录（即每行输入）的解析比较模糊。Unix工具通常通过空格或制表符将一行拆分为字段，但是也可以用CSV（逗号分隔）、管道分隔和其他编码。即使是像`xargs`这样相当简单的工具，也有六个命令行选项来指定如何解析其输入。

统一的ASCII文本接口大部分时间可以工作，但并不是很漂亮：我们的日志分析示例使用`{print $7}`来提取URL，这不是很易读。在一个理想的环境中，这可以是`{print $request_url}`或类似的东西。我们稍后再来谈谈这个想法。

虽然并不完美，但即使在几十年后，Unix统一接口仍然非常出色。没有多少软件可以像Unix工具一样互相操作和撰写：你无法轻松地通过自定义分析工具将电子邮件帐户的内容和在线购物历史记录输入电子表格，然后把结果发布到社交网络或一个wiki页面上。今天，可以像Unix工具一样顺利地协同工作的程序是一个例外，而不是正常情况。

即使是具有*相同数据模型*的数据库，也往往不容易把数据从一个库转移到另一个中。集成的缺乏导致了数据的碎片化。

#### 逻辑与连接的分离

Unix工具的另一个特点是它们使用标准输入（`stdin`）和标准输出（`stdout`）。如果你运行一个程序，而不指定任何其他设置，那么`stdin`来自键盘而`stdout`将转到屏幕上。但是，你也可以从文件中获取输入然后将输出重定向到文件。管道允许你将一个进程的`stdout`附加到另一个进程的`stdin`(使用一个小内存缓冲区，并且不会把整个中间数据流写入磁盘)。

如果需要的话程序仍然可以直接读写文件，但是如果程序不担心特定的文件路径而只是使用`stdin`和`stdout`，那么Unix方式的效果最好。这使得命令行用户可以用他们想要的任何方式连接输入和输出；程序不知道也不关心输入的来源和输出到哪里。（可以说，这是一种松散耦合、延迟绑定或控制反转的形式。）将输入/输出连接与程序逻辑分离，可以更容易地把小型工具组合到更大的系统中去。

你甚至可以编写自己的程序，然后把它们与操作系统提供的工具结合起来。你的程序只需要从`stdin`读取输入并将输出写入`stdout`，它就可以参与数据处理的管道中。在日志分析示例中，可以编写工具将用户代理字符串转换为更合理的浏览器标识符，或者一个可以把IP地址转换为国家代码的工具，然后把它插入到管道里。`sort`程序不关心是与操作系统的另一部分通信，还是与你编写的程序通信。

然而有了`stdin`和`stdout`能做的事情也是有限制的。需要多个输入输出的程序是存在的，但很棘手。你也不能把程序的输入通过管道发送到网络上。如果一个程序直接打开文件进行读写，或者启动另一个程序作为子进程，或者打开一个网络连接，那么这些I/O就会由程序自己连接起来。它仍然可以配置（比如通过命令行选项），但是在命令行中连接输入和输出的灵活性降低了。

#### 透明度与尝试

使Unix工具如此成功的部分原因在于它们让你很容易了解到正在发生的事情：

* Unix命令的输入文件通常被视为不可变的。这意味着你可以根据需要运行命令，尝试各种命令行选项，而不会损坏输入文件。

* 你可以在任何时候结束管道，将输出输送到`less`，然后查看它是否具有预期的形式。这种检查能力对调试是很好的。

* 你可以将管道某一阶段的输出写入到文件，然后把这个文件用作下一个阶段的输入。这使得你可以重新启动后期阶段，而无需重新运行整个管道。

因此尽管相比于关系数据库的查询优化器，Unix工具是相当简单的工具，但是它们仍然非常有用，特别是在尝试阶段。

然而Unix工具的最大限制在于它们只能在一台机器上运行——这就是Hadoop之类的工具出现的地方。

## MapReduce与分布式文件系统

MapReduce有点像Unix工具，但是可以分布在数千台机器上。就像Unix工具一样，它是一个相当简单、暴力，但又令人惊讶的有效工具。单个MapReduce任务可以与单个Unix进程相比较：它接受一个或多个输入，然后产生一个或多个输出。

与大多数Unix工具一样，运行MapReduce任务通常不会修改输入，除了产生输出之外没有任何副作用。输出文件只会按顺序写入一次（一旦写入，就不会再修改文件的现有部分）。

Unix工具使用`stdin`和`stdout`作为输入和输出，而MapReduce任务是在分布式文件系统上读写文件。在Hadoop的MapReduce实现中，这个文件系统称为HDFS（Hadoop Distributed File System），它是Google文件系统（GFS）的开源重新实现。

除了HDFS之外，还有其他各种分布式文件系统，比如GlusterFS和Quantcast文件系统（QFS）。对象存储服务，如Amazon S3、Azure Blob存储和OpenStack Swift在许多方面都是相似的。在本章中，我们将主要使用HDFS作为运行示例，但这些原则适用于任何分布式文件系统。

HDFS基于无共享原则（见第二部分的介绍），与网络附加存储（NAS）和存储区域网络（SAN）体系结构的共享磁盘方式形成对比。共享磁盘存储由集中式存储设备实现，通常使用自定义硬件和特殊的网络基础设施，比如光纤通道。另一方面，武功想方式不需要特殊硬件，只需要通过传统的数据中心网络连接起来的计算机。

HDFS由运行在每台机器上的守护进程组成，它公开了一个网络服务，允许其他节点访问存储在该机器上的文件（假设数据中心中的每台设备都附加了一些磁盘）。名为NameNode的中央服务器跟踪记录哪些文件区块存储在哪台设备上。因此，HDFS在概念上构建了一个大文件系统，它可以使用所有运行守护进程设备的磁盘上的空间。

为了容忍机器与磁盘故障，文件块被复制到多台设备上。复制可能仅仅意味着多台机器上相同数据的几份拷贝，如第5章中那样，或者是一个擦除编码方案，例如里德-所罗门码，它允许以比完全复制更低的存储开销来恢复丢失的数据。这些技术类似于RAID，它提供了连接到同一台计算机的多个磁盘的冗余性；不同之处在于分布式文件系统中，文件访问与复制是在没有特殊硬件的常规数据中心网络上完成的。

HDFS扩展性很好：在撰写这本书时，最大的HDFS部署运行在数万台设备上上，合计存储容量为数百PB。如此大的规模之所以变得可行，是因为在HDFS上使用商品化硬件以及开源软件存储和访问数据的成本，比使用专用存储设备获得等效容量的成本低得多。

### MapReduce任务的执行

MapReduce是一个编程框架，有了它你可以编写代码来处理在类似HDFS这样的分布式文件系统中的大型数据集。理解它的最简单方法是参考“简单日志分析”一节中的Web服务器日志分析案例。MapReduce中的数据处理模式非常类似于这个示例：

1. 读取一组输入文件，并将其分解为记录。在Web服务器日志示例中，每条记录都是日志中的一行（也就是说，`\n`是记录分隔符）。

2. 调用映射函数从每个输入记录中提取键和值。在前面的示例中，映射函数是`awk ‘{print $7}’`：它提取URL（`$7`）作为键，而值留空。

3. 按键对所有键值对进行排序。在日志示例中，这是通过第一个`sort`命令完成的。

4. 调用归纳函数来迭代排了序的键值对。如果同一键多次出现，排序让它们在列表中相邻，因此很容易组合这些值而不必将大量状态保存在内存中。在前面的示例中，归纳函数由命令`uniq -c`实现，该命令使用相同的键计算相邻记录的数量。

这四个步骤可以由一个MapReduce任务执行。步骤2（映射）和步骤4（归纳）是编写自定义数据处理代码的地方。步骤1（将文件分解为记录）由输入格式解析器处理。步骤3排序步骤，在MapReduce中是隐式的——你不用写它，因为映射函数的输出总是排序之后再交给归纳函数。

要创建MapReduce任务，你需要实现两个回调函数，即映射函数和归纳函数，它们的行为如下（也参考“MapReduce查询”一节）：

映射函数

映射函数对每个输入记录调用一次，任务是从输入记录中提取键和值。对于每个输入，它可能生成任意数量的键值对（也包括没有）。它不保留从一个输入记录到下一个输入记录的任何状态，因此每个记录都是独立处理的。

归纳函数

MapReduce框架采用映射函数生成的键值对，收集属于同一键的所有值，并在该值集合上迭代调用归纳函数。归纳函数可以产生输出记录（例如同一个URL的出现次数）。

在Web服务器日志案例中，我们在步骤5中有第二个`sort`命令，它根据请求的数量对URL进行排序。在MapReduce中，如果需要第二个排序阶段，可以通过编写第二个MapReduce任务并使用第一个作业的输出作为第二个作业的输入来实现它。这样看，映射函数的作用是通过将数据转换为适合排序的形式来准备数据，而归纳函数的作用是处理排好序的数据。

#### MapReduce的分布式执行

MapReduce与Unix命令管道的主要区别在于它可以在许多机器上并行化计算，而无需编写代码显式处理并行性。映射函数和归纳函数一次只对一条记录进行操作；它们不需要知道它们的输入来自何处或它们的输出将去何处，所以框架可以处理在机器之间移动数据的复杂性。

在分布式计算中是可以使用标准Unix工具作为映射函数和归纳函数，但更常见的是它们是使用传统编程语言的函数实现的。在Hadoop MapReduce中，映射函数和归纳函数是实现了特定接口的Java类。在MongoDB和CouchDB中，映射函数和归纳函数是JavaScript函数（见“MapReduce查询”一节）。

图10-1显示了Hadoop MapReduce任务中的数据流。它的并行化基于分区（见第6章）：任务的输入通常是HDFS中的文件目录，输入目录中的每个文件或文件块被视为单独的分区，可以通过单独的映射任务（由图10-1中的*m 1*、*m 2*和*m 3*标记）来处理。

每个输入文件的大小通常为数百MB。MapReduce调度程序（图中未显示）试图在存储输入文件副本的机器上执行每个映射函数，条件是该设备有足够的空闲内存和CPU资源来执行映射任务。这一原则被称为*把计算放在数据附近*：这样不用通过网络复制输入文件，减少了网络负载而增加了局部性。

*图10-1 有三个映射函数和三个归纳函数的MapReduce任务*

在大多数情况下，应该运行在映射任务中的应用程序代码还没有出现在被分配执行它的设备上，所以MapReduce框架首先拷贝代码（比如是Java程序情况的JAR文件）到适当的机器。然后，启动映射任务并开始读取输入文件，每次将一条记录传递给映射回调函数。映射函数的输出是由键值对组成的。

计算的归纳部分也是分区的。映射任务的数量取决于输入文件块的数量，而归纳任务的数量是由任务作者配置的（它可以与映射任务的数量不同）。为了确保所有具有相同键的键值对最终位于同一归纳函数上，框架使用键的哈希值来确定哪个归纳任务应该接收特定的键值对（见“按键的哈希值进行分区”一节）。

键值对必须进行排序，但数据集很可能太大，而无法在单台设备上使用常规排序算法进行排序。取而代之的是，排序是分阶段执行的。首先，每个映射任务根据键的哈希值把它的输出按归纳函数分区。每一个这样的分区都被写入映射器本地磁盘上排了序文件里，用到了类似于我们在“SSTable与LSM树”一节中讨论到的技术。

每当映射函数完成对其输入文件的读取并写入已排序的输出文件时，MapReduce调度程序会通知归纳函数，它们可以开始从归纳函数获取输出文件。归纳函数连接到每个映射函数，并从它们的分区下载排好序的键值对文件。这个通过归纳函数分区、排序，然后从映射函数拷贝数据分区到归纳函数的过程被称为混洗（这是一个让人困惑的词——与洗牌不同，MapReduce中没有随机性）。

归纳函数从映射函数中获取文件，并将它们合并在一起，保持排序顺序。因此，如果不同的映射函数使用相同的键生成记录，那么它们将在合并后的归纳函数输入中相邻。归纳函数是用一个键和一个迭代器调用的，迭代器用相同的键递增地扫描所有记录（在某些情况下，内存是无法放下所有的记录的）。

归纳函数可以使用任意逻辑来处理这些记录，并且可以生成任意数量的输出记录。这些输出记录被写入分布式文件系统上的文件（通常，在运行归纳函数的设备本地磁盘上有一个副本，在其他机器上有其它副本）。

#### MapReduce工作流

使用单个MapReduce任务可以解决的问题范围是有限的。回到日志分析示例，单个MapReduce任务可以确定每个URL的页面浏览量，但无法确定最流行的URL，因为这需要进行第二轮排序。

因此，MapReduce任务被链接到工作流中是非常常见的，这样任务的输出就变成了下一个任务的输入。Hadoop MapReduce框架不支持任何特定的工作流，因此链接是通过目录名称隐式完成的：必须配置第一个任务将其输出写入HDFS中的指定目录，而第二个任务必须配置为读取同样的目录。从MapReduce框架的角度来看，它们是两个独立的任务。

因此，链式MapReduce作业就不太像Unix命令的管道（这些命令只使用内存中的一个小缓冲区，将一个进程的输出直接传递给另一个进程作为输入）而更像是命令序列，每个命令的输出被写入一个临时文件而下一个命令从临时文件中读取。这种设计有它的优缺点，我们将在“中间状态的物化”一节中讨论这个问题。

批处理任务的输出只有在任务成功完成时才被认为有效（MapReduce会丢弃失败任务的部分输出）。因此，工作流中的任务只有在先前的作业——即生成其输入目录的任务——成功完成时才能启动。为了处理任务执行之间的这些依赖关系，开发了各种Hadoop工作流的调度程序，包括Oozie、Azkaban、Luigi、Airflow和Pinball。

这些调度程序还有管理功能，在维护大量批处理任务时非常有用。在构建推荐系统时，包含50到100个MapReduce任务组成的工作流很常见，而在一个大型组织中，许多不同的团队会运行不同的任务来读取彼此的输出。工具支持对于管理这样复杂的数据流非常重要。

Hadoop的各种高级工具，如Pig、Hive、Cascading、Crunch和FlumeJava，也设置了包含多个可以自动连在一起的MapReduce阶段的工作流。

### 归纳侧的连接与分组

我们在第2章中研究数据模型和查询语言时讨论了连接，但我们还没有深入研究连接是如何实现的。现在是我们重新开始讨论它的时候了。

在许多数据集中，一条记录与另一条记录有关联是很常见的：关系模型中的*外键*，文档模型中的*文档引用*，亦或是图形模型中的*边*。每当一些代码需要访问关联两边的记录（包含引用的记录与被引用的记录）时，连接都是必需的。正如在第2章中所讨论的，去正规化可以减少对连接的需求，但通常无法完全不依赖它。

在数据库中，如果执行的查询只涉及到少量的记录，数据库通常会使用索引快速定位感兴趣的记录（见第3章）。如果查询涉及连接，就会需要多个索引查找。然而，MapReduce没有索引的概念——至少通常意义上并非如此。

当给予MapReduce任务一组文件作为输入时，它会读取所有这些文件的全部内容；数据库将此操作称为*全表扫描*。如果你只想读取少量的记录，那么相比于索引查找，全表扫描代价要高得多。然而在分析性查询中（见“事务处理还是分析？”一节），想要计算大量记录的聚合是很常见的。在这种情况下，扫描整个输入可能是相当合理的事情，尤其是如果可以在多台机器上并行处理的话。

当我们在批处理主题中讨论连接时，我们是指解决数据集中所有的某种关联。例如，我们假设一项任务同时为所有用户处理数据，而不仅仅是查找某个特定用户的数据（使用索引可以更有效地完成这一任务）。

#### 案例：用户活动事件的分析

批处理任务中连接的典型案例如图10-2所示。左边是描述已登录的用户在网站上的操作事件日志（称为*活动事件*或*点击流数据*），右边是用户数据库。你可以把这个案例视为星型模式的一部分（见“星型与雪花型：用于分析的模式”一节）：事件日志是事实表，而用户数据库是维度之一。

*图10-2 在用户活动事件的日志与用户资料的数据库之间的连接。*

分析任务会需要把用户活动与用户资料信息相关联：例如，如果资料包含用户的年龄或出生日期，系统可以判断哪些页面在哪个年龄组最流行。然而，活动事件只包含用户ID，而不是完整的用户资料信息。在每个活动事件中都嵌入用户资料信息太浪费了。因此，活动事件需要与用户资料数据库连接。

这个连接最简单实现是挨个检查活动事件，然后（在远程服务器上）查询用户数据库中遇到的每个用户ID。这是可能的，但是性能可能会非常差：处理的吞吐量受到访问数据库服务器往返时间的限制，本地缓存的有效性很大程度上取决于数据的分布，而并行执行大量的查询很容易使数据库拥堵。

为了在批处理过程中获得良好的吞吐量，计算必须（尽可能多地）发生在本地单台设备上。通过网络对你想要处理的每一条记录进行随机访问请求太慢了。此外，查询远程数据库意味着批处理作业变得不确定起来，因为远程数据库中的数据可能会发生变化。

因此，更好的方法是获取用户数据库的副本（比如使用ETL进程从数据库备份中提取——见“数据仓库”一节），并把它放在用户活动事件日志所在的同一个分布式文件系统中。这样，用户数据库放在HDFS中的一组文件中而用户活动记录放在另一组文件中，于是可以使用MapReduce将所有相关记录集中在同一个地方，从而有效地处理它们。

#### 归并连接

回想一下，映射函数的目的是从每个输入记录中提取一个键和值。在图10-2的情况中，这个键是用户ID：一组映射函数将遍历活动事件（提取用户ID作为键而活动事件作为值），而另一组映射函数将遍历用户数据库（提取用户ID作为键而用户的出生日期作为值）。这个过程如图10-3所示.

*图10-3 归纳侧在用户ID上排序合并连接。如果输入数据集被分成数个文件，每一个文件可以用多个映射函数并行处理。*

当MapReduce框架把映射函数的输出按键分区，然后对键值对进行排序时，结果是所有活动事件和具有相同用户ID的用户记录在归纳函数的输入中彼此相邻。MapReduce任务甚至可以为记录排序，使得归纳函数总是先看到用户数据库的记录，然后才是按时间戳顺序排列的活动事件——这种技术被称为次级排序。

之后，归纳函数可以很容易地执行实际的连接逻辑：对每个用户ID都调用一次归纳函数，因为有了次级排序，第一个值应该是来自用户数据库的出生日期记录。归纳函数将出生日期存储在一个局部变量中，然后使用相同的用户ID迭代活动事件记录，输出一对浏览的URL与用户年龄。随后的MapReduce任务可以计算每个URL的用户年龄分布，并按年龄进行分组聚合计算。

由于归纳函数一次处理了特定用户ID的所有记录，因此在任何时候它都只需要在内存中保存一个用户记录，而且不需要通过网络发出任何请求。这个算法被称为*归并连接*，因为映射函数的输出是按键排序的，然后归纳函数将连接两边排好序的记录列表合并在一起。

#### 把相关数据放在一起

在归并连接中，映射函数和排序过程保证把执行特定用户ID连接操作所需的所有数据集中在同一个位置：这样只需要调用一次归纳函数。事先排列好所有需要的数据之后，归纳函数可以是一段相当简单的单线程代码，以很高的吞吐量和很低的内存开销遍历这些记录。

看待这个体系架构的一种方式是映射函数“发送消息”给归纳函数。当映射函数发送一个键值对时，键的作用类似于值被发送到的目标地址。尽管键只是一个任意字符串（而不是有着IP地址和端口号那样的真实网络地址），但它表现地像一个地址：所有具有相同键的键值对都将被发送到相同的目的地（对归纳函数的调用）。

使用MapReduce编程模型把真实世界中网络通信方面的计算（把数据发送到正确的设备）从应用程序逻辑（一旦获取数据就处理它）分离出来。这种分离与数据库的典型使用形成鲜明对比：从数据库获取数据的请求经常发生在应用程序代码的深处。由于MapReduce处理所有网络通信，这使得应用程序代码不用担心部分失效问题，比如其它节点的崩溃：MapReduce自动重试失败的任务而不影响应用程序逻辑。

#### GROUP BY

除了连接以外，“把相关数据放在一起”模式的另一种常见用法是按某个键（如SQL中的`GROUP BY`子句）对记录进行分组。所有具有相同键的记录构成一个组，下一步通常是在每个组内执行某种聚合操作——比如：

* 对每组中记录的数目计数（就像我们统计页面浏览量示例中，你可以用SQL中的`COUNT(*)`表示它）

* 用SQL把某个特定字段的值都加起来（`SUM(fieldname)`）

* 根据某个排序函数选出前*k*个记录

用MapReduce实现这种分组操作最简单方法是设置映射函数，从而生成的键值对使用期望的分组键。之后，分区与排序过程把有着相同键的所有记录放在同一个归纳函数中。因此，在MapReduce之上实现分组和连接看起来非常相似。

分组的另一种常见用途是整理特定用户会话的所有活动事件，方便找出用户采取的动作序列——这叫做*会话化*过程。例如，这样的分析可以用来计算那些浏览你的网站新版本的用户是否比那些浏览旧版本（A/B测试）的用户更愿意消费，或是计算一些营销活动是否物有所值。

如果有多个Web服务器处理用户请求，那么特定用户的活动事件很可能分散在不同服务器的日志文件中。你可以使用会话cookie、用户ID或类似的标识符作为分组键来实现会话化，并且把特定用户的所有活动事件集中在一个地方，同时把不同用户的事件分布在不同的分区中。

#### 处理倾斜

如果有大量数据与单个键有关，“把所有具有相同键的记录放在一起”的模式就会出问题。举个例子，在社交网络中，大多数用户只会与几百人有联系，但少数名人会拥有数百万粉丝。这种不成比例的活动数据库记录被称为*关键对象*或*热键*。

在单个归纳函数种收集与一个名人相关的所有活动（例如，对他们发表内容的回复）会导致明显的倾斜（也被称为*热点*）——也就是说，某个归纳函数必须比其他归纳函数处理了更多的记录（参见“倾斜的工作负载和缓解热点”）。因为MapReduce任务只有在所有的映射函数和归纳函数完成后才算完成，任何后续任务都必须等待最慢的归纳函数完成之后才可以进行。

如果连接的输入有热键，现在有了一些算法可以用来进行补偿。举个例子，Pig中的偏斜联接方法首先执行一个抽样任务从而判断哪些键是热键。在执行实际连接时，映射函数把与热键有关的任何记录发送给任意选择的几个归纳函数中的一个（与传统的MapReduce不同，后者基于键的散列值确定地选择归纳函数）。对于连接的其他输入，与热键相关的记录需要被复制到所有处理那个键的归纳函数中。

这种技术把处理热键的工作分散到多个归纳函数上，使得可以更好地并行化处理，而代价是不得不把其它的连接输入复制到多个归纳函数。Crunch中的分片连接方法也很类似，但需要显式地指定热键而不是使用抽样作业。这种技术也与我们在“倾斜工作负载和缓解热点”中讨论的技术非常类似，它使用了随机化来缓解分区数据库中的热点。

Hive的倾斜连接优化采用了另一种方法。它要求在表的元数据中显式指定热键，之后它会把与这些键相关的记录存储在单独的文件中，与其它记录隔离开。在该表上执行连接时，它对热键使用映射侧连接（见下一节）。

当用热键分组记录并对它们执行聚合操作时，可以用两个阶段执行分组。第一个MapReduce阶段把记录发送到一个随机归纳函数，从而每个归纳函数只在一部分热键记录上执行分组，输出更紧凑的聚合值。然后，第二个MapReduce任务把所有第一阶段归纳函数的值组合成每个键的单个值。

### 映射侧连接

上一节中描述的连接算法在归纳函数中执行实际的连接逻辑，因此被称为归纳侧连接。映射函数负责准备输入数据：从每条输入记录中提取键和值，把键值对分配给归纳函数分区，然后按键进行排序。

归纳侧方式的好处在于你不用对输入数据进行任何假设：无论它的属性和结构如何，映射函数都能为连接做好准备。然而，缺点是所有这些排序、复制到归纳函数然后合并归纳函数的输入，代价都是非常高的。根据可用的内存缓冲区大小，数据在通过MapReduce的各个阶段时有可能会多次写入磁盘。

另一方面，如果你可以对输入数据做出某些假设，使用所谓的映射侧连接可以使连接速度更快。这种方法使用了一个缩减了的MapReduce任务，其中没有归纳函数也没有排序。取而代之的是每个映射器只从分布式文件系统读取一个输入文件块，并只写入一个输出文件到文件系统——仅此而已。

#### 广播哈希连接

执行映射侧连接最简单的方式是在一个大数据集连接一个小数据集的时候。特别是，小数据集需要足够小，从而每个映射函数都可以把它整体加载到内存中。

例如在图10-2中，假设用户数据库足够小到可以放在内存中。在这种情况下，当映射函数启动时，它首先可以把用户数据库从分布式文件系统读入内存中的哈希表。一旦完成，映射函数就可以扫描用户活动事件，查找哈希表中每个时间的用户ID。

这里还有几个映射任务：一个是用于连接的每个大型输入的文件块（在图10-2的例子中，活动事件是大型输入）。每个映射函数都把小型输入完全加载到内存中。

这个简单但有效的算法称为广播哈希连接：广播这个词反映了这样一个事实：用于大型输入分区的每个映射函数都读取小型输入的全部内容（因此，小型输入有效地“广播”到了大型输入的所有分区），而散列这个词反映了它对哈希表的使用。这种连接方法由Pig（名为“复制连接”）、Hive（“MapJoin”）、Cascading和Crunch支持。它还用于数据仓库查询引擎，比如Impala。

与把小型连接输入加载到内存中的哈希表不同，另一种方法是把小型连接输入储存在本地磁盘上的只读索引中。索引中经常使用的部分会保留在操作系统的页面缓存中，因此这种方法可以提供几乎与内存哈希表一样快的随机访问查找，而实际上不需要把数据集放入内存。

#### 分区哈希连接

如果映射侧连接的输入也以相同的方式进行分区，那么哈希连接方法可以独立地应用于每个分区。在图10-2的场景中，你可以根据用户ID的最后一个十进制数字对活动事件和用户数据库进行分区（因此，任意一边都有10个分区）。例如，映射函数3首先把ID以3结尾的所有用户加载到哈希表中，然后扫描每个ID以3结尾的用户的所有活动事件。

如果分区操作正确，可以确保所有你希望连接的记录都位于同一个编号分区中，因此每个映射函数只从每个输入数据集中读取一个分区就足够了。这样做的优点是每个映射函数可以把较少的数据加载到它的哈希表中。

这种方法只有在两个连接输入都有相同数量的分区时才可以工作，并根据相同的键和相同的哈希函数把记录分配给分区。如果输入是由已经执行此分组的现有MapReduce任务生成的，那么这是一个合理的假设。

在Hive中，分区哈希连接称为分桶映射连接。

#### 映射侧合并连接

如果输入数据集不仅以相同的方式进行分区，而且根据相同的键进行排序，那么映射侧连接的另一个变体也适用。在这种情况下，输入是否足够小到可以放在内存里并不重要，因为映射函数可以执行通常由归纳函数执行的合并操作：递增地读取两个输入文件，按升键顺序，然后使用相同的键匹配记录。

如果映射侧合并连接是可能的，这可能意味着先前的MapReduce任务首先将输入数据集引入到这个分了区且排了序的表单中。原则上，这个连接可以在先前任务的归纳阶段执行。但是，在一个单独的、只有映射的任务中执行合并联接仍然是合适的，比如分了区且排了序的数据集除了这个特定的连接之外，还用于其他目的。

#### 有映射侧连接的MapReduce工作流

当MapReduce连接的输出被下游任务消耗时，映射侧或归纳侧连接的选择会影响输出的结构。归纳侧连接的输出通过连接键进行分区和排序，而映射端连接的输出以与大型输入相同的方式进行分区和排序（因为对于连接的大型输入的每个文件块都启动了一个映射任务，而不管是不是使用了分区连接还是广播连接）。

正如前面所讨论的，映射侧连接还对输入数据集的大小、排序和分区做出了更多假设。在优化连接策略时，了解分布式文件系统中数据集的物理布局变得很重要：仅仅知道存储数据目录的编码格式以及名称是不够的；你还必须知道分区的数量以及数据被分区和排序的键。

在Hadoop生态系统中，这类关于数据集划分的元数据通常保存在HCatalog和Hive元存储中。

### 批处理工作流的输出

我们已经讨论了许多实现MapReduce任务工作流的算法，但是我们忽略了一个重要问题：一旦完成了这些处理，结果是什么？为什么我们首先要执行所有这些任务？

对于数据库查询，我们将事务处理（OLTP）的目的与分析目的区分开（见“事务处理还是分析？”一节）。我们看到，OLTP查询通常使用索引按键查找少量记录，以便将它们呈现给用户（比如在网页上）。另一方面，分析性查询通常会扫描大量记录，执行分组和聚合操作，并且输出通常以报告的形式出现：可以是一张显示某项指标随时间的变化的图表，或者是根据某种排名规则排列的前10项，要么是将一定数量的记录细分为几个子类。这类报告的使用者通常是需要作出业务决策的分析师或是经理。

批处理适合出现在哪里？它既不是事务处理，也不是分析任务。它更接近于分析任务，在这个过程中批处理通常会扫描输入数据集的大部分。然而，MapReduce任务的工作流与用于分析目的SQL查询不一样（见“将Hadoop与分布式数据库进行比较”一节）。批处理过程的输出通常不是报表，而是某种其他类型的结构。

#### 构建搜索索引

谷歌最初使用MapReduce是为了它的搜索引擎建立索引，它是由5到10个MapReduce作业的工作流实现的。尽管谷歌后来不再使用MapReduce来实现这个目标，但是如果从构建搜索索引的角度来看，它能帮助你了解MapReduce。（即使在今天，Hadoop MapReduce仍然是为Lucene/Solr构建索引的好方法。）

我们在“全文搜索和模糊索引”一节中简要地看到了全文搜索索引，比如Lucene，是如何工作的：它是一个文件（术语字典），在该文件中你可以有效地查找特定关键字，并找到包含这个关键字的所有文档ID列表（公告列表）。这是搜索索引一个非常简单的视图——实际上它需要许多额外的数据，以便按照相关性对搜索结果进行排序，纠正拼写错误，解析同义词等等——但原则是一直不变的。

如果你需要对一组固定的文档执行全文搜索，那么批处理过程是构建索引的一种非常有效的方法：映射函数根据需要划分文档集合，每个归纳函数为它处理的分区构建索引，之后索引文件被写入分布式文件系统中。构建这样的文档分区索引（见“分区与次级索引”一节)可以非常好地并行化。由于按关键字查询搜索索引是只读操作，这些索引文件一旦创建就不可变。

如果已经建立索引的文档集合变了，一个选择是定期重新运行整个文档集的索引建立工作流，并在完成时用新的索引文件整体替换先前的索引文件。如果只是少量文档发生了变化，这种方法在计算上代价会很高，但是优点是索引建立过程非常容易解释：输入文档，输入索引。

或者可以增量构建索引。如第3章所述，如果想在索引中添加、删除或更新文档，Lucene将写出新的段文件，并在后台异步合并、压缩段文件。我们会在第11章中看到更多关于这种增量处理的信息。

#### 键值对存储作为批处理输出

搜索索引只是批处理工作流输出的一个例子。批量处理的另一个常见用途是构建机器学习系统，例如分类器（例如垃圾邮件过滤器、异常检测、图像识别）和推荐系统（比如你可能认识的人，你可能感兴趣的产品，或是相关的搜索）。

这些批处理任务的输出通常是某种数据库：比如，可以通过用户ID查询数据库从而获得适合该用户的推荐朋友，或者可以通过产品ID查询数据库从而获得相关产品的列表。

这些数据库需要从处理用户请求的Web应用程序中查询，这个应用通常是与Hadoop基础结构分离的。那么，批处理过程的输出如何返回到Web应用程序可以查询的数据库中呢？

最明显的选择大概是直接在映射函数或者归纳函数中使用你喜欢的数据库的客户端开发库，然后在批处理作业中直接写入数据库服务器，每次记录一条。这行得通（假设防火墙规则允许从Hadoop环境直接访问生产数据库），但这是个坏主意，原因有几点：

* 正如前面在讨论连接时提到的，为每个记录发出网络请求比批处理任务一般正常的吞吐量要低几个数量级。即使客户端库支持批处理，性能也会很差。

* MapReduce任务通常并行执行许多任务。如果所有的映射函数或者归纳函数同时写入相同的输出数据库，并且期望有批处理一般的速率，这个数据库很容易不堪重负，并且查询的性能也会受到影响。这反过来又会在系统的其他部分造成运营问题。

* 一般地，MapReduce为任务输出提供了一个干净的全有或全无保证：如果任务成功，结果就是每个任务只运行一次的输出，哪怕有些任务失败而且必须重试；如果整个任务失败，就不会产生任何输出。然而，从任务内部写入外部系统会产生外部可见的副作用，而这些副作用无法用这种方式隐藏。因此，你必须担心其他系统可以看到部分完成的任务结果这件事，以及Hadoop任务尝试与推测执行的复杂性。

一个更好的解决方案是在批处理任务内部构建一个全新的数据库，并将其作为分布式文件系统中任务输出目录内的文件写入，就像上一节中的搜索索引一样。这些数据文件一旦写入就是不可变的，并且可以批量加载到处理只读查询的服务器中。各种键值存储支持在MapReduce任务中构建数据库文件，包括Voldemort、Terrapin、ElephantDB和HBase批量加载。

构建这些数据库文件是MapReduce的一个很好的应用：使用映射函数提取键，然后按这个键进行排序已经是构建索引的很大一部分工作。由于大部分键值存储是只读的（文件只能由批处理作业写入一次，之后是不可变的），所以数据结构非常简单。例如，它们不需要WAL（见“使B树变得可靠”一节）。

把数据加载到Voldemort的时候，服务器在继续用旧数据文件响应请求的同时，新数据文件从分布式文件系统拷贝到服务器的本地磁盘。一旦复制完成，服务器就会自动切换到查询新文件。在这个过程中如果发生了什么问题，它可以很容易地切换回旧文件，它们仍然在那并且是不变的。

#### 批处理输出的哲学

我们在这一章前面讨论的Unix哲学（见“Unix哲学”一节）鼓励以非常明确的数据流来实验：程序读取它的输入并写入它的输出。在这个过程中，输入将保持不变，任何先前的输出都将完全替换为新的输出，并且没有其他副作用。这意味着你可以随时重新执行一个命令，优化或调试它，而不用担心扰乱系统的状态。

处理MapReduce任务的输出遵循同样的原则。通过将输入视为不可变并且避免各种副作用（比如写入外部数据库），批处理作业不仅可以获得良好的性能，而且更易于维护：

* 如果在代码中引入了bug，而输出是错误的或被损坏了，你可以简单地回滚到先前的版本并重新执行任务，输出将再次正确。或者更简单的是，你可以把旧的输出保存在不同的目录中，然后切换到它。具有读写事务的数据库没有这种属性：如果部署了有bug的代码，写入了错误的数据到数据库，那么回滚代码对于修复数据库中的数据没有任何帮助。（能够从错误代码中恢复的理念被称为*人为容错*）。

* 因为易于回滚，功能开发相比于在错误可以导致无法挽回损失的环境中要快很多。这种把*不可逆转性最小*化的原则有利于敏捷软件开发。

* 如果映射或归纳任务失败，MapReduce框架将自动重新调度它，并在相同的输入上再次运行它。如果失败是由于代码中的bug造成的，它将不断导致崩溃，并在几次尝试后最终导致任务失败；但是如果失败是由于暂时的问题造成的，那么故障是可以容忍的。这种自动重试只有在输入是不可变的，且失败任务的输出会被MapReduce框架丢弃的前提下才是安全的。

* 同一组文件可以用作各种不同任务的输入，包括计算指标，并评估任务输出是否具有预期的特征（例如，把它与上一次运行的输出进行比较，并衡量差异）的监视任务。

* 与Unix工具一样，MapReduce任务将逻辑与连接分离（即配置输入和输出目录），这分离了关注点，使得代码可以重用：一个团队可以专注于实现一项任务，而其他团队则可以决定在何时何地执行这个任务。

在这些领域，对于Unix有益的设计原则对于Hadoop似乎也很有效，但是在某些方面Unix和Hadoop也有所不同。比如说，由于大多数Unix工具都假定输入是无类型的文本文件，因此它们必须执行大量的输入解析（本章开头的日志分析示例使用`{print $7}`来提取URL）。在Hadoop上，一些这种低价值的语法转换通过使用更结构化的文件格式被消除了：Avro（见“Avro”一节）和Parquet（见“面向列的存储”一节）经常被使用，因为它们提供了高效的基于模式的编码，并允许模式定义随时间的演变（见第4章）。

### 比较Hadoop与分布式数据库

如我们所看到的，Hadoop有点像分布式版本的Unix，其中HDFS是文件系统而MapReduce是Unix进程的奇特实现（它总是在映射与归纳阶段之间运行`sort`工具）。我们看到了如何在这些元类型之上实现各种连接和分组操作。

当MapReduce论文发布的时候，它——从某种意义上讲——并不都是新的。我们在前几节中讨论的所有处理与并行连接算法，在十多年前就已经在所谓的*大规模并行处理*（MPP）数据库中实现了。例如，Gamma数据库设备，Teradata以及Tandem NonStop SQL都是这一领域的先驱。

最大的区别在于MPP数据库侧重于在一组机器上并行执行分析性SQL查询，而MapReduce与分布式文件系统的组合则提供了更类似于可以运行任意程序的通用操作系统。

#### 存储的多样性

数据库要求你根据特定的模型（比如关系模型或文档模型）来构造数据，而分布式文件系统中的文件只是字节序列，可以使用任何数据模型和编码来编写。它们可能是数据库记录的集合，但也可以是文本、图像、视频、传感器读数、稀疏矩阵、特征向量、基因组序列，或者任何其他类型的数据。

坦率地说，Hadoop开启了不加区分地把数据转储到HDFS中的可能性，并且在此之后才会搞清楚如何进一步处理它。相比之下，MPP数据库通常需要对数据和查询模式预先进行仔细的建模，然后才能把数据导为数据库专有的存储格式。

从纯粹主义者的角度来看，谨慎的建模和导入过程似乎是可取的，因为它意味着数据库的用户有更高质量的数据可供使用。然而在实践中，简单快速地提供数据——哪怕它是一种奇怪、难用、原始的格式——也往往比预先确定理想的数据模型更有价值[54]。

这种理念与数据仓库类似（见“数据仓库”一节）：只是把来自大型组织不同组件的数据集中在一个地方是就很有价值，因为它使得连接先前完全分离的数据集成为可能。MPP数据库需要的精细的模式设计使集中化数据收集的速度变慢；收集原始形式的数据，而之后再考虑模式设计可以加快数据收集速度（这个概念有时被称为“数据湖”或是“企业数据中心”）。

不加区分的数据导把负担转移到了数据的解读阶段：不再强制数据集的生产者将数据转为标准格式，数据的解读变成了消费者的问题（读取时定义模式方式；见“文档型模型中模式的灵活性”一节）。如果生产者和消费者是有不同优先级的不同团队，这会是一个优势。甚至可能没有一个理想的数据模型，而只是适用于不同目的数据视图。简单地把数据以原始格式导出就可以进行几个这样的转换。这种方法被称为寿司原则：“原始数据更好”。

因此，Hadoop经常被用于实现ETL进程（见“数据仓库”一节）：来自事务处理系统中的数据以某种原始形式导出到分布式文件系统中，之后MapReduce任务被用来清理数据，把它转换为关系型形式，然后导入MPP数据仓库进行分析。数据建模仍然会发生，但是是在一个单独的步骤之内，与数据收集解耦。这种解耦之所以是可能的是因为分布式文件系统支持以任何格式编码的数据。

#### 处理模型的多样性

MPP数据库是一个完整的、紧密集成的软件，负责磁盘上的存储布局、查询规划、调度以及执行。由于这些组件都可以根据数据库的特定需求进行优化，所以系统作为一个整体可以在其设计的查询类型上获得非常好的性能。此外，SQL查询语言支持表达性的查询和优雅的语义，而无需编写代码，使业务分析人员所使用的图形工具（比如Tableau）能够访问它。

另一方面，并非所有类型的处理都可以以SQL查询的形式明智地表达。例如，如果你正在构建机器学习和推荐系统，或者具有相关性排序模型的全文搜索索引，或者执行图像分析，那么你很可能需要一个更通用的数据处理模型。这类处理通常特定于某个特定应用程序（例如机器学习中的特征工程、机器翻译中的自然语言模型、欺诈预测的风险评估功能），因此它们不可避免地需要编写代码，而无法只是查询。

MapReduce使工程师能够轻松地在大型数据集上运行自己的代码。如果你有HDFS和MapReduce，你可以在这之上构建一个SQL查询执行引擎，事实上这就是Hive项目所做的事。然而你还可以编写许多其他形式的批处理过程，它们本身并不适合作为SQL查询来表示。

之后，人们发现MapReduce对于某些类型的处理来说限制太多，执行效果太差，因此在Hadoop之上开发出了许多其它处理模型（我们将在“MapReduce之外”一节中看到其中的一些模型）。有SQL和MapReduce两个处理模型是不够的：我们需要更多不同的模型！由于Hadoop平台的开放性，实现一系列不同的方法是可行的，而在单个MPP数据库的范围内是不可能的。

关键的是，这些不同的处理模型都可以运行在一个共享使用的机器集群上，都可以访问分布式文件系统上的相同文件。在Hadoop方法中，不需要为不同类型的处理方式把数据导入几个不同的专门系统：系统本身具有足够的灵活性，能够支持同一集群中的一组相互不同的工作负载。不需要移动数据，于是可以更容易地从数据中获得值，也可以更容易地尝试新的处理模型。

Hadoop生态系统包括支持随机访问的OLTP数据库，比如HBase（见“SSTable与LSM树”一节），以及MPP风格的分析性数据库，比如Impala。无论是HBase还是Impala都不使用MapReduce，但它们都使用HDFS进行存储。它们在访问和处理数据方面是非常不同的方法，然而它们仍然可以共存并集成在同一系统中。

#### 针对频繁故障的设计

在比较MapReduce与MPP数据库时，设计方法方面又出现了两个不同之处：故障的处理，以及对内存和磁盘的使用。与在线系统相比，批处理过程对故障的敏感性较低，因为如果失败用户不会立即受到影响，而且它们可以再次运行。

如果节点在执行查询时崩溃，大多数MPP数据库会中止整个查询，之后要么让用户重新提交查询，要么自动再次运行。由于查询通常最多运行几秒钟或几分钟，这种处理错误的方式是可以接受的，因为重试的成本并不高。MPP数据库也倾向于将尽可能多的数据保存在内存中（比如使用哈希连接）从而避免从磁盘读取的消耗。

另一方面，通过在单任务粒度上重试，MapReduce可以容忍映射任务或者归纳任务的失败，而不影响整个任务。它也非常愿意将数据写入磁盘，一部分原因是为了容错，另一部分是因为数据集会大到无法放在内存中使用。

MapReduce方法更适合于大型任务：处理这么多数据且运行时间如此长的作业，过程中很可能会遇到至少一次任务失败。在这种情况下，由于单个任务失败而重新运行整个作业是很浪费的。即使单个任务粒度的恢复使得无故障处理的时间更慢，如果任务的失败率足够高，这样做仍然是合理的权宜之计。

但这些假设的真实性有多少？在大多数集群中，确实会发生机器故障，但并不是特别频繁——可能很少，以至于大多数作业不会发生机器故障。为了容错引入大量的消耗真的值得吗？

为了理解MapReduce节省内存以及起任务级恢复的原因，了解MapReduce最初被设计针对的环境是很有帮助的。谷歌有混合使用的数据中心，在线服务与离线批处理作业运行在同一台机器上。每个任务都有使用容器强制分配的资源（CPU核心、内存、磁盘空间等）。每个任务也有一个优先级，如果一个高优先级的任务需要更多的资源，那么同一台机器上的低优先级任务就会被终止（抢占），以便释放资源。优先级也决定了计算资源的定价：团队必须为他们所使用的资源付费，而高优先级的进程花销更多。

这种体系结构使得非生产性质（低优先级别）计算资源过量使用，因为系统知道它可以在必要时回收这些资源。相对于把生产任务和非生产任务分开的系统，过量使用反而可以更好地利用机器，提高效率。然而由于MapReduce作业以低优先级运行，因此它们在任何时候都有被抢占的风险，因为更高优先级的进程需要它们的资源。批处理作业有效地“消化了残羹剩饭”，利用了在高优先级进程获得它们所需要以外的任何计算资源。

在谷歌，一个运行了一个小时的MapReduce任务有大约5%的风险被终止，以便为更高优先级的进程腾出空间。这个速率比硬件问题、机器重新启动或其他原因导致的故障率高出一个数量级。按照这种抢占率，如果一个作业有100个任务，每个任务运行10分钟，则至少有一个任务在完成之前被终止的风险大于50%。

这就是MapReduce被设计用来容忍频繁的意外任务终止的原因：这并不是因为硬件特别不可靠，而是因为任意终止进程的自由能够更好地利用计算集群中的资源。

在开源的集群调度器中，抢占的应用并不广泛。YARN的CapacityScheduler支持抢占从而平衡不同队列之间的资源分配，但在截至目前，一般性质的优先级抢占在YARN、Mesos以及Kubernetes中都不支持。在任务不会经常终止的环境中，MapReduce的设计决策就没有那么重要了。在下一节中，我们将研究MapReduce的一些替代方案，它们可以做出不同的设计决策。

## MapReduce之外

虽然MapReduce在2000年代后期变得非常流行且被大量宣传，但是它只是分布式系统中许多编程模型中的一种。根据数据的体积、结构以及与之相关的处理类型，一种计算也许用其它工具更适合于表达。

尽管如此我们在本章中还是花了很多时间讨论MapReduce，因为它是一个有用的学习工具，因为它是一种分布式文件系统之上相当清晰和简单的抽象。这里的简单，是指理解它是用来做什么的很简单，而不指它简单易用。事实恰恰相反：使用原始的MapReduce API实现一个复杂的处理任务实际上是非常困难和费力的——比如说，你需要从头开始实现连接的算法。

针对直接使用MapReduce的困难，在MapReduce之上创建出了各种高级编程模型（Pig、Hive、Cascading、Crunch）作为抽象。如果了解了MapReduce是如何工作的，那么它们是相当容易学习的，并且它们的高级数据结构使得许多常见的批处理任务更容易实现。

然而MapReduce执行模型本身也存在问题，这些问题无法通过添加另一层抽象来解决，而且表现为在某些种处理的性能较差。一方面，MapReduce非常健壮：你可以使用它在不可靠的、频繁中止任务的多租户系统上处理任意数量的海量数据，而且它仍然可以完成任务（尽管速度缓慢）。另一方面，对于某些类型的处理工作，其他工具有时要快几个数量级。

在本章的其余部分中，我们将介绍一些批处理的替代方案。在第11章中我们将转向流处理，这可以被看作是另一种加快批处理的方式。

### 中间状态的物化

如前所述，每个MapReduce作业都独立于其他每个作业。作业与其它事物的主要接触点是其在分布式文件系统上的输入和输出目录。如果你希望一个作业的输出成为第二个作业的输入，就需要把第二个作业的输入目录配置为与第一个作业的输出目录相同，并且外部工作流调度程序必须在第一个作业完成之后才启动第二个作业。

如果第一个作业的输出是希望在组织中广泛发布的数据集，那么这个设置是合理的。在这种情况下，你需要能够按名称引用它，并把它作为几个不同作业（包括其他团队开发的作业）的输入重用。把数据发布到分布式文件系统中的众所周知的位置使得作业之间松散耦合，这样作业就不需要知道谁在产生输入谁在使用输出（见“逻辑与连接的分离”）。

然而许多情况下，你也知道一个作业的输出只是作为另一个作业的输入，而这个作业也是由同一个团队维护的。在这种情况下，分布式文件系统上的文件只是*中间状态*：一种把数据从一个作业传递到下一个作业的方法。在由50或100个MapReduce作业组成的用于构建推荐系统的复杂工作流中，就有许多这样的中间状态。

把这些中间状态写入文件的过程称为*物化*。（我们在“聚合：数据立方体与物化视图”一节讨论物化视图的时候遇到过这个术语。它是指计算某些操作的结果并记录它，而不是在请求时按需计算。）

与之相反，本章开头的日志分析示例使用Unix管道把一个命令的输出与另一个命令的输入连接起来。管道不会完全物化中间状态，相反只是使用一个小内存缓冲区把输出递进地串流到输入端。

与Unix管道相比，MapReduce完全物化中间状态的方法有这些缺点：

* MapReduce作业只能在前面作业中的所有（生成输入的）任务完成之后才开始，而由Unix管道连接起来的进程将同时启动，并在输出生成时立即被消耗了。在不同的机器上的偏斜或是变化的负载意味着，作业通常包含一些比其他任务花费更长的时间来完成的分层任务。必须等到所有先前作业的任务完成导致工作流作为一个整体变慢了。

* 映射函数通常是多余的：它们只是读取了归纳函数刚刚写入的同一个文件，并为下一阶段的分区和排序做好准备。在许多情况下，映射器代码可以是前一个归纳函数的一部分：如果归纳函数的输出按照映射函数输出相同的方式进行分区和排序的话，那么归纳函数可以直接链接在一起，而无需接入映射函数。

* 把中间状态存储在分布式文件系统中意味着这些文件被复制到了多个节点上，这对于这些临时数据来说往往是没有必要的。

#### 数据流引擎

为了用MapReduce解决这些问题，开发了几个用于分布式批量计算的新执行引擎，其中最著名的是Spark、Tez和Flink。它们的设计方式有不同之处，但它们有一个共同点：它们把整个工作流作为一个任务来处理，而不是把它分解为各自独立的子作业。

由于它们用好几个处理阶段对数据流进行显式地建模，这些系统被称为*数据流引擎*。与MapReduce类似，它们的工作方式是在单线程上反复调用用户定义的函数每次处理一条记录。通过对输入分区并行化工作，之后通过网络复制一个函数的输出，从而成为另一个函数的输入。

与MapReduce不同的是，这些函数不需要扮演交替映射和归纳的严格角色，而可以以更灵活的方式组织起来。我们把这些函数称为*操作符*，数据流引擎提供了几种不同的选项来把一个操作符的输出连接到另一个操作符的输入：

* 一种选择是按照键重新划分、排序记录，就像MapReduce的洗牌阶段（见“MapReduce的分布式执行”一节）。这个功能使得归并连接以及分组的方式与ManReduce的相同。

* 另一种可能是接受多个输入并以相同的方式对它们进行分区，但是跳过排序。这样可以节省分区哈希连接的工作，在这里对记录分区很重要但是顺序是无关的，因为无论如何构建哈希表的时候顺序会随机化。

* 对于广播哈希连接，可以把来自一个操作符的相同输出发送到联接运算符的所有分区。

这种类型的处理引擎基于Dryad和Nephele等研究系统，与MapReduce模型相比，它具有几个优点：

* 诸如排序之类的代价高昂的工作只需要在真正需要的地方执行，而不总是在每个映射和归纳阶段之间默认执行。

* 不存在没有必要的映射任务，因为映射函数所做的工作通常可以合并到先前的归纳操作符中（因为映射函数不会更改数据集的分区）。

* 因为工作流中的所有连接和数据依赖都是显式声明的，所以调度程序可以总览在哪里需要使用哪些数据，因此可以进行局部优化。例如，它可以尝试把消费某些数据的任务与生成数据的任务放在同一台设备上，这样就可以通过共享内存缓冲区交换数据，而无需通过网络复制数据。

* 把操作符之间的中间状态保存在内存中或写入本地磁盘通常就足够了，这比把它们写入HDFS所需的I/O要少（在HDFS中它们就必须被复制到多台设备上，并且写入每个副本上的磁盘中）。MapReduce已经把这种优化用于映射函数的输出，但是数据流引擎将此思想推广到所有中间状态。

* 操作符可以在输入就绪后立即执行；不需要等待前一阶段完全完成再执行下一个阶段。

* 现有的Java虚拟机（JVM）进程可以被重用以运行新的操作符，与MapReduce相比减少了启动开销（MapReduce为每个任务启动一个新的JVM）。

你可以使用数据流引擎实现与MapReduce工作流相同的计算，而且由于这里描述的优化它们通常执行得更快。因为运算符是映射与归纳的泛化，相同的处理代码可以在任何执行引擎上运行：在Pig、Hive或Cascading中实现的工作流可以通过简单的配置改变从MapReduce切换到Tez或者Spark，而无需修改代码。

Tez是一个相当轻量的库，它依赖YARN洗牌服务在节点之间实际复制数据，而Spark和Flink是包含着各自网络通信层、调度器和面向用户的API的大型框架。我们稍后会讨论这些高级API。

#### 容错

把中间状态完全物化到分布式文件系统的优点之一是持久性，这使得MapReduce中的容错相当容易：如果任务失败，可以在另一台机器上重新启动它，然后从文件系统再次读取相同的输入。

Spark、Flink和Tez避免把中间状态写入HDFS，因此它们采用不同的方法来进行容错：如果机器失效而设备上的中间状态丢失，那么从其它仍然可用的数据（如果可以的话是最好是先前的中间阶段，或者通常位于HDFS上的原始输入数据）重新计算。

要启用这种计算，框架必须跟踪给定的数据块是如何计算的——它使用了哪些输入分区，对这些数据应用了哪些操作符。Spark使用弹性的分布式数据集（RDD）抽象来跟踪数据的祖先，而Flink设置检查点检查操作符状态，允许它继续运行在执行过程中遇到错误的操作符。

在重新计算数据的时侯，知道计算是不是确定性的是很重要的：也就是说，如果给定相同的输入数据，操作符是不是总是产生相同的输出？如果一些丢失了的数据已经被发送到下游操作符，那么这个问题就变得很重要了。如果操作符重新启动但是用于重新计算的数据与原始丢失了的数据不相同，那么下游的操作符很难解决旧数据和新数据之间的矛盾。在非确定性操作符情况的解决方案是通常也杀死下游操作符，并再次在新数据上运行这些操作符。

为了避免这种级联故障，最好是让这些操作符具有确定性。然而请注意，不确定行为很容易意外地蔓延进来：例如，许多编程语言在迭代哈希表元素时不保证任何特定的顺序，许多概率和统计算法显式地依赖使用随机数，而对系统时钟或外部数据源的任何使用都是不确定的。为了可靠地从故障中恢复，需要消除这种不确定性，例如使用固定的种子产生伪随机数。

通过重新计算数据从故障中恢复并不总是正确的答案：如果中间数据比源数据小得多，或者计算如果非常需要CPU，那么把中间数据物化到文件会比重新计算要便宜。

#### 论物化

回到Unix类比，我们看到MapReduce就像把每个命令的输出写入一个临时文件，而数据流引擎看起来更像Unix管道。尤其是Flink，它是围绕着管道执行的理念构建的：也就是说，把一个操作符的输出逐步传递给其他操作符，而不是等待输入完成之后再开始处理它。

排序操作在产生任何输出之前不可避免地需要消费它的整个输入，因为最后一个输入记录可能会有排序最小的键，因此需要排在输出记录的第一个。因此任何需要排序的操作符都需要累积状态，至少也是暂时的。但是工作流的许多其他部分可以以流水线方式执行。

当作业完成时，它的输出需要变得持久以便用户能够找到并使用它——最可能的情况是它再次被写入分布式文件系统。因此当使用数据流引擎时，在HDFS上的物化数据集通常仍然是作业的输入和最终输出。与MapReduce类似，输入是不可变的，输出被完全替换。与MapReduce相比的改进是，你不需要自己把所有中间状态写入文件系统。

### 图与迭代处理

在“类图数据模型”一节中，我们讨论了使用图对数据建模，以及使用图形查询语言遍历图中的边和顶点。第二章的讨论集中在OLTP风格的使用上：快速地执行查询以找到少量匹配某些条件的顶点。

在批处理主题中研究图也是很有趣的，它的目标是在整张图上执行某种离线处理或分析。这种需求通常出现在机器学习类应用中，比如推荐引擎或排名系统中。例如，最著名的图表分析算法之一是PageRank，它试图基于与其链接的其他网页来估计网页的流行度。它用于确定Web搜索引擎呈现结果顺序的公式的一部分。

> **注意**
>
> 数据流引擎，如Spark，Flink和Tez（见“中间状态的物化”一节）通常把作业中的操作符按有向无圈图（DAG）安排。这与图处理不同：在数据流引擎中，从一个操作符到另一个操作符的数据流被构造为一张图，而数据本身通常由关系风格的元组组成。在图处理中，数据本身具有图的形式。又是一个不幸的命名混乱问题！

许多图算法是通过一次遍历一条边，把一个顶点和一个相邻顶点连接起来以传播某些信息，重复这个过程知道某些条件被满足——例如，直到没有更多的边以遍历，或者直到某些指标收敛。我们在图2-6中看到了一个例子，它通过重复跟踪指示哪个位置在哪个其他位置之中的边（这种算法称为传递闭包），构建出了数据库中北美所有位置的列表。

把图存储在分布式文件系统中（在包含顶点和边的列表的文件中）是可能的，但是这种“重复直到完成”的想法无法用普通MapReduce表示，因为它只传递一次数据。因此，这种算法通常以迭代的方式实现：

1. 外部调度程序运行批处理来计算算法的一个步骤。

2. 当批处理过程完成时，调度程序将检查它是否已经完成（基于完成条件——例如，没有更多的边可以遍历，或者与上一次迭代相比，变动小于某个阈值）。

3. 如果尚未完成，调度程序返回到步骤1，并运行另一轮批处理过程。

这种方法可以工作，但是使用MapReduce实现它通常效率很低，因为MapReduce没有考虑到算法的迭代性质：它总是读取整个输入数据集并生成一个全新的输出数据集，即使是与上一次迭代相比图只有一小部分发生了变化。

#### Pregel处理模型

作为批量处理图的优化模型，批量同步并行（BSP）的计算模型开始变得流行。其中，它被Apache Giraph、Spark的GraphX API以及Flink的Gelly API实现。它也被称为Pregel模型，因为谷歌的Pregel论文推广了这种处理图的方法。

回想一下在MapReduce中，映射函数在概念上“发送消息”到对归纳函数的特定调用，因为框架收集了所有有着相同键的映射函数输出。Pregel背后也有类似的理念：一个顶点可以“发送消息”到另一个顶点，通常这些消息是沿着图中的边发送的。

在每一次迭代中，都会为每个顶点调用一个函数，把所有发送给它的消息传递给它——就像对归纳函数的调用一样。与MapReduce不同的是在Pregel模型中，顶点从一次迭代到下一次迭代的过程中都会在内存中记住自己的状态，因此函数只需要处理新传入的消息。如果在图的某些部分没有发送任何消息，那么就不需要做任何事。

这有点类似于参与者模型（见“分布式参与者框架”），如果您把每个顶点都看作一个参与者，除了顶点状态以及顶点之间的消息是容错的和持久的，并且通信以固定的循环进行：那么在每次迭代时，框架会传递在上一次迭代中发送的所有消息。参与者通常没有这样的时间性保证。

#### 容错

顶点只能通过消息传递通信（而不是直接互相查询）这个事实有助于提高Pregel作业的性能，毕竟消息可以被批处理，而且等待通信的时间更短。唯一的等待发生在迭代之间：因为Pregel模型保证在当次迭代中发送的所有消息都会在下一次迭代中传递，前次迭代必须完成，并且所有的消息必须通过网络复制，然后下一个迭代才能开始。

尽管底层网络可能丢掉、复制或任意性地延迟消息（见“不可靠的网络”一节），然而Pregel的实现仍然保证消息在下一次迭代的目标顶点上只处理一次。与MapReduce类似，框架自动地从故障中恢复从而简化了Pregel上算法的编程模型。

这种容错是通过在迭代结束时定期检查所有顶点的状态来实现的——即把所有的状态都写入持久存储中。如果一个节点失效并且失去了内存中的状态，最简单的解决方案是把整个图的计算回滚到前一个检查点，然后重新启动计算。如果算法是确定性的且记录了所有的消息，那么还可以选择性地只恢复丢失的分区（就像我们之前讨论的数据流引擎一样）。

#### 并行执行

顶点不需要知道它正在哪个物理设备上执行；当它向其他顶点发送消息时，它只是把它们发送到一个顶点ID。框架来决定如何分割图——也就是决定哪个顶点在哪个设备上运行，以及如何在网络上路由消息以便它们被发送到了正确的地方。

因为编程模型一次只处理一个顶点（有时称为“像顶点一样思考”），所以框架可能以任意方式划分图。理想情况下，如果顶点之间需要大量通信，就可以把它们分配到同一台机器上。然而，要找到这样的优化分区是很难的——在实践中，图通常只是被任意分配的顶点ID分割，而不会尝试把相关的顶点分配在一起。

因此，图算法往往存在大量的跨设备通信开销，中间状态（节点之间发送的消息）往往比原始图还要大。通过网络发送消息的开销会显著降低分布式图算法的速度。

因此，如果图能够放在单台设备的内存里，那么很可能单机（甚至是单线程）算法的性能将优于分布式批处理过程的性能。即使图太大无法放在内存中，它也可以放在单台设备的磁盘上，使用诸如GraphChi这样的框架进行单设备处理是可行。如果图大到无法适应一台机器，像Pregel这样的分布式方法是免不了的；高效的并行化图算法是一个正在进行的研究领域。

### 高级API与语言

自从MapReduce流行这么多年以来，用于分布式批处理的执行引擎变得成熟了。到目前为止，基础设施已经足够强大，可以在10000多台机器集群上存储和处理数以PB字节的数据。随着运营这样规模的批处理问题被或多或少地解决了，人们的注意力转向了其他领域：改进编程模型，提高处理效率，以及扩大这些技术能够解决的问题。

如之前谈到的，例如Hive、Pig、Cascading以及Crunch这样的高级语言和API变得流行起来，是因为手工编写MapReduce作业非常困难。随着Tez的出现，这些高级语言还有了额外的好处，那就是能够迁移到新的数据流执行引擎而无需重写作业代码。Spark和Flink还包括了它们自己的高级数据流API，通常是从FlumeJava[34]中获得灵感。

这些数据流API通常使用关系式构建模块来表示计算：把数据集按照某个字段连接起来；按键对元组进行分组；按某种条件进行过滤；通过计数、求和或其他函数聚合元组。在内部，这些操作是使用我们在本章前面讨论过的各种连接和分组算法来实现的。

除了需要较少代码的这个明显优点外，这些高级接口还允许交互使用，在这种情况下，您可以在命令行中增量地编写分析代码，并经常运行它来观察它正在做什么。这种开发风格在探索数据集以及尝试处理数据集的方法时非常有用。它也让人联想到我们在“Unix哲学”一节中讨论过的Unix哲学。

此外，这些高级接口不仅使使用系统的人更加高效，而且在设备级别提高了作业的执行效率。

#### 向声明性查询语言的迁移

与编写代码执行连接相比，把连接指定为关系型运算符的一个优点是，框架可以分析连接输入的属性，然后自动决定上述哪些连接算法最适合手头的任务。Hive、Spark和Flink有基于成本的查询优化器可以做到这一点，甚至可以更改联接的顺序从而最小化中间状态的数量。

连接算法的选择对批处理作业的性能有很大的影响，而不需要理解和记住本章中所讨论的各种连接算法是很好的。如果连接是以声明式的方式指定的，这就是可能的：应用程序只需声明哪些联接是必需的，而查询优化器将决定如何最好地执行这些连接。我们以前在“查询数据语言”中遇到过这种想法。

但是在其他方面，MapReduce及其数据流继承者与SQL完全声明性的查询模型有很大不同。MapReduce是围绕函数回调的思想构建的：对于每个记录或每组记录，调用一个用户定义的函数（映射函数或还原函数），而这个函数可以自由调用任意代码以决定输出内容。这种方法的优点是，您可以利用现有开发库的大生态系统来进行解析、自然语言分析、图像分析，以及运行数值或统计算法。

轻松运行任意代码的自由是MapReduce传统批处理系统与MPP数据库的最大区别（见“把Hadoop与分布式数据库进行比较”一节）；虽然数据库有编写用户定义函数的工具，但它们通常使用起来很麻烦，而且与大多数编程语言中广泛使用的包管理器和依赖关系管理系统（比如Java中的Maven、JavaScript中的npm以及Ruby中的Rubygems）集成得并不好。

然而数据流引擎发现，在除了连接之外的领域加入更多声明性特性也是有好处的。例如，如果一个回调函数只包含一个简单的过滤条件，或者它只是从一个记录中选择了一些字段，那么在每个记录上调用该函数都需要大量的CPU开销。如果这种简单的过滤和映射操作是以声明的方式表示的，查询优化器可以利用面向列的存储布局（见“面向列的存储”一节），并且只从磁盘读取所需的列。Hive、Spark DataFrames以及Impala还使用向量化执行（见“内存带宽与向量化处理”一节）：在对CPU缓存友好的内部循环中迭代数据，并避免函数调用。Spark生成JVM字节码，而Impala使用LLVM为这些内部循环生成本地代码。

通过在高级API中包含声明性特性，并在执行过程中使用能够利用它们的查询优化器，批处理框架开始看起来更像MPP数据库（并且可以实现类似的性能）。同时，通过具有能够运行任意代码和读取任意格式数据的可扩展性，它们保留了它们的灵活性优势。

#### 不同领域的特化

虽然能够运行任意代码的这种可扩展性是有用的，但是也有许多常见的情况中标准处理模式不断重复出现，因此值得拥有常用构建模块的可重用实现。传统上，MPP数据库满足了商业智能分析和业务报告的需要，但这只是应用批处理众多领域中的一个。

另一个日益重要的领域是统计和数值算法，这是机器学习应用需要的，比如分类和推荐系统。可重用的实现正在出现：例如，Mahout在MapReduce、Spark和Flink之上实现了各种机器学习算法，而MADlib在关系MPP数据库（Apache HAWQ）中实现了类似的功能。

同样有用的是空间算法，如K-近邻算法，它在多维空间中搜索接近给定项的项——一种相似性搜索。近似搜索对于基因组分析算法也很重要，这些算法需要找到相似但不完全相同的字符串。

批处理引擎正在越来越广泛的领域中用来执行分布式执行算法。随着批处理系统获得内置功能和高级的声明性操作符，并且随着MPP数据库变得更可编程化和灵活，二者开始变得越来越相似：毕竟，它们都只是存储和处理数据的系统。

## Summary

In this chapter we explored the topic of batch processing. We started by looking at Unix tools such as awk, grep, and sort, and we saw how the design philosophy of those tools is carried forward into MapReduce and more recent dataflow engines. Some of those design principles are that inputs are immutable, outputs are intended to become the input to another (as yet unknown) program, and complex problems are solved by composing small tools that “do one thing well.”

In the Unix world, the uniform interface that allows one program to be composed with another is files and pipes; in MapReduce, that interface is a distributed filesystem. We saw that dataflow engines add their own pipe-like data transport mechanisms to avoid materializing intermediate state to the distributed filesystem, but the initial input and final output of a job is still usually HDFS.

The two main problems that distributed batch processing frameworks need to solve are:

*Partitioning*

In MapReduce, mappers are partitioned according to input file blocks. The output of mappers is repartitioned, sorted, and merged into a configurable number of reducer partitions. The purpose of this process is to bring all the related data — e.g., all the records with the same key — together in the same place.

Post-MapReduce dataflow engines try to avoid sorting unless it is required, but they otherwise take a broadly similar approach to partitioning.

*Fault tolerance*

MapReduce frequently writes to disk, which makes it easy to recover from an individual failed task without restarting the entire job but slows down execution in the failure-free case. Dataflow engines perform less materialization of intermediate state and keep more in memory, which means that they need to recompute more data if a node fails. Deterministic operators reduce the amount of data that needs to be recomputed.

We discussed several join algorithms for MapReduce, most of which are also internally used in MPP databases and dataflow engines. They also provide a good illustration of how partitioned algorithms work:

*Sort-merge joins*

Each of the inputs being joined goes through a mapper that extracts the join key. By partitioning, sorting, and merging, all the records with the same key end up going to the same call of the reducer. This function can then output the joined records.

*Broadcast hash joins*

One of the two join inputs is small, so it is not partitioned and it can be entirely loaded into a hash table. Thus, you can start a mapper for each partition of the large join input, load the hash table for the small input into each mapper, and then scan over the large input one record at a time, querying the hash table for each record.

*Partitioned hash joins*

If the two join inputs are partitioned in the same way (using the same key, same hash function, and same number of partitions), then the hash table approach can be used independently for each partition.

Distributed batch processing engines have a deliberately restricted programming model: callback functions (such as mappers and reducers) are assumed to be stateless and to have no externally visible side effects besides their designated output. This restriction allows the framework to hide some of the hard distributed systems problems behind its abstraction: in the face of crashes and network issues, tasks can be retried safely, and the output from any failed tasks is discarded. If several tasks for a partition succeed, only one of them actually makes its output visible.

Thanks to the framework, your code in a batch processing job does not need to worry about implementing fault-tolerance mechanisms: the framework can guarantee that the final output of a job is the same as if no faults had occurred, even though in reality various tasks perhaps had to be retried. These reliable semantics are much stronger than what you usually have in online services that handle user requests and that write to databases as a side effect of processing a request.

The distinguishing feature of a batch processing job is that it reads some input data and produces some output data, without modifying the input — in other words, the output is derived from the input. Crucially, the input data is bounded: it has a known, fixed size (for example, it consists of a set of log files at some point in time, or a snapshot of a database’s contents). Because it is bounded, a job knows when it has finished reading the entire input, and so a job eventually completes when it is done.

In the next chapter, we will turn to stream processing, in which the input is unbounded — that is, you still have a job, but its inputs are never-ending streams of data. In this case, a job is never complete, because at any time there may still be more work coming in. We shall see that stream and batch processing are similar in some respects, but the assumption of unbounded streams also changes a lot about how we build systems.