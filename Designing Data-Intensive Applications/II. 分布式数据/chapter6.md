# 第六章 分区

*显然，我们必须脱离顺序同时不去限制计算机。 我们必须声明定义并提供数据的优先级和描述。 我们必须说明关系，而不是过程。*

葛丽丝·穆雷·霍普, *未来的管理与计算机* (1962)

---

在第五章我们讨论了复制——也就是，在不同节点上有同样数据的多个拷贝。对于非常大的数据集，或者非常高的查询吞吐量，这是不够的：我们需要把数据*分区*，也叫做*分片*。

> 术语上的混乱
>
> 我们这里称之为*parition*的东西在MongoDB、Elasticsearch以及SolrCloud中叫做*shard*；在HBase叫做*region*，在Bigtable中叫做*tablet*，在Cassandra和Riak中叫做vnode，在Couchbase中叫做vBucket。然而，分区时最成熟的名词，所以我们会继续用它。

通常，分区时这样定义的，每条数据（每条记录，每个行或者每个文档）只属于一个分区。有许多方式可以实现这个目标，我们会在本章深入讨论它们。实际上，每个分区都是一个小数据库，虽然数据库支持同时涉及多个分区的操作。

想要对数据分区的主要原因是*可扩展性*。不同分区可以放在无共享集群中的不同的节点上（*无共享*的定义详见第二部分的介绍）。因而，大的数据集分布在多个磁盘，查询负载分布在多个处理器上。

对于在单个分区上操作的查询请求，每个节点可以在自己的分区上独立执行查询请求，所以查询吞吐量可以通过添加更多节点扩展。大型复杂的查询请求可以并行在多个节点上执行，虽然这样会变得非常困难。

分区数据库的先行者是1980年代的产品，比如Teradata和Tandem NonStop SQL，而最近被NoSQL数据库和基于Hadoop的数据仓库重新发现。有些系统为事务性工作量设计，而其它是为了分析（见“事务处理，还是分析？”一节）：这个差别影响了系统如何调优，但是分区的基本原理适用于两种工作负载。

在这一章中我们将首先了解大型数据集分区的不同方式，并观察为数据建立索引是如何与分区互动的。然后我们会聊到再平衡，如果你要添加或移除集群中的节点的话是很必要的。最后我们会概览数据库是如何路由请求到正确的分区而后执行查询请求的。

## 分区与复制

分区通常会结合复制从而每个分区的拷贝存放于数个节点。这意味着，即使每条记录只属于一个分区，为了容错性它仍可能被储存在几个不同的节点上。

一个节点可以储存多个分区。如果使用了领机-从机复制模型，分区与复制结合之后看起来如图6-1一般。每个分区的领机被分配到一个节点，它的从机被分配到其它节点。每个节点可以是某些分区的领机也可以是其它分区的从机。

我们在第五章讨论的所有关于数据库复制的东西同样适用于分区的复制。分区方法的选择很大程度上独立于复制方法的选择，所以我们在本章会忽略复制，让事情保持简单。

*图6-1 结合复制与分区：每个节点对于某些分区来说是领机，对于其它分区来说是从机。*

## 键值对数据的分区

假设你有大量的数据，你想要对它分区。如何决定哪些条目储存在哪些节点上？我们分区的目标是把数据与查询请求负载平分到多个节点上。如果每个节点承担同样的份额，那么——理论上——10个节点应该可以应对10被于单个节点的数据以及读写吞吐量（暂时忽略复制）。

如果分区不是平均分配的，那么某些分区相比其它分区有更多的数据以及查询请求，我们把它叫做*倾斜*。倾斜的出现使得分区效果变差。在极端情况下，所有的负载可能都在一个分区上，以至于10个节点中的9个是空闲的而最忙碌的那个节点成为了瓶颈。有着不成比例的高负载分区叫做*热点*。

避免热点的最简单的方式是随机地分配条目到结点，但是这样做有个大缺点：当你尝试读取特定的条目，你没有办法知道它在哪个节点上，所以你需要并行查询所有节点。

我们可以做得更好。让我们暂时假设你有一个简单的键值对数据模型，你总是通过主键访问条目。举个例子，在过时了的纸质百科全书中，你通过标题查找条目；由于所有的条目是按照标题的字母顺序排序的，你可以很快地找到你要找的那条。

### 按键的范围分区

一种分区的方式是指定一段连续范围的键（从某个最小值到某个最小值）到各个分区，就好像纸质百科全书（图6-2）的分册一样。如果你知道范围的边界，你可以很容易地判断哪个分区包含给定的键。如果你也知道哪个分区被指定到了哪个节点，那么你可以直接向恰当的节点直接发起请求（或者，在百科全书的例子中，从书架上选正确的那本）。

*图6-2 印刷版百科全书是按照键的范围分册的。*

键的范围不必要平均间隔开，因为你的数据不一定是平均分布的。举个例子，在图6-2中，第一册包含了以A和B开头的词，但是第12册包含了以T、U、V、X、Y和Z开头的词。如果每一册都只覆盖两个字母的话将导致某些册比其它一些要大很多。为了使数据均匀的分布，分区的范围需要适应数据本身。

分区边界可以是管理员手动选的，也可以数据自动选择的（我们会在“再平衡分区”一节详细讨论分区边界的选择）。这种分区策略用在了Bigtable，它的开源版本HBase，RethinkDB以及2.4版本前的MongoDB。

在每个分区中，我们可以把键按排好的顺序保存（见“SSTable与LSM树”）。这样做使得范围扫描变得容易，并且你可以把键当作连接后的索引从而在一个查询请求中获取多个相关的条目（见“多列索引”）。举个例子，设想一个储存传感器网络数据的应用，它的键是测量（*年-月-日-时-分-秒*）的时间戳。范围扫描在这种情况下特别有用，因为它让你很容易获得，比如，特定月份的所有读数。

然而，按键的范围分区的缺点是特定访问的模式会导致热点出现。如果键是时间戳，那么分区对应了时间范围——比如，每天对应一个分区。不幸的是，当测量发生时我们把来自传感器的数据写入数据库，所有写入最终都进入到了同一个分区（对应今天的那个），所以那个分区会被写入请求过载，而其它分区都闲置了。

为了避免传感器数据库里的这个问题，你需要使用时间戳意外的东西作为键的首元素。举个例子，你可以为每个时间戳加上传感器名称前缀，这样分区首先按照传感器名称再是时间。假设同时有许多活动的传感器，写入负载最终会更平均地分布在许多分区上。现在，当你需要获取某个时间段内多个传感器的值，你需要执行对每个传感器名称执行独立的范围查询。

### 按键的哈希分区

由于倾斜和热点的风险，许多风不是数据存储使用哈希函数来决定给定键的分区。

一个好的哈希函数接受倾斜的数据并使它均匀分布。假设你有个返回值为32位的接受字符串的哈希函数。每当你给它一个新字符串，它返回一个看似随机的，在0到2<sup>32</sup> - 1之间的数字。哪怕输入的字符串非常类似，它们的哈希值还是在数字范围内平均分布。

出于分区的目的，哈希函数没有必要是加密级别的强度：举个例子，Cassandra与MongoDB用的是MD5，而Voldemort用的是Fowler-Noll-Vo函数。许多编程语言都有内奸的简单哈希算法（因为被用在哈希表上），但是它们不太适合用来分区：举个例子，在Java的`Object.hashcode()`以及Ruby的`Object#hash`中，同样的键在不同的进程中有着不同的哈希值。

一旦你有了合适的用作键的哈希函数，你可以指定每个分区的哈希范围（而不是键的范围），哈希落在分区范围内的键将被存在那个分区里。如图6-3所示。

*图6-3 按键的哈希分区*

这个技巧可以很好地把键公平地分布到不同分区。分区边界可以均匀间隔的，或者可以假随机地选择（在这种情况下这个技巧被叫做*一致哈希*）

> 一致哈希
>
> 一致哈希，由戴维·卡尔热等人定义，是一种在互联网范围内的缓存系统——比如内容分发网络（CDN）——中均匀分布负载的方法。它使用随机选择的分区范围来避免对集中控制或者是分布式共识的需要。值得注意的是一致在这里与复制的一致性（见第五章）或是ACID一致性（见第七章）无关，它只是描述了再平衡的一种特别方式。
>
> 如我们即将在“再平衡分区”一节看到的，这种特别方式实际上对数据来说工作得并不是很好，所以事件中很少用到（一些数据库的说明文档仍然引用了一致哈希，但是通常并不准确）。因为这太令人困惑了，最好还是避免术语*一致哈希*而只是把它叫做哈希分区。

然而不幸的是，使用以键的哈希分区我们丢掉了以键的范围分区的一个良好特性：进行高效范围查询的能力。先前相邻的键现在分散到了所有分区里，于是排序次序丢失。在MongoDB中，如果你启用了基于哈希的分片模式，任何范围查询都必须发送到所有分区。Riak、Couchbase和Voldemort根本不支持主键上的范围查询。

Cassandra采取了两种分区策略的折中方案。Cassandra中的表声明时可以带上由数个列组成的复合主键。只有键的首部分哈希后用来决定分区，而其它列被用作Cassandra SSTable中数据排序的连接索引。因此查询请求不能搜索符合键首列中的一些列值，但是如果它为首列指明了一个固定值，他可以在键的其它列执行高效地范围扫描。

连接索引的方法使我们可以为一对多关系使用一种优雅的数据模型。举个例子，在社交媒体网站上，用户可以发布许多状态更新。如果更新的主键选的是`(user_id, update_timestamp)`，那么你可以高效地获取特定用户某个时间段内所有的状态更新，按时间戳排序。不同的用户可以储存在不同的分区，但是对于同一个用户，状态更新按照时间戳排序保存在单个分区上。

### 倾斜的工作负载与热点缓解

如上所述，以键的哈希来判定它的分区可以帮助减少热点。然而，它不能完全的避免特点：在极端情况下，所有的读写都是针对同一个键的，最终所有的请求还是被发送到了同一个分区。

这一类工作负载也许是不常见的，但是不是没有听说过：举个例子，在社交媒体网站上，当拥有数百万粉丝的名人用户做了些什么的时候可以导致一场活动风暴。这个事件可以引起大量对同一个键的写入请求（键也许是这个名人的用户ID，或者是人们评论的动作的ID）。取键的哈希没有帮助，因为两个同样ID的哈希仍然是一样的。

今天，绝大部分数据系统都无法为这样极度倾斜的工作负载自动补偿，所以降低倾斜是应用程序的职责。举个例子，如果一个键已知很热了，一个简单的技巧是添加一个随机数到键的开始或者末尾。只有两个数字的十进制随机数把对这个键的写入请求分到了100个不同的键上，使得这些键分不到了不同的分区。

然而，把写入分到了不同的键，任何读取请求现在需要做更多的工作了，因为它们必须读取所有100个键的数据并结合起来。这个技巧也需要更多的记录，对少数热键添加随机数字才有意义；对于大多数有着很低写入吞吐量的键这样做会产生没有必要的消耗。因而，你也需要某种记录哪些键被分开的方式。

也许在未来，数据系统可以自动检测并补偿倾斜的工作负载；但现在，你需要为你自己的应用程序仔细权衡。

## 分区与二级索引

我们目前讨论了的分区方法依赖键值对数据模型。如果记录只用主键访问过，我们可以从那个键判定分区，并且用它把读取和写入请求发送到键所对应的分区。

如果牵扯到二级索引的话情况就变得更复杂了（见“其它索引结构”）。二级索引通常不用来唯一标识一条记录，而只是一种搜索特定值的方式：找出用户123所有的操作，找到所有包含词语`hogwash`的文章，找出所有红色的车，等等等等。

二级索引是关系型数据库的核心产品，它在文档型数据库中也很常见。许多键值对存储（比如HBase与Voldemort）不支持二级索引是因为它增加了实现的复杂度，但是有些（比如Riak）开始支持它，因为它对于数据模型很有用。最后，二级索引是Solr和Elasticsearch等搜索服务器的*存在理由*。

二级索引的问题在于它们不能整齐地映射到分区。用二级索引对数据库分区有两种主要方式：基于文档分区与基于术语分区。

### 根据文档对二级索引分区

举个例子，设想你在运营一个售卖二手车的网站（如图6-4所示）。每个清单有一个唯一ID——叫它*文档ID*吧——然后你用文档ID把数据库分区了（举个例子，从0到499的ID在分区0，从500到999的ID在分区1，以此类推）。

你想让用户通过搜索找车，允许他们通过颜色和厂商过滤，所以你需要`color`和`make`的二级索引（在文档型数据库中这些可以是字段；在关系型数据库中这些是列）。如果你声明了索引，数据库可以自动建立索引。举个例子，每当一台红色的车被录入数据库，数据库分区自动把它添加到索引条目`color:red`的文档ID列表中。

*图6-4 根据文档对二级索引分区*

在这种索引方法中，每个分区是完全独立的：每个分区维护自己的二级索引，只覆盖分区中的文档。它不关心在其它分区储存了什么数据。每当需要写入数据库时——添加、删除或更新文档——你只需要处理包含着你在写入的文档ID的分区。由于这个原因，文档分区的索引也被称为*本地索引*（与下一节将描述的*全局索引*相对应）。

然而，从文档分区的索引读取需要很小心：除非对文档ID做了特殊处理，那么没有理由把所有具有特定颜色或特定品牌的车都会在同一个分区中。在图6-4中，红色的车出现在了分区0和分区1。因而，如果你要搜索红色的车，你需要发送查询请求到所有分区，然后把返回的所有结果结合起来。

这种查询分了区的数据库的方法叫做*分散/收集*，而这使得向二级索引发起读取查询的代价非常高。即使并发查询分区，分散/收集很容易出现尾部延迟放大效应（详见“实践中的百分位”）。然而，它被广泛地使用着：MongoDB、Riak、Cassandra、Elasticsearch、SolrCloud以及VoltDB全都使用文档分了区的二级索引。大多数数据库厂商推荐你构建自己的分区方法于是二级索引查询可以由单个分区响应，但是这部总是可能的，尤其是在你在单个查询中使用了多个二级索引（比如说同时按颜色和厂商过滤车）。

*图6-5 根据术语对二级索引分区*

### 根据字词对二级索引分区

与其每个分区有自己的二级索引（*本地索引*），我们可以构建覆盖所有分区数据的*全局索引*。然而，我们不能把这个索引只存在一个节点上，因为这很有可能变成平静进而失去了分区的意义。全局索引也必须被分区，但是可以与主键索引的分区不同。

图6-5展示了这大概会是什么样子：来自所有分区的红色的车都出现在索引的`color:red`下边，但是索引也被分区了，于是以字母*a*到*r*的开头的颜色出现在分区0，以字母*s*到*z*开头的颜色出现在分区1。汽车厂商的索引也是类似分区的（分区边界为*f*和*h*）。

我们把这种索引叫做*按字词分区*的索引，因为我们寻找的字词决定了索引的分区。在这个例子中，字词是`color:red`。*字词*这个名字来源于全文索引（一种特别的二级索引），在这里字词就是所有文章中出现过的词。

跟先前一样，我们可以按字词对索引分区，或者使用字词的哈希值。按字词本身分区在范围扫描（比如，对一个数字属性，比如询问车价）时很有用，而按字词的哈希值分区可以得到更平均分配的负载。

相比于按文档分区的索引，全局（按字词分区）索引的优点是它让读取更有效率：与其在所有分区上做分散/收集，客户端只需要向包含所需字词的分区发起请求。然而，全局索引的缺点在于写入更慢更复杂了，因为对单个文档的写入请求现在会影响索引的多个分区（文档中的每一个字词都会在不同的分区，不同的节点上）。

在理想的世界里，索引总是最新的，每个写入数据库的文档会立即反映在索引中。然而，在按字词分区的索引中，这需要一个横跨所有受写入影响的分区的分布式的事务，但是并不是所有数据库都支持它（见第七与第九章）。

在实践中，对全局二级索引的更新经常是异步的（那就是，如果你在写入之后没多久读取，你刚刚做出的变更不一定会体现在索引中）。举个例子，亚马逊的DynamoDB声明它的全局二级索引正常情况下会在几分之一秒钟内更新，但是如果基础架构发生故障，就很可能遇到更长的传播延迟。

全局按字词分区的索引的其它用途包括了Riak的搜索功能以及Oracle的数据仓库，它允许你在本地与全局索引之间选择。我们会在第十二章回到如何实现按字词分区的二级索引这个主题。

## 再平衡分区

随着时间的推移，数据库中的东西发生变化：

* 查询吞吐量增加，所以你要添加更多的CPU来应对负载。

* 数据集大小增加，所以你要添加更多的磁盘和内存来储存它。

* 设备故障，而其它设备需要接管故障设备的职责。

所以这些变化都需要把数据和请求从一个节点转移到另一个节点。把负载从集群中的一个系欸按移到另一个的过程叫做*再平衡*。

不管使用的是哪种分区方法，再平衡通常都要求满足某些最低标准：

* 再平衡之后，负载（数据存储，读写请求）应当在集群中的节点之间平均分配。

* 当再平衡进行时，数据库应当继续接受读写请求。

* 只有必须移动的数据才会在节点之间转移，从而使得再平衡过程很快并且只占用最少的带宽与磁盘I/O负载。

### 再平衡的策略

There are a few different ways of assigning partitions to nodes [23]. Let’s briefly discuss each in turn.

#### 如何不这样做: 哈希值余N

当按照键的哈希值分区时，我们之前说过（图6-3）最好把可能的哈希值分成不同的范围，并把每个范围指定到一个分区（比如，如果0 ≤ *hash(key)* < b<sub>0</sub>就把键指定到分区0，如果b<sub>0</sub> ≤ *hash(key)* < b<sub>1</sub>就指定到分区1，以此类推）。

也许你回想为什么不用*取余*（许多编程语言中的%操作符）。举个例子，*hash(key) mod* 10会返回会返回0到9之间的数字（如果我们把哈希值写作十进制的数字，那么哈希值取10的*余数*就是最后一位数字）。如果我们有10个节点，编号0到9，这看起来是一种指定键到节点的简单方法。

*取N余数*的方式问题在于如果节点数N发生变化，大多数的键需要从一个节点转移到另外一个节点。举个例子，假如*hash(key)* = 123456。如果一开始有10个节点，那么键最初在节点6（因为对123456取10的余数是6）。当增加到11个节点的时候，键需要转移到节点3（对123456取11的余数是3），当增加到12个节点的时候，键需要转移到节点0（对123456取12的余数是0）。如此频繁地迁移使得再平衡代价相当得高。

我们需要一种只在必要时移动数据地方法。

#### 固定数量的分区

幸运的是，有一个相当简单的解决方案：创建比节点数多很多的分区，并指定数个分区到每个节点。举个例子，一个运行在拥有10个节点的集群上的数据库从一开始就被拆分成1000个分区，于是大约100个分区被指定到了各个节点上。

现在，如果一个节点被添加到急群众，新节点可以从每个已有节点上偷一些分区过来，直到分区再一次平均分配为止。这个过程如图6-6所示。如果一个节点从集群中删除，同样的事情会反着发生。

只有整个分区在节点之间转移。分区的数量不会变化，键所指定的分区也不会。唯一会变得是分区指定的节点。这种指定关系的变化不会立即生效——通过网络传输大量数据需要花些时间——所以旧的指定分区用来服务传输过程中任何读写请求。

*图6-6 添加新节点到每个节点有多个分区的数据库集群*

原则上，你甚至可以说明集群中为什么有不匹配的硬件：通过指定更多的分区到性能更强大的节点，你可以强制这些节点承担更多的负载。

这种再平衡方式用在了Riak、Elasticsearch、Couchbase以及Voldemort。

在这种配置环境中，分区的数量通常在数据库建立时就固定了，之后也不会改变。

虽然原则上拆分合并分区是可能的（见下一节），然而固定数目的分区运营时更简单，于是许多固定分区的数据库选择不实现分区拆分。因而，在一开始配置的分区数量就是你可以拥有的最大节点数，所以你需要选择足够高的数字来适应未来的增长。然而，每个分区还有管理消耗，所以数字选得太高也会适得其反。

如果数据集的总大小是非常动态的话（比如，开始很小但是随着时间推移变得非常大），选择正确的分区数量很困难。因为每个分区都包含了所有数据固定的一小部分，每个分区的大小与集群中的数据总量成比例增长。如果分区非常大，再平衡以及节点崩溃的恢复都会变得代价很高。但是如果分区太小，也会导致大量的消耗。最好的性能实在分区大小“刚刚好”的时候实现的，既不会太大也不会太小，如果分区的数量固定但是数据集大小变化的话这是很难实现的。

#### 动态分区

对于使用按键的范围分区（见“按键的范围分区”）的数据库，固定数量的固定边界分区会非常的不方便：如果边界错了，最终有可能所有数据在一个分区而其它分区都是空的。手动重新配置分区边界又会非常枯燥。

由于这个原因，按键的范围分区的数据库，比如HBase和RethinkDB，会动态创建分区。当一个分区的大小超过了设定值（在HBase上，默认大小是10GB），它被拆分为两个分区从而各大约一半的数据在新的分区内。相反地，如果大量数据被删除而分区缩小到低于某个阙值，他会并入相邻地分区。这个过程与B树顶层发生的事类似（见“B树”一节）。

每个分区被指定到一个节点，而每个节点可以处理多个分区，与固定数量分区的案例类似。大分区被拆分之后，其中的一半可以被转移到另一个节点以平衡负载。在HBase的案例中，分区文件的传输通过HDFS，底层的分布式文件系统进行的。

动态分区的优点是分区的数量适应总数据量。如果只有很少量的数据，那么很小数量的分区就足够了，消耗也很小；如果有海量的数据，每个分区的大小被限制为可配置的最大值。

然而，需要注意的是空数据库最初只有一个分区，因为没有关于在哪里划分分区边界的*先验*信息。当数据集很小的时候——直到它足够大导致第一个分区被拆分——所有的写入请求需要被单个节点处理，同时其它节点是闲置的。为了缓解这个问题，HBase与MongoDB允许在空数据库中配置初始分区集合（这叫做*预拆分*）。在按键的范围分区的场景中，预分区要求你预先知道之后键分布的情形。

动态分区不止适用于按键的范围分区的数据，也同样使用于按哈希值分区的数据。MongoDB自2.4版同时支持按键的范围分区以及按哈希值分区，两种状况下都会动态地拆分分区。

#### 按节点的比例分区

对于动态分区来说，分区数量与数据集的大小成比例，因为拆分与合并过程将每个分区的大小保持在某个固定的大小之间。另一方面，对于固定数目的分区来说，每个分区的大小与数据集的大小成比例。在两种情况下，分区的数量与节点数量无关。

第三种选择，Cassandra与Ketama在用的，是让分区的数量与节点的数量成比例——换句话说，每个节点有固定数量的分区。在这种情况下，每个分区大小增长的速度与数据集的大小成比例而节点数量不变，但是当你增加节点数量时，分区再一次变小了。由于更大的数据量一般需要更多的节点来存储，所以这种方式也将每个分区的大小保持稳定。

当新节点加入集群时，它随机地选区固定数量的已有分区进行拆分，之后获取一半新分区的控制权而剩下的一半原封不动。随机过程有可能产生不公平的拆分结果，但是当平均发生在大量分区上（在Cassandra，默认每个节点有256个分区）时，新节点最终从已有节点上获得公平份额的负载。Cassandra 3.0版本还引入了另一种再平衡算法防止不平衡的拆分。

随机选择分区边界需要使用基于哈希的分区方法（于是边界可以选择由哈希函数生成的数字的范围）。确实，这种方式最符合一致哈希的最初定义（见“一致哈希”一节）。更新的哈希函数可以再更低的元数据消耗下实现类似的效果。

### 运营：自动还是手动再平衡

考虑再平衡的时候有一个很重要的问题还没有考虑：再平衡是自动发生的还是手动发生的？

完全自动再平衡（系统在没有任何管理员参与的情况下自动决定什么时候把分区从一个节点转移到另一个节点）与完全手动再平衡（分区位于哪个节点的指定是明确由管理员配置的，并且只在管理员明确再配置时才会变化）之间是有渐变的。举个例子，Couchbase、Riak以及Voldemort自动生成建议的分区分配，但是需要管理员提及之后才生效。

完全自动的再平衡可以很方便，因为对于正常的维护来说少了许多运营工作。然而，它可能很难预测。再平衡是一个代价高昂的操作，因为他需要重新路由请求并且在节点之间转移大量数据。如果不仔细做，这个过程会过载网络或者节点，从而在再平衡过程中损害其它请求的性能。

这种自动化过程与自动故障检测结合起来会很危险。举个例子，假设一个节点过载了，暂时对请求响应地很慢。其它节点认为过载节点已经崩溃了，从而自动再平衡了整个集群从而把负载移走了。这会给过载的节点，其他节点以及网络带来额外负担——使得整个情况更糟糕，甚至导致级联故障。

由于这个原因，再平衡过程中引入人的参与是一件好事。相比于完全自动的过程它会更慢，但是这可以预防运营的意外问题。

## 请求的路由

现在我们把我们的数据集分区到了多台设备上的数个节点上了。但是还有一个悬而未决的问题：当客户端要发起请求时，它怎么知道要连接哪个节点？由于分区被再平衡了，分区到节点的分配也会变。需要有人知道所有这些变化来回答这个问题：如果我要读写键“foo”，我需要连接哪个IP地址与端口？

这是更一般性问题*服务发现*的一个例子，并不只限于数据库。任何一个可以从网络访问的软件都有这个问题，尤其是当它针对高可用性（在多台机器上运行的冗余配置）的时候。许多公司都写了他们自己的内部服务发现工具，其中许多都已经开源了。

在高级层面上，这个问题有几种不同的解决办法（如图6-7所示）：

1. 允许客户端访问任意节点（比如，通过循环负载均衡器）。如果节点碰巧拥有请求应用的分区，它可以直接处理请求；否则，它把请求转发到对应的节点，接受应答然后传递给客户端。

2. 首先发送所有来自客户端的请求到路由层，它判断哪个节点应该处理哪个请求并相应地转发出去。路由层本身不处理任何请求，它只是一个知道分区信息的负载均衡器。

3. 要求客户端知道分区信息以及分区到节点的分配信息。在这种情况下，客户端可以直接连接到对应的节点，不需要任何中间层。

在所有的情况下，核心问题都是：做出路由选择的组件时如何了解分区到节点分配信息的变化的？

*图6-7 发送请求到正确节点的三种不同方式。*

这是个很有挑战的问题，因为所有的参与者的同意是很重要的——否则请求被发送到错误的节点也不能正确地处理。有协议可以达成在分布式系统中的共识，但是它们很难正确地实现（见第九章）。

许多分布式数据系统依赖一个独立的协调服务，比如ZooKeeper，来追踪集群信息，如图6-8所示。每个节点在ZooKeeper中注册自己，然后ZooKeeper维护权威的分区到节点的映射关系。其它参与者，比如路由层或者是知道分区信息的客户端，可以在ZooKeeper中订阅这个信息。每当分区变更了拥有者，或者节点被添加或是删除，ZooKeeper通知路由层从而保证路由信息是最新的。

*图6-8 使用ZooKeeper来追踪分区到节点的分配关系。*

举个例子，领英的Espresso系统使用Helix做集群管理（而它又依赖于ZooKeeper），它实现了如6-8所示的路由层。HBase、SolrCloud以及Kafka也使用ZooKeeper跟踪分区分配信息。MongoDB有更简单的架构，但是它依赖自己的*配置服务器*实现以及*mongos*守护进程作为作为路由层。

Cassandra和Riak走了另外一条路：它们在节点之间使用*流言协议*来传播集群状态中任何的变化。请求可以被发送到任意节点，而后那个节点把它们转发到请求的分区的对应节点（图6-7中的方法1）.这种模型使得数据库节点更复杂，但是避免了对外部协调服务，比如ZooKeeper的依赖。

Couchbase不会自动再平衡，这简化了设计。通常它会配置一个路由层叫做*moxi*，它从集群中的节点了解路由的变化。

当使用了路由层或者发送请求到了随机节点，客户端仍然需要找出需要连接的IP地址。相比于分区到节点的分配它们不会变得那么频繁，所以一般用DNS就足够了。

### 并行查询的执行

截至目前我们聚焦于非常简单的只读写单个键的查询请求（对于按文档分区的二级索引来说还要加上分散/收集查询请求）。这与大多数NoSQL分布式数据存储所支持的访问级别有关。

然而，大规模并行处理（MPP）关系型数据库，经常被用来作分析用，在它们支持的查询类型中更加复杂。一个典型的数据仓库查询包括了数次连接，过滤，分组以及聚合操作。MPP查询优化器把复杂查询拆成了数个执行阶段和分区，其中许多可以并行执行在数据库集权的不同节点上。涉及到扫描大部分数据集的查询请求从这样的并行执行受益特别多。

数据仓库查询的快速并行执行是一类专门的话题，由于分析对于商业的重要性，它收到了许多商业性的关注。我们会在第十章讨论一些并行查询的执行的技巧。关于用在并行数据库中更详细的技巧概览，请参见参考【1，33】。

## 总结

在这一章里我们探索了把大数据集分区成小子集的几种不同方式。当你有太多的数据以至于在单台设备上储存和处理都不再可行的时候，分区就是必要的了。

分区的目标是把数据和查询负载平均分布到多个设备上，避免出现热点（有着不成比例的高负载节点）。这要求选择一种适合你的数据的分区方法，而当集群中的节点被添加或是删除的时候再平衡分区。

我们讨论了两种主要的分区方式：

*按键的范围分区*，在这里键被排序，一个分区拥有从某个最小值到某个最大值之间所有的键。排序的优势在于高效的范围查询是可能的，但是如果程序经常访问彼此排序之后接近的键那么就有出现热点的风险。在这种方式中，当分区变得太大的时候通过拆分范围为两个子范围从而分区被动态地再平衡了。

*按哈希值分区*，在这里哈希函数被应用到每一个键，每个分区拥有一系列的哈希。这个方法破坏了键的顺序，使得范围查询没有效率，但是可以负载可以分布的更平均。当通过哈希分区，一般事先会创建固定数据的分区，并指定数个分区到每个节点，当节点添加或删除时还要把整个分区从一个节点转移到另外一个。也可以用动态分区方法。

混合的方法也是可能的，举个复合键例子：可以用键的一部分标记分区，另一部分用来排序。

我们还讨论了分区与二级索引的互动。二级索引也需要分区，有两种方法：

按文档分区的索引（本地索引），它把二级索引如主键和值一样存储在同一个分区。这意味着写入时只有单个分区需要更新，但是读取二级索引就需要横跨所有的分区进行分散/收集。

按字词分区的索引（全局索引），它把二级索引单独分区，用索引的值。二级索引中的一个条目包含来自主键的所有分区的记录。当文档被写入时，数个二级索引的分区需要更新；然而，一个读取请求可以由单个分区响应。

最后，我们讨论了路由请求到对应分区的技巧，从简单的知道分区信息的负载平衡到复杂的并行查询执行引擎。

根据设计，每个分区大部分时候时独立运行的——这使得分了区的数据库可以扩展到多台设备。然而，需要写入多个分区的操作难以推理：举个例子，如果写入到一个分区成功，但是另一个失败了会发生什么？我们会在接下来的章节解决这个问题。