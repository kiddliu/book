# 第九章 一致性与共识

*活着但是是错的与正确但是死掉了，哪个更好？*

杰·克莱普斯，*关于Kafka与Jepsen的几个注意事项*（2013）

---

正如第8章所讨论的，分布式系统中许多事情会出错。处理这些故障最简单方法就是直接让整个服务失效，并向用户显示错误消息。如果这个解决方案不可接受，我们需要找到容错的方法——也就是即使某些内部组件出现故障，也可以保持服务正常运行。

在这一章里，我们将讨论构建可容错分布式系统算法与协议的一些例子。我们将假设第8章中的所有问题都会发生：网络中的数据包会丢失，会被重新排序，会重复或任意地延迟; 时钟最多是近似的; 并且节点可以暂停（例如，由于垃圾收集）或者随时崩溃。

构建容错系统的最佳方式是找到一些具有有用保证的通用的抽象概念，实现它们，然后让应用程序依赖这些保证。 这与我们在第7章中用于事务的方法相同：通过使用事务，应用程序可以假装没有崩溃（原子性），没有其他人同时访问数据库（隔离性），并且存储设备是完全可以依赖的（耐用性）。即使发生崩溃，竞态条件和磁盘故障确实会发生，但是事务抽象隐藏了这些问题所以应用程序不需要担心它们。

我们现在将继续沿着同样的路线前进，尝试寻找能够让应用程序忽略分布式系统部分问题的抽象概念。例如，分布式系统最重要的抽象之一就是共识：也就是说，让所有的节点都能达成一致。我们将在本章中看到，尽管存在网络故障和进程故障，可靠地达成一致是个令人惊讶的棘手问题。

一旦实现了协商一致，应用程序可以将其用于各种目的。例如，假设你有一个单主机复制的数据库。如果主机失效而你需要故障迁移到另一个节点，那么其余的数据库节点可以使用协商一致来选出新的主机。正如在“处理节点中断”中所讨论的，重要的是只有一个主机，并且所有节点都同意谁是主机。如果两个节点都认为自己是主机，那么这种情况就叫做裂脑，并且常常导致数据丢失。正确实现协商一致才可以避免此类问题。

在本章的后面，我们将在“分布式事务与协商一致”一节中研究解决协商一致以及相关问题的算法。但是首先，我们需要探索在分布式系统中可以提供的各种保证和抽象概念。

我们需要了解能做什么不能做什么的范围：在某些情况下，系统是可以容忍错误并继续工作的；在其他情况下，这就不可能了。在理论证明和实际实现中已经深入探讨了什么可能什么不可能的极限。我们将在这一章里概述这些基本限制。

分布式系统领域的研究人员几十年来一直在研究这些问题，因此有大量的材料——我们只会接触一些表面问题。在这本书中，我们没有空间去深入到正式模型与证明的细节，所以我们将坚持非正式的直觉。如果你感兴趣的话，这些参考文献提供了足够多的深度讨论。

## 一致性保证

在“复制延迟问题”一节中，我们研究了复制了的数据库中出现的一些计时问题。如果同时查看两个数据库节点，你很可能会在两个节点上看到不同的数据，因为写入请求在不同的时间到达不同的节点。无论数据库使用哪种复制方法（单主机、多主机或无主机复制），都会发生这些不一致的情况。

大多数复制了的数据库至少提供了最终一致性，这意味着如果停止对数据库的写入并等待一段时间，那么最终所有读取请求都会返回相同的值。换句话说，不一致是暂时的，最终是可以自己解决的（假设任何的网络故障也最终被修复了）。最终一致性的一个更好的名称可能是收敛，因为我们期望所有的副本最终收敛到相同的值。

然而，这是一个很弱的保证——它没有提到副本什么时候会收敛。在收敛之前，读取可以返回任何内容或什么都不返回。例如，如果写入一个值然后马上读取它，则无法保证你会看到刚才写入的值，因为读取请求可能被路由到另一个副本（见“读取你自己的写入”一节）。

对于应用程序开发人员来说最终一致性是很难，因为它与普通单线程程序中变量的行为非常不同。如果给变量赋值然后立即读取它，你不会期望读到旧值，或者读取失败。数据库表面上看起来像一个可以读写的变量，但实际上它有更复杂的语义。

当使用只提供弱保证的数据库时，你需要一直意识到它的局限性，而不是意外地假设太多。Bug通常很微妙，很难通过测试找到，因为大部分时间应用程序都能正常工作。最终一致性的边缘情况只有在系统中出现故障（例如网络中断）或高并发时才会变得明显。

在这一章里我们将探讨数据系统会选择提供的更强的一致性模型。它们并不是免费的：与有较弱保证的系统相比，具有更强保证的系统性能会更差，容错性会更低。尽管如此，因为更方便正确地使用，更有力的担保是有吸引力的。一旦你了解了几个不同的一致性模型，你就可以更好地决定哪一个最适合你的需要。

分布式一致性模型与我们前面讨论过的事务隔离级别的层次结构有一些相似之处（见“弱隔离级别”一节）。但是虽然有一些重叠之处，但它们绝大部分是独立的：事务隔离主要是为了避免由于并发执行事务而产生的竞争条件，而分布式一致性则主要是在遇到延迟和错误时协调副本的状态。

这一章涵盖了广泛的主题，但正如我们将看到的，这些领域之间实际上有着深刻的联系：

* 我们会首先研究其中一个最常用的最强一致性模型，线性化模式，并研究其利与弊。

* 然后我们将研究分布式系统中的事件排序问题（“排序保证”），特别是围绕因果关系和完全排序的问题。

* 在第三部分（“分布式事务和协商一致”）中，我们将探讨如何原子性地提交分布式事务，这将最终为协商一致问题找到解决方案。

## 线性化

在最终一致的数据库中，如果同时问两个不同的副本相同的问题，你可能会得到两个不同的答案。这太让人困惑了。如果数据库能给人一种错觉，以为只有一个副本（即只有一个数据副本），那岂不是简单得多吗？这样，每个客户端都将拥有相同的数据视图，而不必担心复制滞后。

这就是线性化（也称为*原子一致性*[7]、*强一致性*、*即时一致性*或*外部一致性*）背后的思想。线性化的确切定义是非常微妙的，我们会在这一节剩下的部分中对其进行探讨。但是基本的思想是让系统看起来好像只有一个数据的副本，并且对它的所有操作都是原子的。有了这个保证，即使现实中会有多个副本，应用程序也不需要担心它们。

在一个可线性化的系统中，一旦一个客户端成功地完成了一次写入，所有正在从数据库读取的客户端都必须能够看到刚刚写入的值。保持单个数据副本的错觉意味着确保读取的值是最近更新的值，而不是来自陈旧的缓存或副本。换句话说，线性化是新近性的保证。为了澄清这个想法，让我们看一个非线性化的系统的例子。

*图9-1 系统是非线性化的，这让球迷非常的困惑*

图9-1展示了一个非线性体育网站的例子。爱丽丝和鲍勃坐在同一个房间里，两人都在手机上查看2014年世界杯决赛的结果。最后的比分刚刚公布以后，爱丽丝刷新了页面，看到宣布了获胜者，兴奋地告诉了鲍勃。鲍勃疑惑地点击了手机上的重新加载按钮，然而他的请求转到了一个滞后的数据库副本，于是他的手机显示比赛仍在进行。

如果爱丽丝和鲍勃同时点击重新加载按钮，得到两个不同的查询结果就不会那么令人惊讶了，因为他们不知道服务器处理各自请求的确切时间。然而，鲍勃知道他在听到爱丽丝惊呼最后的分数后按了重新加载按钮（启动了他的查询），因此他希望他的查询结果至少和爱丽丝的一样新。他的查询返回了一个旧结果的事实违反了线性化。

### 是什么让系统线性化的？

线性化背后的基本思想很简单：使系统看起来好像只有一个数据副本。然而，准确地确定这意味着什么，实际上要很小心。为了更好地理解线性化，让我们看更多的例子。

图9-2展示了三个客户端同时在线性化数据库中读写相同的键*x*。在分布式系统文献中，*x*被称为*寄存器*——在实践中，它可以是键值对存储中的一个键，关系型数据库中的一行，或者文档型数据库中的一个文档。

*图9-2 如果读取请求与写入请求并发，它要么返回新值，要么是旧值。*

为了简单起见，图9-2只展示了客户端角度的请求，而不是数据库的内部。每个条形框是一个客户端发出的请求，条形框的起始位置是发送请求的时间，终止位置是客户端接收响应的时间。由于可变的网络延迟，客户端不知道数据库什么时候会处理它的请求——它只知道发生的时间一定在客户端发送请求之后与接收响应之间。

在本例中，寄存器有两种类型的操作：

* *read(x) ⇒ v*表示客户端请求读取寄存器*x*的值，数据库返回了值*v*。

* *write(x, v) ⇒ r*表示客户端请求将寄存器*x*设置为值*v*，数据库返回响应*r*（可以是*正常*或*错误*）。

在图9-2中，x的值初始为0，客户端C执行写入请求将其设置为1。当这种情况发生时，客户端A和B在反复轮询数据库以读取最新值。A和B在它们的读取请求中可能得到什么响应？

* 客户端A的第一个读取操作在写入开始之前完成，因此它必然返回旧值0。

* 客户端A的最后一次读取是在写入完成后开始的，因此，如果数据库是可线性的，则必须返回新的值1：我们知道，数据库写入必然是在写入操作开始和结束之间的某个时间进行的，读取数据库必然是在读取操作开始和结束之间的某个时间进行的。如果读取是在写入结束后开始的，那么读取的处理必然是在写入之后的，因此它必然看到写入的新值。

* 任何与写入操作重叠的读取操作都可能返回0或1，因为我们不知道在处理读取操作时写入是否已经生效。这些操作与写入是并行的。

但是，这还不足以充分地描述线性化：如果与写入并发的读取操作可以返回旧值也可以返回新值，那么读者在写入过程中可以多次看到旧值和新值之间的来回翻转。这不是我们所期望的、可以模拟“单一数据副本”的系统。

为了使系统可线性化，我们需要添加另一个约束，如图9-3所示。

*图9-3 在任何一个读取操作返回新值之后，之后所有（在相同或其他客户端上的）读取操作也必须返回新值。*

在线性化系统中我们设想必然有某个时间点（在写入操作的开始和结束之间）*x*的值从0原子翻转为1。因此，如果一个客户端的读取操作返回了新的值1，那么所有后续读取操作也必须返回新的值，即使写入操作还没有完成。

图9-3中的箭头展示了这种时间依赖性。客户端A是第一个读到新值的，1。在A的读取请求返回之后，B开始了新的读取操作。由于B的读取操作严格地发生在A的读取操作之后，所以它也必须返回1，即使C的写入仍在进行之中。（这与图9-1中的爱丽丝和鲍勃的情况相同：爱丽丝读取了新值之后，鲍勃也希望读取到新值。）

我们可以进一步细化这个时间图，从而可视化每个发生在某个时间点的，原子性生效的操作。图9-4显示了一个更复杂的例子。

在图9-4中我们添加了读取与写入之外的第三种操作：

* *cas(x, v<sub>old</sub>, v<sub>new</sub>） ⇒ r*意味着客户端请求了原子性的比较后设置操作（见“比较后设置”一节）。如果寄存器x的当前值等于*v<sub>old</sub>*，则应该原子地将其设置为*v<sub>new</sub>*。如果*x ≠ v<sub>old</sub>*，那么操作应该保持寄存器不变，并返回一个错误。*r*是数据库的响应（*正常*或*错误*）。

在我们认为操作被执行的时侯，图9-4中的每个操作都用垂直线（每个操作的条形框内部）标记。这些标记按顺序连接，结果对于寄存器来说必须是有效的读写顺序（每个读取操作必须返回由最近一次写入设置的值）。

线性化的要求是连接操作标记的线总是在时间上向前移动（从左到右），而不是向后移动。这个要求确保了我们前面讨论过的新近性保证：一旦一个新的值被写入或读取，所有后续的读取都会看到被写入的值，直到它再次被覆写。

*图9-4 可视化读写生效的时间点。B的最终读取操作不是线性化的。*

图9-4中一些有趣的细节需要指出：

* 首先客户端B发送读取*x*的请求，然后客户端D发送将*x*设置为0的请求，然后客户端A发送将x设置为1的请求。然而，返回给B的读值是1（由A写的值）。这是正常的：这意味着数据库首先处理了D的写入请求，然后是A的写入请求，而最后是B的读取请求。虽然这不是请求发送的顺序，但是这是一个可接受的顺序，因为这三个请求是并发的。也许B的读取请求在网络中稍微延迟了一些，所以在两次写入完成之后才到达数据库。

* 客户端B的读取请求在客户端A从数据库接收到响应之前返回1，说明值1的写入是成功的。这也是正常的：这并不意味着值是在写入之前读取的，它只是意味着从数据库到客户端A的*正常*响应在网络中稍微延迟了。

* 此模型不假定任何事务隔离：另一个客户端会随时更改值。例如，C首先读取1，然后读取2，因为B在两个读取请求之间更改了值。原子性的比较后设置（CAS）操作可以用于检查没有被另一个客户端更改的值：B和C的*比较后设置*请求成功，但D的*比较后设置*请求失败了（当数据库处理它时，*x*的值不再为0）

* （在阴影条形栏中的）客户端B的最后一次读取操作不是线性化的。这个操作与C的*比较后设置*写入并行，它把*x*从2更新到4。在没有其他请求的情况下，B的读取返回2是正常的。然而，客户端A在B的读取开始之前已经读取了新值4，所以B不允许读取比A更老的值。这与图9-1中的爱丽丝和鲍勃的情况还是一样的。

这就是线性化背后直觉的知识；正式的定义更精确地描述了它。通过记录所有请求和响应的时间，并检查它们是否可以排列成有效的顺序[11]，可以测试系统的行为是不是线性化的（尽管计算的成本很高）。

> 线性化 vs 串行化
>
> 线性化很容易与串行化混淆（参见“串行化”），因为这两个词似乎都意味着“可以按顺序排列”。然而，它们是两种完全不同的保证，必须加以区分：
>
> *串行化*
>
> 串行化是事务的隔离属性，每个事务都可以读写多个对象（行、文档、记录）——见“单对象与多对象操作”一节。它保证事务的行为与它们按照某种串行顺序执行的行为相同（每个事务在下一个事务启动之前执行完毕）。这个串行顺序与实际执行事务的顺序不同是可以的。
>
> *线性化*
>
> 线性化是寄存器（单个对象）读写的新近性保证。它不会将操作分组到事务中，因此不会防止诸如写偏（见“写偏和幻影”一节）等问题，除非采取了其他措施，例如物化冲突（参见“物化冲突”）。
>
> 数据库可以同时提供可串行化和线性化，这种组合被称为*严格串行化*或*强单拷贝串行化*（strong-1SR）[4，13]。基于两阶段锁定（见“两阶段锁定（2PL）”一节）或实际串行执行（参见“实际串行执行”）的串行化实现通常是线性化的。
>
> 然而，串行化快照隔离（见“串行化快照隔离（SSI）”一节）不是线性化的：根据设计，它从一致快照进行读取，以避免读取器和写入器之间的锁定争用。一致快照的全部要点是，它不包括比快照更近期的写入，因此从快照中读取的内容不是线性化的。

### 依赖线性化

在什么情况下线性化是有用的？查看一场体育比赛的最后比分也许是一个轻率的例子：在这种情况下，比赛结果晚了几秒更新不太可能造成任何真正的伤害。然而，在一些领域线性化是使系统正常工作的重要条件。

#### 锁定与主机选举

使用单主机复制的系统需要确保确实只有一个主机，而不是多个主机（裂脑）。选举主机的一种方法是使用锁：每一个节点启动的时候都试图获得锁，成功的节点将成为主机。不管这个锁是如何实现的，它必须是线性化的：所有节点都必须同意哪个节点拥有锁；不然没用。

像Apache ZooKeyer和etcd这样的协调服务通常用于实现分布式的锁与主机选举。他们使用协商一致算法以容错方式实现线性化操作（我们在本章后面的“容错协商一致”一节中讨论了这些算法）。正确实现锁和主机选举仍然有许多微妙的细节（见比如“主机与锁”中的围栏问题），而诸如Apache Curator这样的库，通过提供基于ZooKeeper的高级解决方案起到帮助作用。然而，线性化的存储服务是这些协调任务的基础。

分布式的锁定也用在一些分布式数据库中的更高粒度级别，例如Oracle Real Application Clusters （RAC）。由于多个节点共享对同一磁盘存储系统的访问，RAC在每个磁盘页上都用到锁。由于这些线性化的锁位于事务执行的关键路径上，因此RAC的部署通常有一个专用的集群互连网络，用于数据库节点之间的通信。

#### 约束与唯一性保证

唯一性约束在数据库中是很常见的：例如，用户名或电子邮件地址必须唯一地标识一个用户，而在文件存储服务中，不可能有两个路径和文件名相同的文件。如果要在写入数据时强制执行这个约束（如果两个人试图同时用同样的名字创建用户或者文件，其中一个将被返回错误），你需要线性化。

这种情况实际上与锁类似：当用户注册你的服务时，你可以想象成他们获得了他们选择的用户名上的“锁”。这个操作也非常类似原子性的比较后设置，如果用户名没有被占用，就把这个用户名设为申请该用户名用户的ID。

如果你想确保银行账户余额不会为负数，或者你不会卖出比仓库库存更多的物品，或者两个人不会同时在飞机上或剧院里预订相同的座位，就会出现类似的问题。这些约束都要求有一个所有节点都同意的最新值（帐户余额、库存水平、座位是否占用）。

在实际应用中，宽泛地对待这些约束有时是可以接受的（例如，如果航班超订，你可以把客人转移至不同的航班，并为带来的不便之处向他们进行赔偿）。在这种情况下，线性化也许不需要，我们会在“及时性和完整性”一节讨论这种的被宽泛解读的约束。

但是，硬唯一性约束，例如通常在关系型数据库中找到的那种，需要线性化。其他类型的约束，例如外键或属性约束，不需要线性化的情况下就可以实现。

#### 跨通道时序依赖

注意图9-1中的一个细节：如果爱丽丝没有报出分数，鲍勃就不会知道他的查询结果已经过时了。几秒钟后，他就会再次刷新页面，并最终看到了最后的比分。只有在系统中有另一条通信通道（爱丽丝的声音对到鲍勃的耳朵），才会注意到这种线性化违规。

计算机系统中也可能出现类似的情况。例如，假设你有一个网站，用户可以上传照片，背景进程调整照片大小到较低分辨率以便更快地下载（缩略图）。该系统的架构和数据流如图9-5所示.

需要显式地指示图像编辑器执行调整大小的任务，并且通过消息队列将此指令从Web服务器发送到编辑器（见第11章）。Web服务器不会将整个照片放在队列中，因为大多数消息代理都是专为小消息设计的，而一张照片的大小可能是几兆字节。取而代之的是，照片首先被写到文件存储服务中，一旦写入完成，给编辑器的指令就会放在队列上。

*图9-5 Web服务器和图像编辑器通过文件存储和消息队列进行通信，从而打开了竞争条件的潜在可能性。*

如果文件存储服务是线性化的，那么这个系统应该可以正常工作。如果消息队列不是线性化的，就有存在竞争条件的风险：消息队列（图9-5中的步骤3和步骤4）可能比存储服务中的内部复制的速度更快。在这种情况下，当编辑器获取图像（步骤5）时，它可能会看到图像的旧版本，或者什么也看不到。如果它处理图像的旧版本，则文件存储中的全尺寸的图片和大小调整后的图片就不一致了。

这个问题之所以出现，是因为Web服务器和编辑器之间有两个不同的通信通道：文件存储和消息队列。没有新近性线性化的保证，两个信道之间的竞争条件是可能的。这种情况类似于图9-1，图中两个通信通道之间也存在竞争条件：数据库复制，以及真实世界里爱丽丝嘴巴到鲍勃耳朵之间的音频通道。

线性化并不是避免这种竞赛条件的唯一方法，却是最容易理解的方式。如果你控制着多余的那条通信通道(比如消息队列的情况，但不是爱丽丝和鲍勃的情况)，你可以使用与我们在“读取自己的写操作”中讨论的类似替代方法，代价是额外的复杂性。

### 实现线性化系统

现在既然我们已经看了几个线性化有用的例子，那么让我们考虑一下如何实现提供线性化语义的系统吧。

由于线性化本质上意味着“表现地就像只有一份数据拷贝，而且对它的所有操作都是原子性的”，那么最简单的答案就是真的只用一份数据拷贝。但是，这种方法将没有办法容错：如果保存该副本的节点失效，数据将丢失，或者至少在节点再次上线之前是无法访问的。

使系统可以容错的最常见方法是使用复制。让我们重新回顾第5章中提到的复制方法，然后比较它们是否可以线性化：

*单主机复制（有可能可以线性化）*

在有着单主机复制的系统中（见“主机与从机”一节），主机拥有用于写入的数据的主副本，而从机在其他节点上维护数据的备份副本。如果你从主机或同步更新的从机中读取数据，那么它们有可能是线性化的。然而，并不是每个单主机数据库实际上都是线性化的，要么是因为设计（比如因为它使用快照隔离），要么是由于并发bug。

使用主机完成读取请求依赖于这样一个假设——你确实知道哪个是主机。正如在“真理由多数人定义”一节中所讨论的那样，节点很有可能认为它是主机，而实际上并非如此——如果妄想的主机继续响应请求，就很可能违反了线性化。使用异步复制，故障转移甚至可能会丢失已经提交的写入操作（见“处理节点离线”一节），这既违反了持久性也违反了线性化。

*协商一致算法（线性化的）*

一些我们将在本章稍后讨论到的协商一致算法，与单主机复制相似。然而，协商一致协议包含了防止裂脑和陈旧复本的措施。正是由于这些细节，协商一致算法可以安全地实现线性化存储。这就是例如ZooKeeper以及etcd的工作方式。

*单主机复制（非线性化的）*

具有多主机复制的系统通常是非线性化的，因为它们同时处理多个节点的写入请求，并异步地将它们复制到其他节点。由于这个原因，它们会产生需要解决的冲突写入请求（见“处理写入冲突”一节）。这种冲突是缺少数据单个拷贝的产物。

*无主机复制（大概不可以线性化）*

对于无主机复制的系统（Dynamo风格；见“无主机复制”一节），人们有时会说通过要求仲裁读写（*w + r > n*）你可以获得“强一致性”。取决于仲裁的确切配置，以及如何定义强一致性，这并不完全正确。

基于现世时钟的“以最后一次写入为准”的冲突解决方法（例如在Cassandra中；见“依赖同步时钟”一节）几乎肯定是非线性化的，因为时钟时间戳不能保证与由于时钟偏斜导致的实际事件顺序一致。草率的仲裁(“草率的定额值与提示交换”一节)也破坏了任何实现线性化的机会。即使有严格的仲裁，还是有可能出现非线性化行为，如下一节所示。

#### 线性化和仲裁

从直觉上看，似乎严格的仲裁读写应该在Dynamo风格的模型中是线性化的。然而，当我们有可变的网络延迟时，就有可能有竞争条件，如图9-6所示。

*图9-6 非线性化的执行，尽管使用了严格的仲裁。*

在图9-6中，*x*的初始值是0，并且写入器客户端通过将写入请求发送到所有三个副本（*n*＝3，*w*＝3）把*x*更新到1。同时，客户端A从两个节点的仲裁团中读取（*r* = 2），并且在其中一个节点上看到新值1。同时在写入过程中，客户端B从两个节点构成的另一个仲裁团中读取，并且从这两个节点取回旧值0。

仲裁条件（*w + r > n*）是满足了，但这种执行仍然不是线性化的：B的请求是在A的请求完成后开始的，但是B返回旧值的同时A返回了新值。（这又是图9-1中的爱丽丝和鲍勃的情况。）

有趣的是，可以在牺牲性能的情况下使Dynamo风格的仲裁线性化：在将结果返回到应用程序之前，读取器必须同步执行读修复（见“读取修复与反熵”一节），而编写器必须在发送其写入请求之前读取仲裁团节点的最新状态。然而由于性能损失，Riak不执行同步的读取修复。Cassandra确实在仲裁读取时等待读取修复完成，但是如果对相同的键有多个并发写入，就会失去线性化，因为它使用了以最后一次写入为准的冲突解决方案。

此外，只有线性化的读写操作才能以这种方式实现；线性化的比较后设置操作不能实现，因为它需要协商一致的算法。

总之，最安全的假设是具有Dynamo风格复制的无主机系统不提供线性化。

#### 线性化的代价

正是由于一些复制方法可以提供线性化而另一些不能，因此更深入地探讨线性化的利弊是很有意思的。

我们已经在第5章中讨论了不同复制方法的一些用例；例如，我们看到多主机复制通常是多数据中心复制的一个很好的选择(见“多数据中心操作”一节)。图9-7展示了这样一个部署的例子。

*图9-7 网络中断强制在线性化与可用性之间选择。*

考虑一下如果两个数据中心之间网络中断了会发生什么情况。让我们假设每个数据中心内的网络都在工作，客户端可以访问数据中心，但是数据中心之间不能相互连接。

使用多主机数据库，每个数据中心可以继续正常工作：由于来自一个数据中心的写入请求被异步地复制到另一个数据中心，因此在网络连接恢复的时侯，只需排起队来等待交换写入操作。

另一方面，如果使用单主机复制，那么主机必然位于其中一个数据中心。任何写入和任何可线性读取都必须发送给主机——因此，对于任何连接到从机数据中心的客户端，这些读写请求必须同步地通过网络发送到主机数据中心。

如果数据中心之间的网络在单主机设置下中断了，连接到从机数据中心的客户端无法与主机取得联系，因此他们不能对数据库发起任何写入请求，也不能发起任何线性化读取请求。他们仍然可以向从机发起读取请求，但数据可能是陈旧的（非线性化的）。如果应用程序需要线性化的读写，那么网络中断将导致应用程序在无法与主机取得联系的数据中心中不可用。

如果客户端可以直接连接到主机的数据中心，这并不是一个问题，因为应用程序继续在那里正常工作。但是，只能访问从机数据中心的客户端将经历断线，直到（数据中心之间的）网络链接被修复为止。

#### CAP定理

这个问题不仅仅是单主机和多主机复制的结果：任何线性化数据库都有这个问题，不管它是如何实现的。这个问题也不限于多数据中心部署，它可能发生在任何不可靠的网络上，甚至在一个数据中心内。权衡如下：

* 如果你的应用程序需要线性化，并且一些副本由于网络问题而与其他副本断开了连接，于是有些副本在断开连接时无法处理请求：它们必须要么等待网络问题的解决，要么返回错误（无论哪种方式，它们都变得不可用）。

* 如果你的应用程序不需要线性化，那么可以编写为每个副本都可以独立处理请求，即使它与其他副本（例如多主机）断开连接。在这种情况下，即使面对网络问题应用程序仍然可用，只是它的行为不是线性化的。

因此，不需要线性化的应用程序可以更好地容忍网络问题。这种洞察力被通称为CAP定理，由埃里克·布鲁尔于2000年命名，尽管自20世纪70年代以来分布式数据库的设计者就知道了这种取舍。

CAP最初是作为经验法则提出的，没有准确的定义，目的是为了开始讨论数据库中的取舍问题。当时，许多分布式数据库专注于在具有共享存储的设备集群上提供可线性化的语义[18]，CAP则鼓励数据库工程师探索更广泛的分布式无共享系统的设计空间，这些系统更适合用于实现大规模Web服务。CAP值得称赞的地方是这种文化的转变——见证了自2000年代中期以来新数据库技术的爆发（也就是NoSQL）。

> **没有用处的CAP定理**
>
> CAP有时会表示为*一致性、可用性、分区耐受性：从3项中选择2项*。然而，这样说会误导人，因为网络分区是一种故障，所以它们不是你有的选的东西：不管喜欢与否它们都会发生。
>
> 当网络正常工作时，系统可以提供一致性（线性化）和完全可用性。当网络发生故障时，你必须在线性化或完全可用性之间进行选择。因此，描述CAP更好的方法是*网络分区时要么有一致性要么有可用性*。更可靠的网络可以降低选择的频率，但在某些时候这种选择是不可避免的。
>
> 在CAP的讨论中，对于术语*可用性*有几个相互矛盾的定义，而形式化为定理[30]并不符合其通常的含义。许多所谓的“高度可用”（容错）的系统实际上不符合CAP对可用性的独具一格的定义。总之，围绕CAP存在许多误解和困惑，它不能帮助我们更好地理解系统，所以最好避免CAP。

正式定义的CAP定理范围很窄：它只考虑一个一致性模型（即线性化）和一种故障（*网络分区*，或者在线但是与其它节点断开的节点）。它没有提到任何关于网络延迟、离线节点或其他取舍的内容。因此，虽然CAP在历史上具有影响力，但它对系统的设计没有什么实际价值。

在分布式系统中有许多更有趣的不可能的结果，而CAP现在已经被更精确的结果所取代，因此它在今天更具有历史意义一些。

#### 线性化与网络延迟

虽然线性化是个有用的保证，但在实践中很少有系统实际上是线性化的。例如，即使是现代多核CPU上的RAM也不是线性化的：如果运行在一个CPU核心上的线程在某个内存地址写入数据，而另一个CPU核心上的线程随后读取相同的地址，是不能保证可以读取第一个线程所写的值的（除非使用内存屏障或栅栏）。

造成这种行为的原因是每个CPU内核都有自己的内存缓存和存储缓冲区。默认情况下，内存访问首先访问缓存，任何更改都异步写入主内存。由于访问高速缓存中的数据要比访问主内存快得多，这个特性对于在现代CPU上获得良好性能是必不可少的。然而，这样数据就有了几个副本(一个在主内存中，可能还有几个在各种缓存内)，这些副本都是异步更新的，因此失去了线性化。

为什么要做这个取舍？用CAP定理来证明多核内存一致性模型是没有意义的：在一台计算机中，我们通常假定通信是可靠的，并且我们不期望一个CPU核心在与计算机的其他部分断开连接的情况下能够继续正常工作。放弃线性化的原因是性能，而不是容错。

许多分布式数据库选择不提供线性化保证也是如此：它们这样做主要是为了提高性能，而不是为了容错。线性化很慢——而且一直都是这样的，而不仅仅是在网络故障的时候。

难道我们找不到一种更有效的实现线性化存储的方法吗？答案似乎是否定的：阿提亚和韦尔奇证明了如果想要线性化，读写请求的响应时间至少与网络中延迟的不确定性成正比。在具有高度可变延迟的网络中，就像大多数计算机网络（参见“超时和无界限延迟”一节）一样，线性化读写的响应时间不可避免地会很高。不存在更快的线性化算法，但是较弱的一致性模型可能会更快，因此这种取舍对于对延迟敏感的系统是很重要的。在第12章中我们将讨论在不牺牲正确性的情况下避免线性化的一些方法。

## 排序保证

我们之前说过，可线性化的寄存器的行为就好像数据只有一个副本，并且每个操作似乎在某个时间点原子性地生效。这个定义意味着操作是以某种定义好的顺序执行的。我们在图9-4中展示了这种排序，方法是把这些操作以它们执行的次序连接起来。

在这本书中排序一直是反复出现的主题，这表明它是一个重要的基本概念。让我们简要回顾一下关于排序我们讨论过的其它一些情况：

* 在第5章中，我们看到主机在单主机复制中的主要目的是确定复制日志中写入的顺序——也就是从机应用这些写入的顺序。如果没有单主机，由于并发操作就会引发冲突（见“处理写入冲突”一节）。

* 串行化，我们在第7章中讨论过，是为了确保事务的行为就好像它们是按照某种顺序执行的一样。它的实现既可以通过真的串行顺序执行事务，也可以在（通过锁定或中止）防止串行化冲突的前提下允许并发执行。

* 我们在第8章中讨论的在分布式系统中使用时间戳和时钟（见“依赖同步时钟”一节），是把顺序引入无序世界的又一次尝试，例如，确定两个写入请求哪一个是稍后发生的。

结果表明，排序、线性化与协商一致之间有着深刻的联系。虽然这个概念比这本书的其他部分更偏理论更抽象一些，但它对于澄清我们对系统能做什么不能做什么的理解是很有帮助的。我们将在接下来的几个小节中探讨这个主题。

### 排序与因果关系

排序持续出现的原因有很多，其中一个是它帮助维护了因果关系。在这本书中，我们已经看到了几个因果关系很重要的例子：

* 在“一致前缀读取”一节（图5-5）中，我们看到了一个例子，会话的观察者首先看到了问题的答案，然后才看到了被回答的问题。这是令人困惑的，因为它违背了我们对原因和结果的直觉：如果问题得到了回答，那么这个问题显然必须首先出现，因为给出答案的人一定看到了这个问题（假设他们不是灵媒，是看不到未来的）。我们说问题与答案之间是存在因果关系的。

* 图5-9中也出现了类似的模式，我们研究了三台主机之间的复制，并注意到一些写入请求可能因为网络延迟的缘故而“超越”其他写入请求。从一个副本的角度来看，好像有一个不存在的行更新。这里的因果关系意味着行必须先创建才能更新。

* 在“检测并发写入”一节中，我们观察到如果有两个操作A和B，那么有三种可能：A发生在B之前，B发生在A之前，以及A和B是并发的。这种*发生在...之前*关系是因果关系的另一种表达：如果A发生在B之前，这意味着B可能知道A，或建立在A之上，或依赖于A，如果A和B同时存在，它们之间就没有因果关系；换句话说，我们确信两者都不知道对方。

* 在事务的快照隔离（“快照隔离和可重复读”一节）的上下文中，我们说，事务从一致快照进行读取。但是，在这种情况下“一致”意味着什么呢？它的意思是*与因果关系一致*：如果快照包含一个答案，它还必须包含被回答的问题。在单个时间点观察整个数据库使其与因果关系一致：在该时间点之前发生的所有操作的结果都是可见的，但在此之后发生的任何操作都是不可见的。读偏（不可重复读，如图7-6所示）意味着在违反因果关系的状态下读取数据。

* 我们举的事务之间写偏的例子（见“写偏和幻影”一节）也显示了因果相关性：在图7-8中，爱丽丝被允许离岗是因为事务认为鲍勃仍然在值，反之亦然。在这种例子中，离岗的动作依赖于谁正在当值的观察结果。可序列化快照隔离（见“可序列化快照隔离（SSI）”）通过跟踪事务之间的因果依赖来检测写偏。

* 在爱丽丝和鲍勃看足球的例子中（图9-1），鲍勃在听到爱丽丝惊叹结果后从服务器那里得到了旧结果，这违反了因果关系：Alice的感叹在因果关系上取决于比分的宣布，所以鲍勃也应该能够在听到爱丽丝的声音之后看到这个分数。同样的模式再次出现在“跨通道计时依赖”一节，这一次是图片大小调整服务。

因果关系在事件之间强加了顺序：原因先于效果；消息在被收到之前已经被发送了；问题出现在答案之前。就像在现实生活中一样，一件事导致另一件事：一个节点读取了一些数据然后写入了一些东西作为结果，另一个节点读取了这个被写入的东西然后再依次写入了其他的东西，以此类推。这些因果相关的操作链定义了系统中的因果顺序——也就是什么发生在什么之前。

如果一个系统服从因果关系所施加的顺序，我们就说它在*因果一致*的。例如，快照隔离提供了因果一致性：当你从数据库中读取数据时，当你看到一些数据时，你还必须能够看到因果关系之前的任何数据（假设它在此期间没有被删除）。

#### 因果顺序不是全序

全序允许对任意两个元素进行比较，因此如果有两个元素，则始终可以说哪个更大哪个更小。例如，自然数是全序的：如果我给你任意两个数字，比如说5和13，你可以告诉我13大于5。

然而，数学集合是非全序的：{*a, b*}大于{*b, c*}吗？你根本不能比较它们，因为它们彼此都不互为子集。我们说它们是不可比较的，因此数学集合是偏序的：在某些情况下，一个集合大于另一个集合（如果一个集合包含另一个的所有元素），但在其他情况下它们是不可比较的。

全序与偏序之间的差异反映在不同的数据库一致性模型中：

*可线性化*

在一个可线性化的系统中，我们有一个操作全序：如果系统的行为好象数据只有一个拷贝，并且每一个操作都是原子性的，这就意味着对于任何两个操作，我们总是可以说哪一个是先发生的。该总序被展示为图9-4中的时间线。

*因果关系*

我们说过如果两个操作彼此都不是在另一个操作之前发生的，那么两个操作是并发的（见“在...之前发生关系与并发”一节）。换句话说，如果两个事件是因果相关的（一个发生在另一个事件之前）那么它们是有序的，但如果它们是并发的那么它们是不可比较的。这意味着因果关系定义的是偏序，而不是全序：有些操作是根据彼此的顺序排列的，但有些操作是不可比较的。

因此根据这个定义，在可线性化的数据存储中是不存在并发操作的：必然有一个时间线，所有的操作都是全序地沿着它排列。可能有几个请求在等待处理，但是数据存储保证每个请求没有任何并发地，在单个时间线的单个时间点上对数据的单个副本进行操作。

并发意味着时间线先分支，而后再合并——在这种情况下，不同分支上的操作是不可比较的（即并发的）。我们在第5章中看到了这种现象：例如，图5-14不是一条直线的全序，而是多个不同操作同时进行的大杂烩。图中的箭头表示因果关系——操作的偏序。

如果你熟悉分布式版本控制系统，比如Git，它们的版本历史非常类似因果依赖关系图。通常一次提交在另一次提交之后，在一条直线上进行的，但有时会有分支（当几个人同时在一个项目上工作时），而合并分支是在并发创建的提交在合并时创建的。

#### 线性化强于因果一致性

那么因果顺序和线性化之间有什么关系呢？答案是线性化意味着因果关系：任何可线性化的系统都将正确地保留因果关系。特别是，如果系统中有多个通信通道（例如图9-5中的消息队列和文件存储服务），线性化可以确保因果关系自动保持，而无需系统做任何特殊的处理（例如在不同组件之间传递时间戳）。

线性化确保因果关系使线性化系统易于理解并且有吸引力。然而，正如在“线性化的成本”一节中所讨论的，使系统线性化可能会损害其性能和可用性，特别是如果系统有显著的网络延迟(例如，它分布在不同地地理区域)。由于这个原因，一些分布式数据系统已经放弃了线性化，这使得它们能够获得更好的性能但也更难以使用。

好消息是折中方案是可行的。线性化不是保持因果关系的唯一途径——还有其他方法。系统可以是因果一致的，而无需引入线性化带来的性能影响（特别是CAP定理不适用了）。事实上，因果一致性是最强大的一致性模型，它不会因网络延迟而减慢，并且在网络故障面前仍然可用。

在许多情况下，看起来需要线性化的系统实际上只是要求因果一致性，而因果一致性可以更有效地实现。基于这一观察，研究人员正在探索保存因果关系的新型数据库，其性能和可用性特征与最终一致的系统类似。

由于这项研究是很近的，它还没有进入生产性系统中，而且还有一些挑战有待克服。然而，它是未来系统的一个有前途的方向。

#### 抓取因果关系

我们不会在这里讨论非线性系统可以如何保持因果一致性的所有细节，而只是简单地探讨一些关键的想法。

为了保持因果关系，你需要知道哪个操作发生在另外哪个操作之前。这是一个偏序：并发操作可以按任何顺序处理，但如果一个操作发生在另一个操作之前，则必须在每个副本上按照那个顺序处理它们。因此，当副本处理一个操作时，它必须确保所有因果前操作（之前发生的所有操作）都已被处理；如果缺少了某个前序操作，则后面的操作必须等待，直至前序操作被处理。

为了确定因果关系，我们需要某种方法来描述系统中节点上的“知识”。如果一个节点在发出了写入Y时已经看到了值X，那么X和Y可能是因果相关的。这一分析使用了在欺诈指控的刑事调查中你会想到的问题：在他们做出决定Y时，首席执行官*知道*X的情况吗？

用于确定哪个操作发生在另外哪个操作之前的技术与我们在“检测并发写入”一节中讨论的类似。那一节讨论了无主机数据存储中的因果关系，在那种情况下我们需要检测对同一个键的并发写入，以防止丢失更新。因果一致性更进一步：它需要跟踪整个数据库中的因果依赖，而不仅仅是单个键。版本向量可以推广到这里来完成这件事。

为了确定因果顺序，数据库需要知道应用程序读取了哪个版本的数据。这就是为什么在图5-13中，之前操作的版本号在写入时被传回数据库的原因。在SSI的冲突检测中也出现了类似的想法，正如在“可串行化快照隔离（SSI）”一节中所讨论的那样：当一个事务想要提交时，数据库检查它读取的数据版本是否仍然是最新的。为此，数据库会跟踪记录哪个数据被哪个事务读取了。

### 序号排序

虽然因果关系是一个重要的理论概念，但实际上，跟踪记录所有的因果关系是不切实际的。在许多应用程序中，客户端在写入某些内容之前会读取大量数据，然而并不清楚写入请求是否因果依赖于先前的所有还是部分读取请求。明确地跟踪记录已读取的所有数据意味着很大的开销。

然而，有更好的方法：我们可以使用序列号或时间戳对事件排序。时间戳不需要来自现世时钟（或物理时钟，它有许多在“不可靠的时钟”一节讨论到的问题）。取而代之的是逻辑时钟，它是一种生成数字序列从而标识操作的算法，通常使用计数器，每当发生一个操作就增加一。

这样的序列号或时间戳很紧凑（只有几个字节的大小），并且它们提供了全序：即，每个操作都有唯一的序列号，并且总是可以比较两个序列号来确定哪个更大（即，哪个操作发生得更晚）。

特别是，我们可以按照与因果关系一致的全序构建序列号：我们承诺，如果操作A发生在B之前，则A在全序中出现在B之前（A的序列号低于B）。并发操作可以任意地排序。这样的全序包含了所有的因果关系信息，但也强加了比因果关系更严格要求的次序。

在具有单主机复制（见“主机与从机”一节）的数据库中，复制日志定义了与因果性一致的写入操作全序。主机只用为每个操作递增计数器，从而为复制日志中的每个操作分配单调递增的序列号。如果从机按照在复制日志中出现的顺序应用写入，那么从机的状态总是因果一致的（即使状态落后于主机）。

#### 非因果序列号生成器

如果不是单主机(也许是因为你在使用的是多主机或无主机的数据库，或者因为数据库是分了区的)，那么如何为操作生成序列号就不那么清楚了。那么在实践中，用到了各种各样的方法：

* 每个节点可以生成自己独立的序列号集合。比如说，如果有两个节点，一个节点只能生成奇数，而另一个节点只能生成偶数。通常，你可以在序列号的二进制表示中保留一些位，以包含唯一的节点标识符，这样可以确保两个不同的节点永远不会生成相同的序列号。

* 你可以为每一个操作附加来自现世时钟（物理时钟）的时间戳。这样的时间戳并不是连续的，但是如果它们具有足够高的精度，它们可能就足以完全全序操作。在以最后一次写入为准的冲突解决方法中用到了这个事实（见“为事件排序的时间戳”一节）。

* 你可以预先分配序列号块。例如，节点A可能要求从1到1，000之间的序列号块，而节点B可能要求从1，001到2，000之间的块。然后每个节点可以独立地从各自的块分配序列号，并在可用序号减少时分配到一个新块。

相比于把所有的操作都推送到单主机从而使计数器加一，这三个选项执行效果更好，也更具有伸缩性。它们为每个操作生成一个唯一的、近似增加的序列号。然而，它们都有一个问题：它们产生的序列号与因果关系不一致。

之所以会出现因果关系问题，是因为这些序列号生成器无法正确捕获不同节点之间操作的顺序：

* 每个节点每一秒可以处理不同数量的操作。因此，如果一个节点产生偶数而另一个节点产生奇数，偶数计数器可能落后于奇数的计数器，抑或反之。如果你有一个奇数的运算和偶数的运算，你并不能准确地分辨出哪一个是先发生的。

* 来自物理时钟的时间戳受时钟偏差的影响，这可能使它们与因果关系不一致。例如图8-3，其中显示了一个场景，在该场景中因果关系中稍后发生的操作实际上被分配了一个较低的时间戳。

* 在块分配器的情况下，可以给一个操作在1001到2000的范围内的序列号，而随后的操作可以给出范围为1到1000的数字。在这里同样的，序列号与因果关系不一致。

#### 兰波特时间戳

虽然刚才描述的三个序列号生成器与因果关系不一致，但是实际上*有*一种与因果关系相一致的简单的序列号生成方法。它被称为*兰波特时间戳*，由莱斯利·兰波特于1978年提出，现在是分布式系统领域中被引用最多的论文之一。

兰波特时间戳的使用如图9-8所示。每个节点都有一个唯一的标识符，每个节点都有一个记录了它处理了的操作数的计数器。兰波特时间戳只是一对（*计数器，节点ID*）。两个节点有时可能有相同的计数器值，但是通过在时间戳中包含节点ID，每个时间戳都是唯一的。

*图9-8 兰波特时间戳提供了与因果关系一致的全序。*

兰波特时间戳与物理现世时钟没有关系，但它提供了全序：如果你有两个时间戳，那么计数器值越大，时间戳越大；如果计数器值相同，那么节点ID越大，时间戳就越大。

到目前为止，这种描述基本上与上一节中描述的偶数/奇数计数器相同。兰波特时间戳的关键思想是：每个节点和每个客户端都跟踪记录它到目前为止看到的最大计数器值，并在每个请求中包含这个最大值。当节点接收到最大计数器值大于其自身计数器值的请求或响应时，它立即将自己的计数器增大到最大。

如图9-8所示，客户端A从节点2接收计数器值5，然后将最大值5发送给节点1。那时节点1的计数器只有1，但它立即向前移动到5，因此下一次操作的计数器值递增为6。

只要在每次操作中都携带最大计数器值，这个方案就能确保来自兰波特时间戳的排序与因果关系一致，因为每个因果依赖关系都会导致时间戳的变大。

兰波特时间戳有时会与我们在“检测并发写入”一节中看到的版本向量相混淆。虽然它们有一些相似之处，但它们有一个不同的目的：版本向量可以区分两个操作是并发的还是一个是因果依赖的，而兰波特时间戳总是强制执行总排序。从兰波特时间戳的总排序中，你无法判断两个操作是并发的还是因果相关的。兰波特时间戳相对于版本向量的优点是它们更紧凑。

#### 时间戳排序是不够的

尽管兰波特时间戳定义了与因果关系一致的操作的全序，但是它们并不完全足以解决分布式系统中的许多常见问题。

例如，考虑需要确保用户名唯一标识用户帐户的系统。如果两个用户同时尝试创建具有相同用户名的帐户，那么两个用户中的一个应该成功另一个应该失败。（我们之前在“主机与锁”中提到过这个问题。）

乍一看，似乎全序操作(例如，使用兰波特时间戳)就足以解决这个问题：如果创建了两个具有相同用户名的帐户，则选择具有较低时间戳的帐户作为赢家(先获取用户名的帐户)，然后让具有较大时间戳的帐户失败。由于时间戳是全序的，这种比较总是有效的。

这种方法适用于事后确定赢家：一旦收集了系统中所有的用户名创建操作，你就可以比较它们的时间戳了。但是，当节点刚刚收到用户创建用户名的请求，并且需要立即决定请求是成功还是失败时，这是不够的。此时，这个节点不知道另一个节点是否同时正在创建一个具有相同用户名的帐户，也不知道另外那个节点可能为该操作分配的时间戳是什么。

为了确保没有其他节点在并发创建具有相同用户名和较低时间戳的帐户的过程中，你必须检查其他节点，看看它们正在做什么。如果其他节点中的一个由于网络问题而失败或无法到达，该系统将陷入瘫痪。这不是我们需要的那种容错系统。

问题是只有在收集了所有操作之后，操作的全序才会出现。如果另一个节点生成了一些操作，但你还不知道它们是什么，那么就无法构造操作的最终顺序：这些来自另一个节点的未知操作可能需要按全序插入在不同位置。

总结一下：为了实现关于用户名的唯一性约束，只有操作的全序是不够的——你还需要知道排序何时完成。如果你有一个创建用户名的操作，并且你确信没有其他节点能够在操作之前按全序插入相同用户名的声明，那么你可以安全地声明操作成功。

知道全序何时完成的这个理念正是全序广播的主题。

### 全序广播

如果你的程序只运行在在单个CPU核心上，那么定义操作的全序是很简单的：这就是CPU执行它们的顺序。然而，在分布式系统中，让所有节点都同意相同的操作顺序是很困难的。在最后一节中我们讨论了按时间戳或序列号排序，但发现它不如单主机复制（如果使用时间戳排序来实现唯一性约束，则不能容忍任何错误）强大。

我们之前讨论过，单主机复制通过选择一个节点作为主机并把所有的操作排列在主机的单个CPU核心上来决定操作的全序的。接下来的挑战是，如果吞吐量超过单个主机可以处理的量应该如何缩放系统，以及如果主机失败如何处理故障转移（见“处理节点中断”一节）。在分布式系统的文献中，这个问题被称为全序广播或原子广播。

> **排序保证的范围**
>
> 每个分区只有单主机的分区数据库通常只维护每个分区的顺序，这意味着它们不能提供跨分区的一致性保证(例如，一致快照、外键引用)。实现跨所有分区的全序是可能的，但需要额外的协调工作。

全序广播通常被描述为用于节点之间交换消息的协议。它非正式地要求两个安全属性必须总是满足：

可靠地送达

消息不会丢失：如果一个消息被传递到一个节点，那么说明它被传递到了所有节点。

全序地送达

消息以相同的顺序传递给每个节点。

正确的全序广播算法必须确保可靠性和排序属性总是满足的，即使节点或网络有故障。当然，网络中断的时候消息无法传递，但是算法可以继续重试，这样当网络最终被修复时，消息就会传递过去（并且它们仍然必须以正确的顺序送达）。

#### 使用全序广播

协商一致服务，比如ZooKeeper和etcd，实际上实现了全序广播。这个事实暗示了全序广播与协商一致之间有着密切的联系，我们将在本章稍后部分加以探讨。

全序广播正是数据库复制所需的：如果每个消息代表了对数据库的一次写入，并且每个副本以相同的顺序处理相同的写入，那么副本之间将保持一致（除了任何暂时的复制滞后）。这个原理被称为状态机复制，我们将在第11章中再讨论它。

同样地，全序广播可以用来实现串行化事务：正如在“实际串行执行”一节中所讨论的，如果每个消息代表了一个要作为存储过程执行的确定性事务，而且如果每个节点以相同的顺序处理这些消息，那么数据库的分区和副本会彼此保持一致。

全序广播的一个重要方面是顺序在消息发送时是固定的：如果后续地消息已经送出，则不允许节点回溯性地将消息插入到顺序中的先前位置。这使得全序广播比时间戳排序更强。

看待全序广播的另一种方式，那就是，这是一种创建日志（如复制日志、事务日志或预写入日志）的方式：传递消息就像写入日志一样。因为所有节点都必须以相同的顺序传递相同的消息，所以所有的节点都可以读取日志，看到相同的消息序列。

全序广播对于实现提供栅栏令牌的锁服务也很有用（见“栅栏令牌”一节）。获取锁的每一个请求都会作为消息附加到日志中，并且所有消息都会按照它们在日志中出现的顺序编号。之后，序列号可以用作栅栏令牌，因为它是单调递增的。在ZooKeeper中，这个序列号称为`zxid`。

#### 利用全序广播实现线性化存储

如图9-4所示，在一个线性化系统中有一个关于操作的全序。这是不是意味着线性化与全序广播是同一回事？不完全是，但是在两者之间有着密切的联系。

全序广播是异步的：消息保证是以固定的顺序可靠地传递的，但是没有办法保证什么时侯传递消息（因此，一个接收者可能落后于其他接收者）。相比之下，线性化是种新近性保证：读取操作保证可以看到最新写入的值。

然而如果有了全序广播，你可以在它的基础上构建线性化存储。例如，你可以确保用户名是唯一地标识用户帐户的。

想象一下，对于每一个可能的用户名，你可以通过原子性的比较后设置操作拥有一个线性化寄存器。每个寄存器最初的值为`null`（指明用户名还没有取）。当用户想要创建用户名时，在寄存器上为该用户名执行比较后设置操作，将其设置为用户帐户ID，条件是前一个寄存器值为`null`。如果多个用户尝试并发地获取同一个用户名，那么只可能有一个比较后设置操作会成功，因为其他用户（由于线性化）将看到一个不是`null`的值。

你可以通过把全序广播作为只添加日志来实现这样一个线性化的比较后设置操作，如下所示：

1. 在日志中添加一条消息，试探性地指出要求的用户名。

2. 读取日志，然后等待被添加的消息发送回来。

3. 检查任何要求你要的用户名的消息。如果对应你想要的用户名的第一条消息是你自己的消息，那么就成功了：你可以提交用户名声明（也许是通过在日志中附加另一条消息）并且向客户端确认。如果第一条消息来自另一个用户，那么操作中止。

因为日志条目以相同的顺序传递给所有节点的，如果有几个并发写入请求，那么所有节点都会同意哪条日志是第一条。选择冲突写入中的第一个作为胜利者然后中止后面的写入，可以确保所有节点就写入是否已提交或中止达成一致。类似的方法可以用于在日志基础之上实现串行化的多对象事务。

虽然这个过程确保了线性化写入，但是它并不保证线性化读取——如果从一个异步更新于日志的存储中读取，那么有可能会过期。（准确地说，此处所述的过程提供了顺序一致性，有时也称为时间线一致性，相比于线性化这个保证略弱一些。)要使读取线性化，有几个选项：

* 你可以通过添加消息，读取日志为读取排序，当消息传递回来时执行真正的读取动作。因此，消息在日志中的位置定义了读取发生的时间点。（etcd中的仲裁团读取与这个有些类似）。

* 如果日志允许以线性化的方式获取最新日志消息的位置，你可以查询这个位置，等待所有在此之前的条目送达，然后执行读取。（这就是ZooKeeper的`sync()`操作背后的理念。）

* 你可以向写入同步更新的副本发起读取，因此肯定是最新的。（该技术用于链复制；见“关于复制的研究”一节）。

#### 利用线性化存储实现全序广播

上一节展示了如何使用全序广播建立一个线性化的比较后设置操。我们也可以反过来，假设已经有了线性化存储，展示如何用它构建全序广播。

最简单的方法是假设你有一个线性化的寄存器，它可以存储一个整数，并且有原子性的加一后获取的操作。亦或是原子性的比较后设置操作。

算法很简单：对于要通过全序广播发送的每一条消息，你都会加一后获取那个线性化的整数，然后将从寄存器获得的值作为序列号附加到消息中。然后你可以将消息发送到所有节点（重发任何丢失了的消息），接收方将按照序列号连续地传递消息。

值得注意的是与兰波特时间戳不同，通过递增线性化寄存器获得的数字构成了一个没有间隙的序列。因此，如果节点已经传递了消息4并接收到序列号为6的消息，它知道必须等到消息5之后才能传递消息6。用兰波特时间戳的情况就不一样了——事实上，这就是全序广播和时间戳排序之间的关键区别。

让线性化整数有原子性的加一后获取运算是有多难呢？和往常一样，如果从来没有失败过，那就很容易了：你可以将它保存在一个节点上的一个变量中。问题在于处理到该节点的网络连接中断时的情况，以及节点失效时如何恢复值。一般来说，如果对线性化的序列号生成器考虑得够仔细，你就不可避免地需要协商一致的算法。

这不是巧合：可以证明，线性化的比较后设置(或加一后获取)寄存器和全序广播都等同于协商一致。也就是说，如果你能解决其中的一个问题，你可以把它转化为其他问题的解决方案。这是一个相当深刻和令人惊讶的洞察力！

现在是时候直接解决协商一致的问题了，我们将在本章的其余部分中完成它。

## 分布式事务与协商一致

协商一致是分布式计算中最重要和最基本的问题之一。表面上看它似乎很简单：非正式地说，目标就是让几个节点就某件事达成一致。你可能认为这不应该太难。然而，许多蹩脚的系统错误地认为这个问题是容易解决的。

虽然协商一致是非常重要的，关于它的章节之所以出现在这本书的很后边是因为这个话题是相当微妙，而欣赏这种微妙之处需要一些先决的知识。即使在学术界，对协商一致的理解也是经过几十年才逐渐形成的，其间也存在着许多误解。既然我们已经讨论了复制（第5章）、事务（第7章）、系统模型（第8章）、线性化和全序广播（这一章），我们终于准备好解决协商一致问题了。

在许多情况下，节点必须达成一致意见。例如：

*主机的选举*

在具有单主机复制的数据库中，所有节点需要一起商定哪个节点是主机。如果一些节点由于网络故障而无法与其他节点通信，那么主机的地位可能会出现争议。在这种情况下，协商一致在防止出现恶劣的故障迁移非常重要，因为这会导致两个节点都认为自己是领导者的裂脑状态（见“处理节点中断”一节）。如果有两位领导者，他们都会接受写作，他们的数据也会出现分歧，从而导致不一致和数据丢失。


*原子性提交*

在支持事务跨越多个节点或分区的数据库中，我们有这样一个问题：一个事务在某些节点上可能失败，而在另一些节点上会成功。如果我们想维持事务原子性（在ACID的意义上，见“原子性”一节），我们必须让所有节点就事务的结果达成一致：要么它们都中止/回滚（如果出了什么问题），要么它们都提交（如果没有出任何问题）。这个协商一致的例子被称为*原子提交*问题。

> **不可能达成的共识**
>
> 你可能已经听说过FLP结果——以作者菲舍尔、林奇和帕特森命名——它证明了如果存在节点可能会崩溃的风险，那么没有算法总是能够达成一致。在分布式系统中，我们必须假设节点可能会崩溃，因此可靠的一致性是不可能的了。然而，我们现在在这里，正在讨论了达成一致的算法。到底是怎么一回事？
>
> 答案是，FLP结果在异步系统模型(见“系统模型与现实”一节)中证明的，这是一个非常严格的模型，它假设了确定性算法是不能使用任何时钟或超时。如果允许算法使用超时，或者使用其他方法识别可疑的崩溃节点(即使怀疑有时是错误的)，那么协商一致就可以解决。即使只允许算法使用随机数，也足以绕过这个不可能的结果。
>
> 因此，虽然不可能达成协商一致的FLP结果有着重要的理论意义，但分布式系统在实际应用中通常是可以达到共识的。

在这一节中，我们首先会更详细地研究原子提交问题。我们会特别讨论两阶段提交（2PC）算法，这是最常见的解决原子提交的方法，并在各种数据库、消息系统和应用服务器中实现。事实证明，2PC是一种协商一致的算法——但不是最好的那个。

通过学习2PC算法，我们将了解更好的协商一致算法，比如那些用于ZooKeeper（Zab）和etcd（Raft）里的那些算法。

### 原子提交与两阶段提交（2PC）

在第7章中，我们了解到事务原子性的目的，是在发起几次写入时中间出错的情况下提供简单的语义。事务的结果要么是成功提交，在这种情况下事务的所有写入都持久化了，要么是中止，在这种情况下事务的所有写入都回滚了（也就是撤消或者丢弃了）。

原子性防止了失败的事务把完成一半的结果和一半更新的状态丢在数据库中。这对于多对象事务（见“单对象和多对象操作”一节）以及维护二级索引的数据库尤其重要。每个二级索引都是一个独立于主数据的数据结构——因此如果你修改了一些数据，那么还需要在二级索引中进行相应的更改。原子性确保了二级索引与主数据保持一致（如果索引变得与主数据不一致，那就没有太大用处了）。

#### 从单节点到多节点的原子提交

对于在单个数据库节点上执行的事务，原子性通常由存储引擎实现。当客户端请求数据库节点提交事务时，数据库使事务的写操作持久化（通常在预写日志中；见“使B树变得可靠”一节），然后将提交记录添加到磁盘上的日志中。如果数据库在这个过程中间崩溃，那么节点重启之后事务可以从日志中恢复：如果提交记录在崩溃前成功写入磁盘，那么可以认为事务已经提交了；如果没有，任何来自该事务的写入都被回滚。

因此，在单个节点上，事务提交在很大程度上取决于数据持久化写入磁盘的顺序：首先是数据，然后是提交记录。决定事务是提交还是中止的关键时刻是磁盘完成写入提交记录的时刻：在此之前，仍然可以中止（由于崩溃的原因），但在这个时刻之后，事务被提交了(即使数据库之后发生崩溃)。因此，是单个设备(连接到特定节点、某个特定磁盘驱动器的控制器)使提交具有原子性。

但是，如果事务涉及到多个节点该怎么办？例如，你可能在分区数据库中有一个多对象事务，或者有一个术语分区次级索引（索引项可能位于与主数据不同的节点上；见“分区与次级索引”一节）。大多数“NoSQL”分布式数据存储不支持这样的分布式事务，但是各种集群关系型系统支持（见“实践中的分布式事务”一节）。

在这些情况下，仅仅向所有节点发送提交请求并在每个节点上独立地提交事务是不够的。在这样做时，很容易发生在某些节点上提交成功而在其他节点上提交失败的情况，这违反了原子性保证：

* 一些节点可能检测到约束条件被违反或是冲突，因而中止无法避免，而其他节点则可以成功提交。

* 一些提交请求可能在网络中丢失，最终导致超时而事务中止，而其他提交请求则正常通过。

* 某些节点可能在提交记录被完全写入之前崩溃，于是恢复时回滚了，而其他节点则成功提交。

如果一些节点提交了事务而另一些却中止了事务，节点之间就会变得不一致（就像图7-3中那样）。一旦事务在一个节点上提交了，即使后来发现它在另一个节点上被中止，也不能再撤销了。由于这个原因，一个节点只能在事务中的所有其他节点也都会提交的情况下才能提交。

事务提交必须是不可撤销的——你无法在事务提交后改变主意并追溯性地中止事务。这条规则的原因是一旦数据被提交，它对于其他事务就是可见地了，因此其他客户端会开始依赖这个数据；这一原则构成了在“提交读”一节中讨论的提交读隔离的基础。如果允许事务在提交后还可以中止，那么任何读取了已经提交的数据的事务都会是基于被追溯性声明不存在的数据——于是它们也必须被撤销。

（提交了的事务结果稍后可以被另一个抵消性质的事务复原。然而从数据库的角度来看这是一个单独的事务，因此任何跨事务正确性要求都是应用程序自己的问题。）

#### 两阶段提交介绍

两阶段提交是一种跨多个节点实现原子事务提交的算法——即，确保所有节点要么都提交要么都中止。它是分布式数据库中的经典算法。2PC在一些数据库中内部使用，也以*XA事务*（例如被Java事务API支持）的形式提供给应用程序，或者以WS-AtomicTransaction的形式用于SOAP Web服务。

2PC的基本流程如图9-9所示。与单节点事务的单个提交请求不同的是，2PC中的提交/中止进程被分成两个阶段（因此而得名）。

*图9-9 一次两阶段提交（2PC）的成功执行*

> **不要混淆2PC和2PL**
>
> 两阶段提交（2PC）和两阶段锁定（见“两阶段锁定（2PL）”一节）是两件非常不同的事情。2PC在分布式数据库中提供原子提交，而2PL提供序列化的隔离。为了避免混淆，最好把它们看作是完全独立的概念，而忽略名字之间的相似之处。

2PC使用了一个通常不在单节点事务中出现的新组件：协调器（也称为事务管理器）。协调器通常被实现为请求事务的应用程序进程中的一个库(例如，嵌入在JavaEE容器中)，但它也可以是一个单独的进程或服务。这类协调员的例子包括Narayana、JOTM、BTM或MSDTC。

如往常一样，2PC事务始于应用程序在多个数据库节点上读写数据开始。在事务中我们把这些数据库节点称为*参与者*。当应用程序准备提交时，协调器开始第1阶段：它向每个节点发送一个*准备*请求，询问它们是否能够提交。协调员随后跟踪参与者的响应：

* 如果所有参与者都回答“是”，表示他们已经准备好提交，那么协调器将在第2阶段发出提交请求，并且提交实际发生。

* 如果任何参与者回答“否”，协调器将向阶段2中的所有节点发送*中止*请求。

这个过程有点像西方文化中的传统婚礼：牧师分别询问新娘和新郎双方是否愿意结婚，通常都会从双方那里得到“我愿意”的回答。在收到双方的确认后，牧师宣布夫妻双方的婚姻：事务已经提交，并且这一愉快的事实将向所有参与者广播。如果新娘或新郎没有说“是”，婚礼就中止了。

#### 满是承诺的系统

从这个简短的描述中为什么两阶段提交确保了原子性可能并不清楚，而跨几个节点的一阶段提交却不能。在两阶段情况下准备和提交请求当然也同样容易丢失。那是什么让2PC与众不同的？

要了理解它的工作原理，我们必须更详细地分解这个过程：

1. 当应用程序想要开启一个分布式事务时，它会从协调器请求事务ID。这个事务ID是全局唯一的。

2. 应用程序在每个参与者上开始一个单节点事务，并将全局唯一的事务ID附加到单个节点事务。所有的读写都是在这些单节点事务中完成的。如果在此阶段出现任何问题（例如，节点崩溃或请求超时），协调器或任何参与者都可以中止。

3. 当应用程序准备提交时，协调器向所有参与者发送一个准备请求，并使用全局事务ID进行标记。如果这些请求中有任何一个失败或超时，协调器将向所有参与者发送针对该事务ID的中止请求。

4. 当参与者收到准备请求时，它确保在任何情况下都能够提交事务。这包括将所有事务数据写入磁盘（崩溃、电源故障或磁盘空间耗尽都不是稍后拒绝提交不可接受的借口），以及检查是否存在任何冲突或违反约束。通过对协调器回答“是”，节点承诺在请求时正确无误地提交事务。换句话说，参与者交出了中止事务的权限，也没有实际提交事务。

5. 当协调器已经收到所有针对准备请求的响应时，是提交还是中止事务它会做出决定性的决定（只有在所有参与者都投票赞成“是”才会提交）。协调器必须把这个决定写入到磁盘上的事务日志中，以防之后崩溃的话它知道这是如何确定下来的。这被称为*提交点*。

6. 一旦协调器的决定被写入磁盘，提交或中止请求会被发送给所有参与者。如果此请求失败或是超时，协调器必须一直重试，直至成功。不再有回退了：如果这个决定是提交，那么这个决定就必须被执行，无论需要多少次重试。如果一个参与者在此期间崩溃，那么事务会在它恢复的时候提交——因为它投了“是”，因此恢复的时侯它不能拒绝提交。

因此，这个协议包含两个关键的“不回退点”：当参与者投“是”的时侯，它承诺它肯定稍后可以提交（尽管协调器仍可选择中止）；一旦协调器做出决定，决定是不可撤销的。这些承诺保证了2PC的原子性。（单节点原子提交把这两个事件合并为一个：把提交记录写入事务日志。）

回到婚姻的类比中，在说“我愿意”之前，你和你的新娘/新郎可以通过说“没门”（或是有类似效果的东西）自由地中止事务。然而在说了“我愿意”之后，你不能收回那句话。如果你在说“我愿意”之后晕倒了，而你没有听到牧师说“你们现在是夫妻了”，那也改变不了事务已经发生的事实。当稍后恢复意识时，你可以通过查询神父的全局事务ID状态来确定你是否已经结婚了，或者你可以等待神父下一次提交请求的重试（因为重试会在你整个失去意识期间继续进行）。

#### 协调器失效

我们已经讨论了在2PC期间，如果其中一个参与者或是网络失效会发生什么：如果任何准备请求失败或超时，协调器会中止事务；如果任何提交请求或中止请求失败，协调器将无限地重试。但是如果协调器崩溃就不是太清楚会发生什么了。

如果协调器在发送准备请求之前失败，则参与者可以安全地中止事务。但是，一旦参与者收到了准备请求并投了“是”，它就不能单方面中止事务——它必须等待协调员的回复事务是提交了还是中止了。如果协调器在这个时候崩溃或网络失败，参与者什么都做不了只能等待。参与者在这种状态下的事务被称为*怀疑*或*不确定*。

这种情况如图9-10所示。在这个特定的例子中，协调器实际上决定了要提交事务，而且数据库2收到了提交请求。然而，协调器在将提交请求发送到数据库1之前崩溃了，因而数据库1不知道是提交还是中止。这个时候即使是超时也于事无补：如果数据库1在超时后单方面中止，那么最终它会与已经提交了的数据库2不一致。同样地，单方面提交是不安全的，因为另一个参与者可能已经中止了。

*图9-10 在参与者们投了“赞成“票后协调员崩溃了。数据库1不知道是应该提交还是中止。*

如果没有来自协调器的消息，参与者就无法知道是要提交还是中止。原则上，参与者可以相互交流，了解每个参与者是如何投票的并得出某种一致意见，但这并不是2PC协议的一部分。

2PC完成的唯一方法是等待协调器恢复。这就是为什么协调器在向参与者发送提交或中止请求之前必须将其提交或中止的决定写入磁盘上的事务日志的原因：当协调器恢复时，它将通过读取事务日志来确定所有不确定事务的状态。任何在协调器日志中没有提交记录的事务都将被中止。因此，2PC的提交点归结为协调器上的常规单节点原子提交。

#### 三阶段提交

两阶段提交被称为*阻塞*原子提交协议，因为2PC可能会陷入等待协调器恢复的状态。从理论上讲，使原子提交协议变得非阻塞是可能的，这样如果节点失效协议就不会被卡住。然而，在现实中实践它可不是那么简单。

作为2PC的替代方案，已经提出了一种称为三阶段提交（3PC）的算法。然而，3PC假设网络具有有界的延迟，节点具有有界的响应时间；在大多数有着无界网络延迟和进程暂停的实际系统中（参见第8章），它不能保证原子性。

一般来说，非阻塞原子提交需要一个完美的故障检测器——即一个可靠的机制来判断节点是否已经崩溃。在无界延迟的网络中，超时并不是可靠的故障检测器，因为即使没有节点崩溃，请求也可能因网络问题而超时。由于这个原因，2PC会继续使用，尽管已经知道问题是协调器失败。

### 分布式事务在实践中的应用

分布式事务，特别是那些通过两阶段提交实现的，声誉参差不齐。一方面，它们被视为提供了一种重要的安全保障，换做是其它手段是很难实现的；另一方面，它们因造成运营问题、性能损耗以及承诺超出它们所能提供的而饱受批评。许多云服务选择不实现分布式事务，因为它们产生了运营问题。

一些分布式事务的实现有着严重的性能损失——例如，MySQL中的分布式事务据说比单节点事务慢10倍以上，所以当人们建议不要使用它们的时侯也就不足为奇了。两阶段提交固有的性能损耗很大程度上是由于崩溃恢复所需的额外的磁盘强制写入（`fsync`），以及额外的网络往返。

然而与其完全放弃分布式事务，我们应该更详细地研究它们，因为我们可以从中吸取重要的经验教训。首先，我们应该准确地定义“分布式事务”的含义。两种完全不同类型的分布式事务常常被混为一谈：

However, rather than dismissing distributed transactions outright, we should examine them in some more detail, because there are important lessons to be learned from them. To begin, we should be precise about what we mean by “distributed transactions.” Two quite different types of distributed transactions are often conflated: 

*数据库内部的分布式事务*

一些分布式数据库(即在标准配置中使用复制和分区的数据库)支持在数据库节点之间的内部事务。例如，VoltDB和MySQL集群的NDB存储引擎具有这样的内部事务支持。在这种情况下，参与事务的所有节点都运行相同的数据库软件。

*异构分布式事务*

在异构事务中，参与者是两种或更多不同的技术：例如，来自不同供应商的两个数据库，甚至是非数据库系统，比如消息代理。横跨这些系统的分布式事务必须确保原子提交，即使系统从内部实现可能是完全不同的。

数据库内部的事务不必与任何其他系统兼容，因此它们可以使用任何协议并且可以应用特定于这种技术的优化。因此，数据库内部的分布式事务通常可以很好地工作。另一方面，跨越异构技术的事务更有挑战一些。

#### 只有一次的消息处理
Exactly-once message processing

异构分布式事务可以用强大的方式把不同的系统集成起来。例如，消息队列中的消息只有在处理消息的数据库事务成功提交的情况下才能被确认为已处理。这是通过在单个事务中原子地提交消息确认和数据库写入来实现的。有了分布式事务的支持，即使消息代理器和数据库是在不同设备上运行的两种无关的技术，也是可能的。

如果消息传递或是数据库事务失败的话，两者都会中止，因此消息代理器稍后可以安全地重新传递消息。因此，通过原子地提交消息及其处理过程的副作用，我们可以确保消息有效地只处理一次，即使它在成功之前需要几次重试。中止将撤销部分完成事务产生的任何副作用。

然而，只有受事务影响的所有系统都能够使用相同的原子提交协议，这样的分布式事务才是可能的。举个例子，假设处理消息的副作用是发送电子邮件，而电子邮件服务器不支持两阶段提交：如果消息处理失败并重试，电子邮件可能会发送了好几次。但是，如果处理消息的所有副作用在事务中止时可以回滚的话，那么处理步骤可以安全地重试，就好像什么都没有发生一样。

在第11章中，我们会再回到这个主题。让我们先看看原子提交协议，正是它使这样的异构分布式事务成为可能。

#### XA事务

X/Open XA（eXtended Architecture的缩写）是跨异构技术实现两阶段提交的标准。它在1991年引入并得到了广泛的实现：XA被许多传统的关系数据库（包括PostgreSQL、MySQL、DB2、SQL Server和Oracle）和消息代理（包括ActivaMQ、HornetQ、MSMQ和IBM MQ）支持。

XA不是网络协议——它只是用于与事务协调器接口的C语言API。也有用其它语言绑定到这个API的；例如，在Java EE应用程序的世界中，XA事务是用Java事务API（JTA）来实现的，Java事务API又由许多使用Java数据库连接（JDBC）的数据库的驱动程序和使用Java消息服务（JMS）API的消息代理的驱动程序支持。

XA假设你的应用程序使用网络驱动程序或客户端库与参与者数据库或消息传递服务进行通信的。如果驱动程序支持XA，这意味着它会调用XA API来确定一个操作是否应该是分布式事务的一部分——如果是的话，它会向数据库服务器发送必要的信息。驱动程序还暴露了回调接口，协调器可以通过回调要求参与者准备、提交或中止。

事务协调器实现了XA API。标准没有指明应该如何实现它，但在实践中，协调器通常只是一个库，加载到与发出事务的应用程序相同的进程中（而不是一个单独的服务）。它跟踪记录事务中的参与者，在要求他们准备（通过驱动程序的回调）之后收集参与者的响应，并使用本地磁盘上的日志来跟踪记录每个事务的提交/中止决策。

如果应用程序进程崩溃，或者执行应用程序的机器死机了，协调器也会随之崩溃。任何进入了准备但还未提交的事务的参与者都会卡在疑问状态。由于协调器的日志位于应用程序服务器的本地磁盘上，因此必须重启服务器，并且协调器库必须读取日志以恢复每个事务的提交/中止结果。只有在那时协调器才能使用数据库驱动程序的XA回调来要求参与者提交或中止，具体视情况而定。数据库服务器不能直接与协调器联系，因为所有通信都必须通过客户端库进行。

#### 疑问状态下占有锁

为什么我们这么在意一笔事务卡在疑问状态呢？系统的其他部分就不能继续工作，直接忽略那些最终将被清理掉的疑问状态事务吗？

问题在于锁定。正如在“提交读”一节中所讨论的，数据库事务通常对它们修改的任何行使用行级独占锁，以防止脏写。此外，如果你想要序列化的隔离，使用两阶段锁定的数据库也必须对事务读取的任何行使用共享锁（见“两阶段锁定（2PL）”一节）。

在事务提交或中止（如图9-9所示的阴影区域）之前数据库是无法释放这些锁的。因此，在使用两阶段提交的时侯，事务必须在疑问状态期间始终持有锁。如果协调程序已经崩溃，并且花了20分钟才能再次启动，那么这些锁将被持有20分钟。如果协调程序的日志由于某种原因完全丢失，则这些锁将被永远持有——或者直到管理员手动解决此情况为止。

当这些锁被持有时，其他事务都不能修改这些行。根据数据库的不同，其他事务甚至可能被阻止读取这些行。因此，其他事务不能简单地继续其业务——如果它们想访问同一份数据，它们会被阻塞。这可能导致应用程序的大部分功能在处于疑问状态的事务解决之前是不可用的。

#### 从协调器故障中恢复

理论上，如果协调器崩溃并重新启动，它应该可以从日志中干净地恢复状态，并解决任何处于疑问状态的事务。然而在实践中，存在疑问的孤立事务确实会发生——也就是说，无论出于什么原因，协调器都无法决定结果(例如，因为事务日志由于软件bug而丢失，或者数据被损坏)。这些事务无法自动解决，因此它们永远处于数据库中，持有锁并阻塞其他事务。

即使重启数据库服务器也不能解决这个问题，因为2PC的正确实现必须在整个重启期间也保留处于疑问状态事务的锁（不然就有违反原子性保证的风险）。情况相当棘手。

唯一的解决办法是由管理员手动决定是提交还是回滚事务。管理员必须检查每个处于可疑状态事务的参与者，判断是否有参与者已经提交或中止，然后将相同的结果应用于其他参与者。解决这个问题可能需要大量的手动工作，而且很有可能需要在严重的生产中断期间的高压力和时间压力下完成（不然的话，协调器为什么会处于如此糟糕的状态？）

许多XA实现都有一个名为*启发式决策*的“紧急逃生舱口”：允许参与者单方面决定中止或提交一个处于可疑状态的事务，而不需要协调器的明确决定。要明确的是，这里的*启发式*是*可能打破原子性*的委婉说法，因为它违反了系统在两阶段提交中的承诺。因此，启发式决策只是为了摆脱灾难性的情况，而不是为了一般性的使用。

#### 分布式事务的局限性

XA事务解决了让多个参与者数据系统相互一致的既真实又重要的问题，但正如我们所看到的，它们也带来了重大的运营问题。特别在于核心实现是，事务协调器本身就是一种数据库（在其中存储了事务结果），因此需要与任何其他重要数据库一样小心地处理：

* 如果协调器没有副本而只是在一台设备上运行，那么它是整个系统的一个单一失效点（因为它的失效会导致其他应用服务器阻塞在处于可疑状态事务持有的锁上)。令人惊讶的是，许多协调器实现在默认情况下并不是高度可用的，或者只有基本的复制支持。

* 许多服务器端应用程序都是用无状态模型（如HTTP所青睐的）开发的，所有的持久化状态都存储在数据库中，这样具有可以随意添加和删除应用服务器的优点。但是，当协调器是应用服务器的一部分时，它改变了部署的性质。突然之间，协调器的日志成为持久化系统状态的关键部分——与数据库本身一样重要，因为在崩溃后恢复处于可疑状态的事务需要协调器日志。这样的应用服务器就不再是无状态的。

* 由于XA需要与许多的数据系统兼容，因此它必然是最小公分母。例如，它不能检测不同系统之间的死锁（因为这需要一个标准化的协议，以便系统之间可以交换每个事务正在等待的锁的信息），而且它不适用于SSI（见“可序列化快照隔离（SSI）”一节），因为这需要一个协议来识别不同系统之间的冲突。

* 对于数据库内部的分布式事务（而不是XA），限制没有这么多——例如，分布式的SSI是可能的。然而2PC要成功提交事务，所有参与者都必须作出响应的问题依然存在。因此如果系统的*任何*部件坏了，事务也会失败。因此分布式事务具有*放大失效*的倾向，这与我们构建容错系统的目标背道而驰。

这些事实是否意味着我们应该放弃使几种系统相互一致的希望？不完全是——还有其他备选方案可以让我们实现同样的目标，但是不受异构分布式事务带来的痛苦。我们将在第11、12章中回到这些问题上。但是，首先，我们应该结束协商一致的这个主题。

### 容错协商一致

非正式地说，协商一致意味着让几个节点就某件事达成一致。例如，如果几个人同时尝试预订飞机上的最后一个座位，或是剧院中的同一个座位，或者尝试以相同的用户名注册一个帐户，那么可以使用协商一致的算法来确定这些互不兼容的操作中哪一个应该是赢家。

协商一致问题通常被形式化为：一个或多个节点会提出数个值，协商一致算法决定选择其中的一个值。在座位预订示例中，当多个客户同时试图购买最后一个座位时，处理客户请求的每个节点可能会提出它所服务的客户的ID，而决定指明这些客户中的哪一位获得了该座位。

在这种形式中，一致性算法必须满足以下性质：

一致同意

不会出现两个节点判断不同的情况。

完整性

没有节点判断两次。

有效性

如果节点判断值为*v*，那么同一个节点会提出值*v*。

终止性

每一个没有崩溃的节点最终都会得出判断的某个值。

一致同意与完整性属性定义了协商一致的核心理念：每个人都决定了同样的结果，而且一旦决定了，你不能再改变你的想法了。有效性属性的存在主要是为了排除琐碎的解：例如，你可以有一个无论提出什么，都始终决定值为`null`的算法；这个算法可以满足一致性和完整性属性，但不满足有效性属性。

如果你不关心容错，那么满足前三个属性很容易：你只需硬编码一个节点就可以成为“独裁者”，并让该节点做出所有决定。然而如果该节点失效，那么系统将无法作出任何决定。事实上，这就是我们在两阶段提交的情况下所看到的：如果协调器失败，处于疑问状态的参与者无法决定是提交还是中止。

终止性属性把容错理念形式化了。它本质上说，协商一致的算法不可能永远无所事事——换句话说，它必须推动事情进展。即使某些节点失效，其他节点也必须达成一致做出决定。（终止性是一种活跃度属性，而其他三种属性是安全属性——见“安全和活跃度”一节。）

协商一致的系统模型假设当节点“崩溃”的时候，它突然消失并且再也不会回来。（不是软件崩溃，而是假设发生了地震，包含节点的数据中心被山体滑坡摧毁。你必须假设你的节点被埋在30英尺的烂泥之下，永远不会恢复正常运行了。）在这种系统模型中，任何需要等待节点恢复的算法都无法满足终止性。尤其是，2PC不满足终止性的要求。

当然，如果所有节点崩溃，没有一个节点运行的话，那么任何算法都不可能决定任何事情。算法所能容忍的失效个数是有限度的：事实上可以证明，任何协商一致的算法都需要至少大多数节点才能正常工作，以确保终止性。大多数节点可以安全地构成仲裁团（见“读写仲裁”一节）。

因此，终止属性是受制于少于一半的节点是崩溃的或无法到达的假设。然而，绝大多数协商一致的实现都确保安全属性——一致同意性、完整性和有效性——是始终得到满足的，哪怕大多数节点失败或存在严重的网络问题。因此，大规模的中断可以阻止系统处理请求，却无法破坏协商一致系统，使之做出无效的决定。

绝大多数协商一致的算法假设系统中没有拜占庭故障，如我们在“拜占庭故障”一节中所讨论的那样。也就是说，如果一个节点没有正确地遵守协议（比如，它向不同的节点发送互相矛盾的消息)，它会破坏协议的安全属性。只要有拜占庭故障的节点少于总数的三分之一，我们就有可能使协商一致算法对拜占庭故障足够健壮，但是由于篇幅问题，我们在本书中不会详细讨论这些算法。

#### 协商一致算法与全序广播

最著名的容错协商一致算法是视图标记复制（VSR）、Paxos、Raft和Zab。这些算法之间有不少相似之处，但它们并不相同。在这本书里，我们不会详细地介绍这些不同的算法：只要了解它们的一些共有的高层次想法就足够了，除非你自己正在实现一个协商一致系统（也许这样做并不可取——很难）。

这些算法中的大多数实际上不直接使用这里描述的形式模型（建议并决定单个值，同时满足一致同意、完整性、有效性和终止性）。相反，他们决定一系列的值，这使得它们成为全序广播算法，如我们之前在本章所讨论的（见“全序广播”一节）。

记住，全序广播要求消息以相同的顺序准确地只给所有节点传递一次。想一想，这相当于执行了好几轮协商一致：在每一轮中，节点提出它们想要发送的下一个消息，然后决定下一个按全序发送的消息。

因此，全序广播相当于重复几轮的协商一致（每个协商一致的决定对应一次消息传递）

So, total order broadcast is equivalent to repeated rounds of consensus (each consensus decision corresponding to one message delivery):

* 由于协商一致的一致同意性质，所有节点都决定按照相同的顺序传递相同的消息。

* 由于完整性，消息不会重复。

* 由于有效性，讯息不会被破坏，也不会凭空捏造。

* 由于终止性，消息不会丢失。

视图标记复制、Raft和Zab直接实现了全序广播，因为这比重复许多轮的、一次一个值的协商一致更有效率。在Paxos中，这种优化称为多Paxos。

#### 单主机的复制与协商一致

在第5章中，我们讨论了单主机的复制（见“主机与从机”一节），它所有的写入都发生在主机上并把它们以同样的顺序应用到从机上，从而使副本保持最新。这不就是全序广播吗？为什么在第5章我们没有担心协商一致问题？

答案在于主机是如何被选出来的。如果你的运营团队人员手动选择并配置了主机，实际上你有一个独裁的“协商一致算法”：只允许一个节点接受写入(即，对复制日志中的写入顺序作出决定)，如果该节点发生故障，系统将无法写入，直到运维人员手动配置另一个节点为主机为止。这种系统在实践中可以正常工作，但是因为它需要人们的干预才能取得进展，它不满足协商一致中的终止性，。

一些数据库自动执行主机选举和故障转移，一旦旧主机失效就提升一个从机成为新主机（见“处理节点中断”一节）。这使我们更接近于容错的全序广播，从而解决协商一致问题。

然而，这里有个问题。我们之前讨论过裂脑的问题，而且说所有的节点都需要同意谁是主机——否则两个不同的节点都相信自己才是主机，并因此使数据库进入一个不一致的状态。因此，我们需要协商一致算法以选出一个主机。但是，如果这里描述的协商一致算法实际上是全阶广播算法，而全序广播与单主机复制类似，而单主机复制需要一台主机，那么。。。

看上去要选出一台主机，我们首先需要一台主机。要解决协商一致，首先要解决协商一致。我们怎么才能摆脱这个难题呢？

#### 时期编号与仲裁

到目前为止讨论的所有协商一致的协议都在内部使用了某种形式的主机，但它们并不保证主机是独一无二的。取而代之的是，它们可以提供一个较弱的保证：协议定义了一个*时期编号*（在Paxos中叫做*选票编号*，在视图标记复制中叫做*视图编号*，在Raft中叫做*术语编号*）并保证在每个时期里，主机是唯一的。

每一次当前主机被认为已经失效了，就会在节点间开始投票选举新的主机。选举给定了一个递增的时期编号，从而使时期编号完全有序且单调增加。如果两台不同的主机在两个不同的时期发生了冲突（也许是因为前一台主机实际上并没有失效），那么具有较高时期编号的主机占上风。

在允许主机做出任何决定之前，必须先检查是否有其他具有较高时期编号的主机，如果存在它可能会做出相互矛盾的决定。一台主机怎么知道它没有被另一个节点驱逐呢？回想一下“真理是由多数人定义的”一节：一个节点并不一定相信自己的判断——仅仅因为一个节点认为它是主机，这并不一定意味着其他节点接受它作为它们的主机。

取而代之的是，它必须从由众多节点构成的仲裁团那里收集选票（见“读写的仲裁”一节）。对于主机想要做出的每一个决定，它都必须将建议的值发送给其他节点，并等待一定数量仲裁团的节点对建议做出响应。仲裁团通常，但并非总是，由大多数节点组成。节点只有在没有意识到有任何其他具有更高时期编号的主机时才投票支持提议。

因此有了两轮投票：一次选举一台主机，另一次投票表决主机的提议。至关重要的认识是这两次投票的仲裁团必须重叠：如果对提议进行的表决成功了，那么投票赞成该提议的节点中至少有一个也必须参加了最近对主机的选举。因此，如果对提议的表决没有显示出任何具有更高编号的时期，当前主机就可以得出结论，没有发生过任何具有更高时期编号的主机选举，也因此确保了它仍然保有主机地位。之后它可以安全地决定建议的值。

这个投票过程从表面上看类似于两阶段提交。最大的区别在于，在2PC中协调员不是通过选举产生的；而且容错协商一致算法只需要来自大多数节点的投票，但是2PC需要每个参与者投赞成票。此外，协商一致的算法定义了一个恢复流程，通过这个流程，节点可以在选出新的主机后进入一致的状态，从而确保安全特性始终得到满足。这些差异是协商一致算法正确性和容错的关键。

#### 协商一致的限制

对于分布式系统来说，协商一致算法是一个巨大的突破：它们给其他一切都不确定的系统带来了实实在在的安全特性（一致同意、完整性和有效性），而且它们仍然是容错的（只要大多数节点还在正常工作并且可以到达，就可以取得进展)。它们提供全序广播，因此它们还可以容错的方式实现线性化的原子操作（见“利用全序广播实现线性化存储”一节）。

然而，并不是所有的地方都在使用它们，因为它们的好处是有代价的。

提议在决定之前由节点投票的流程是一种同步复制。正如“同步 vs 异步复制”一节，数据库通常被配置为使用异步复制。在这种配置中，一些已经提交了的数据可能会在故障转移中丢失——但是为了更好的性能，许多人选择接受这种风险。

协商一致系统总是需要严格多数（的节点）才能运作。这意味着你需要至少三个节点才能容忍一个失效节点（三个节点中的其余两个构成了多数），或者至少需要五个节点来容忍两个失效节点（五个节点中的其余三个节点占多数）。如果网络故障把某些节点与其他节点隔开了，那么只有网络中由多数节点构成的部分才能够取得进展，而剩余部分将被阻塞（另见“线性化的代价”一节）。

绝大多数协商一致的算法假定参与投票的节点数量是固定的，这意味着你不能添加或删除集群中的节点。协商一致算法的动态成员扩展允许集群中的节点集合随时间而变化，但与静态成员算法相比，现在对它们的理解要少得多。

协商一致的系统通常依靠超时来检测失效的节点。在具有高度可变网络延迟的环境中，特别是在地理位置上分散的系统中，经常会发生由于暂时的网络问题导致节点错误地认为主机失效的情况。虽然这样的错误不会损害安全性，但是频繁的主机选举会导致糟糕的性能，因为系统最终可能会花费更多的时间来选择主机，而不是做任何有用的工作。

有的时候，协商一致算法对网络问题特别敏感。例如，Raft被证明有令人不快的极端场景：如果整个网络都可以正常工作，而只有一个特定网络链路始终不可靠，Raft可能会陷入主机地位在两个节点之间不断地变换，或者当前主机不断被迫辞职的情况，导致系统实际上从未取得进展。其他协商一致的算法也有类似的问题，而针对不可靠网络设计更健壮的算法仍然是一个有待研究的问题。

### 成员关系与协调服务

像ZooKeeper或者etcd这样的项目通常被描述为“分布式键值对存储”，或者是“协调和配置服务”。这种服务的API看起来很像数据库的API：你可以读取和写入给定键的值，并对键进行遍历。那么如果它们基本上算是数据库的话，那么为什么它们要竭尽全力实现一个一致的算法呢？是什么使它们不同于其他类型数据库的？

为了理解这一点，简要地探讨一下像ZooKeeper这样的服务是如何使用的会很有帮助的。作为一名应用程序开发者，你很少需要直接使用ZooKeeper，因为它实际上并不适合作为一个通用数据库。更有可能的是，你将通过其他项目间接依赖它：例如，HBase、Hadoop YARN、OpenStack Nova以及Kafka都依赖后台运行的ZooKeeper。这些项目从它那里得到了什么？

ZooKeeper和etcd被设计用来保存少量的数据，这些数据可以完全存储在内存中（尽管为了持久性它们仍然会写入磁盘）——所以你不会想把应用程序所有的数据都存在这里的。这些少量的数据使用容错的全序广播算法在所有节点上复制。如前面所描述的，全序广播正是数据库复制所需的：如果每一条消息表示了对数据库的一次写入，那么以相同的顺序应用相同的写入可以使副本彼此保持一致。

ZooKeeper是以谷歌的Chubby锁服务为模型的，不仅实现了全序广播（因而实现了协商一致），还实现了一组有趣的其他特性，它们在构建分布式系统时特别有用：

*线性化原子操作*

使用原子比较后设置操作，你可以实现一个锁：如果几个节点同时尝试执行相同的操作，那么其中只有一个节点会成功。协商一致协议保证操作是原子的和线性化的，即使节点失效，或是网络在任何时间点中断。分布式锁通常实现为租约，它有一个过期时间，以便在客户端失效时最终被释放（见“进程暂停”一节）。

*全序操作*

正如“主机与锁”一节中所讨论的，当某些资源受到锁或租约的保护时，你需要栅栏令牌来防止客户端在进程暂停的情况下发生冲突。栅栏令牌是锁每次被获取时单调增加的某个数字。ZooKeeper通过对所有操作进行全序排序、并给每个操作一个单调增加的事务ID（`zxid`）和版本号（`cversion`）来提供它。

*失效检测*

客户端与ZooKeeper服务器之间维护一个长时间会话，客户端和服务器定期交换心跳以检查对方是否仍然运行着。即使连接暂时中断，或者ZooKeeper节点失效，会话仍然处于活动状态。然而，如果心跳停止的时间超过了会话的超时时间，ZooKeeper会宣布会话结束。会话所持有的任何锁都可以配置为会话超时之后自动释放（ZooKeeper把这些叫做*临时节点*）。

*变更通知*

一个客户端不仅可以读取由另一个客户端创建的锁和值，而且还可以监视它们的变更。因此，客户端可以知道另一个客户端什么时候加入集群（基于它写入ZooKeeper的值），或者是另一个客户端是否失败（因为它的会话超时，它的临时节点消失了）。通过订阅通知，客户端避免了频繁轮询来查找变更。

在这些特性中，只有线性化原子操作才需要协商一致。然而，正是这些特性的结合使得像ZooKeeper这样的系统对于分布式协调这么有用。

#### 给节点分配任务

ZooKeeper/Chubby模型运行良好的一个例子是，如果你有几个进程或服务的实例，而其中一个实例需要被选为主机或主要实例。如果主机失效，其他节点中的一个将接管。当然，这对于单主机数据库当然很有用，对于作业调度器和类似的有状态系统也是一样的。

另一个例子是，当你有一些分区资源(数据库、消息流、文件存储、分布式参与者系统等)，并且需要决定将哪个分区分配给哪个节点的时候。当新节点加入集群时，需要将一些分区从现有节点移动到新节点，从而重新平衡负载（参见“重新平衡分区”一节）。当节点被移除或失败时，其他节点需要接管失效节点的工作。

这类任务可以通过正确使用ZooKeeper中的原子操作、临时节点和通知来实现。如果操作正确，这种方法允许应用程序在不需要人工干预的情况下自动从故障中恢复。这并不容易，尽管出现了Apache Curator这样的库，这些库在ZooKeeper客户端API的基础之上提供了更高级别的工具——但它仍然比从零开始实现必要的协商一致算法要好得多，从零开始成功的记录太少了。

应用程序最初可能只在单个节点上运行，但最终可能会增长到数千个节点。试图在这么多节点上执行投票将是非常低效的。取而代之的是，ZooKeeper在固定数量的节点（通常是三个或五个）上运行，在这些节点中执行多数表决的同时支持潜在的大量客户端投票。因此，ZooKeeper提供了一种将一些协调节点（协商一致、操作排序和故障检测）的工作“外包”到外部服务的方法。

通常情况下，由ZooKeeper管理的数据变化得很慢：它呈现的是“运行在10.1.1.23上的节点是分区7的主机”之类的信息，这种信息会在几分钟或几小时的时间尺度上发生变化。它不适于存储应用程序的运行时状态，这些可能每秒更改数千次甚至数百万次。如果需要把应用程序状态从一个节点复制到另一个节点，可以使用其他工具（例如Apache BookKeeper）。

#### 服务发现

ZooKeeper、etcd和Consul也经常用于服务发现——也就是，找出需要连接到哪个IP地址才能到达特定的服务。在云数据中心环境中，虚拟机经常会不断地上线下线，你通常无法提前知道服务的IP地址。相反的，你可以配置你的服务，以便在它们启动时将其网络端点注册到服务注册表中，然后在那里可以被其他服务找到。

然而还不是很清楚服务发现实际上是否需要协商一致。DNS是查找服务名称IP地址的传统方法，它使用多层缓存来实现良好的性能和可用性。来自DNS的读取绝对不是可线性化的，如果DNS的查询结果有点陈旧，通常不会认为是有问题的。更重要的是，DNS是可靠的可用的以及对网络中断的健壮性。

虽然服务发现不需要协商一致，但主机选举需要。因此，如果你的协商一致系统已经知道谁是主机，那么也可以使用这些信息来帮助其他服务发现主机是谁。为此，一些协商一致的系统支持只读缓存副本。这些副本异步地接收协商一致算法所有决策的日志，但不积极参与投票。因此，它们能够为不需要是线性化的读取请求提供服务。

#### 成员服务

ZooKeeper和同类产品可以被看作是成员服务研究悠久历史的一部分，可以追溯到20世纪80年代，并且对于构建高度可靠的系统，比如空中交通管制，是非常重要的。

成员服务确定哪些节点是当前激活并且活跃的集群成员。正如我们在第8章中所看到的，由于没有限制的网络延迟，不可能可靠地检测另一个节点是否已经失效。然而，如果你将故障检测与协商一致性相结合，关于哪些节点可以被认为是活跃的而哪些不能，节点之间是可以达成一致的。

这样还是会发生节点被错误地以协商一致的方式宣布失效的情况，即使节点实际上还可用。但是对于一个系统来说，就哪些节点构成当前成员的问题达成一致是非常有用的。例如，选择一个主机可能就是意味着选择当前成员中编号最低的一个，但是如果不同的节点对当前成员都是谁有不同的意见的话，这种方法就行不通。

## 小结

在本章中，我们从几个不同的角度考察了一致性和协商一致的主题。我们深入研究了线性化，一种流行的一致性模型：它的目标是使复制的数据看起来好像只有一个副本，并使所有操作都原子化。虽然线性化很吸引人，因为它很容易理解——它使数据库在单线程程序中表现得像一个变量——但是它有慢的缺点，特别是在网络延迟较大的环境中。

我们还探讨了因果关系，它对系统中的事件（什么发生在什么之前，基于因果的）带来了顺序。与线性化将所有操作放在一个单一的、有序的时间线中不同，因果关系为我们提供了一个较弱的一致性模型：有些东西可以是并发的，因此版本历史就像一个具有分支与合并动作的时间线。因果一致性没有线性化导致的协调开销，对网络问题的敏感性也要小得多。

然而，即使我们捕获因果排序（比如使用兰波特时间戳），我们也发现有些事情不能以这种方式实现：在“时间戳排序是不够的”一节中，我们考虑了确保用户名是唯一的并且拒绝对同一个用户名进行的并发注册的例子。如果一个节点要接受注册，某种程度上它需要知道另一个节点没有并发地在注册相同名称。这个问题带来协商一致问题。

我们看到，达成协商一致意味着以所有节点都同意的方式所决定内容，而且这样的决定是不可撤销的。通过深入挖掘，我们发现各种各样的问题实际上都可以归结为协商一致问题，并且是等价的(也就是说，如果你有其中一个问题的解决方案，你可以很容易地把它转化为其它问题的解决方案)。类似的等价问题包括：

*线性化比较后设置寄存器*

寄存器需要根据其当前值是否等于操作中给定的参数来原子地决定是否设置其值。

*原子事务提交*

数据库必须决定是否提交或中止分布式事务。

*全序广播*

消息系统必须决定传递消息的顺序。

*锁与租约*

当几个客户端争抢锁或租约时，锁决定哪个客户端成功地获得了它。

*成员服务/协调服务*

给定故障检测器（例如超时），系统必须决定哪些节点是活动的，哪些节点因为它们的会话超时于是应该被认为失效了。

*唯一性约束条件*

好几个事务同时尝试使用相同的键创建相互冲突的记录时，约束条件必须决定哪一项记录是允许的而哪些因为违反约束条件而应该失败。

如果你只有单个节点，或者你愿意把决策功能分配给单个节点，那么所有这些都很好理解。这就是单主机数据库中发生的事情：所有决策的权力属于主机，这就是为什么这样的数据库能够提供线性化的操作、唯一性约束条件、全序的复制日志等等。

然而如果单主机失效了，或者网络中断使主机无法访问，这样的系统将无法取得任何进展。有三种处理这种情况的方法：

1. 等待主机恢复，接受系统将在此期间阻塞的事实。许多XA/JTA事务协调器选择这样做。这种方法不能完全解决协商一致问题，因为它不满足终止属性：如果主机不恢复，系统可能永远阻塞住。

2. 手动故障迁移：通过人为选择新主机节点，并重新配置系统使用新主机。许多关系型数据库都采用这种方法。这是一种由“上帝的行为”达成的协商一致——由计算机系统之外的运维人员来做出决定。故障转移的速度受运维人员行动速度的限制，这通常比计算机慢。

3. 使用算法自动地选择新的主机。这种方法需要协商一致算法，并且最好使用一种经过验证的算法来正确地处理不友好的网络环境。

虽然单主机数据库可以提供线性化，无需在每次写入时执行协商一致的算法，然而它仍然需要协商一致才能维持它的主机地位以及主机地位改变的情况。因此从某种意义上说，只有单台主机“一意孤行”：仍然需要达成协商一致，只是发生在不同的地方，而且没有那么频繁。好消息是，容错算法和协商一致的系统已经有了，我们在这一章里对它们进行了简要的讨论。

像ZooKeeper这样的工具在提供应用程序可以使用的“外包”协商一致算法、故障检测和成员服务诸多方面发挥着重要作用。它不容易使用，但是比尝试开发自己的算法要好得多，这些算法能够经受住第8章中讨论的所有问题的考验。如果你发现自己想要做一件可以转化为协商一致的事情，而且你希望它是可以容错的，那么最好使用诸如ZooKeeper这样的东西。

然而并不是每个系统都必然需要协商一致：例如，无主机和多主机复制系统通常不使用全局协商一致。在这些系统中发生的冲突（见“处理写入冲突”一节）是不同主机之间没有进行协商一致的结果，但也许这是可以接受的：也许我们只需要在没有线性化的情况下进行处理，并且学会更好地处理具有分支和合并动作版本历史的数据。

本章引用了分布式系统理论的大量研究成果。虽然理论论文和证明并不总是容易理解，而且有的时侯也会做出不切实际的假设，但是它们为这一领域实际工作提供的信息是非常有价值的：它们帮助我们解释什么可以做什么不能做，并帮助我们找到分布式系统经常存在缺陷的违反直觉的方式。如果你有时间的话，这些参考资料是很值得探索的。

我们来到了这本书第二部分的结尾，在这一部分我们讨论了复制（第5章）、分区（第6章）、事务（第7章）、分布式系统故障模型（第8章）以及最后的一致性和协商一致（第9章）。现在我们已经奠定了坚实的理论基础，在第三部分中，我们将再次转向更实际的系统，并讨论如何用异构的基础模块构建强大的应用程序。