# 第一章 可靠的、可扩展的、可维护的应用程序

*Internet是如此得成功以至于绝大多数人们以为它就像太平洋一样是一种自然资源，而不是人们动手建立起来的。上一次出现这种有着如此规模却又近乎完美的技术是什么时候来着？*

摘自Alan Kay在2012年Dr Dobb期刊中的访谈

今天许多的应用程序不是*计算集约*而是*数据集约型的*了。对于这些应用程序，CPU的性能很少是瓶颈了——更大的问题是数据的数量、数据的复杂度以及数据变化的速度。

一个数据集约型的应用程序通常由满足一般性需求功能的标准构建模块构建的。举个例子来说，许多应用程序都需要：
* 存储数据从而它们自己，或者其他应用程序可以稍后使用（数据库）
* 存储代价高的运算结果，从而提升访问速度（缓存）
* 允许用户通过各种方式进行关键词搜索或者过滤（搜索索引）
* 向其它进程发送消息，进行异步处理（流处理）
* 周期性的处理大量的累积数据（批处理）

如果这些都听起来相当的显而易见，这正是因为这些*数据系统*是一种成功的抽象：我们一直使用着它们却没有思考太多。构建应用程序的时候，绝大部分工程师不会想着从头开始写一个新的数据存储引擎，毕竟数据库是可以完美胜任这项工作的。

但是现实可没有这么简单。因为不同的应用程序有着不同的需求，于是也就有了许多特性各不相同的数据库系统。同样也有许多种方式实现缓存，简历搜索索引，等等等等。构建应用程序的时候，我们仍然需要搞清楚哪些工具、哪些解决方式对于手边的任务是最合适的。当单一工具无法解决问题的时候，组合不同的工具本身就是难题。

这本书既讨论数据系统的原则也讨论实用性的问题，以及你可以如何使用它们来构建数据集约型的应用程序。我们将探索不同工具之间的共性、差别、以及它们是如何实现不同特性的。

在这一章的开始，我们将探索我们尝试实现的可靠的、可扩展的、可维护的数据系统
的基础部分。我们将阐明它们的含义，提炼一些思考它们的方式，并对后续章节用到的基础知识一一复习。而在稍后几章我们将逐一解析在设计数据集约型应用时那些需要考虑到的设计决定。

## 想想数据系统

我们通常认为数据库、队列、缓存等等是非常不同类别的工具。虽然表面上数据库与消息队列有一些类似——两者都在一段时间内储存数据——然而它们有着非常不同的访问模式，这意味着非常不同的性能特性，并因此有着非常不同的实现。

那我们为什么要把它们都归结为*数据系统*呢？

最近一些年出现了许多新的数据存储与处理工具。它们针对各种各样不同的用例优化，而不再适合归结为传统的一类。比如说，现在有了也可以用作消息队列的数据存储（Redis），也有了类似数据库一样持久性保证的消息队列（Apache Kafka）。这些类别之间的边界渐渐模糊起来。

其次，越来越多的应用程序现在有着越来越多越来越广的需求，以至于单一工具不再能满足所有的数据处理以及储存需求。取而代之的是这些工作被拆分成了许多小任务，每一个任务都*能*被单一工具有效的处理，而应用程序代码把这些工具连在了一起。

假如说你有一个应用程序管理的缓存层（借助Memcached或者类似工具），或者一个与主数据库分离的全文搜索服务器（比如Elasticsearch或者Solr），一般地保证缓存、索引与主数据库同步是应用程序代码的责任。图1-1大致给出了这也许是什么样的（我们会在稍后章节详细讨论）

*图1-1 一种结合了数个模块的数据系统架构*

为了提供服务而合并数个工具时，服务的接口或者是应用编程接口（API）通常会向客户端隐藏这些实现细节。这时事实上你已经用较小的通用模块创建了一个新的、有着特定用途的数据系统。你合成的数据系统也许可以保证，比如说，缓存在被写入时将被正确的校验、更新从而客户端可以得到持续一致的结果。这时你也不再只是一个应用程序开发者，同时也是一个数据系统设计者了。

如果你现在正在设计一个数据系统或者服务，一大堆复杂的问题会冒出来。你将如何保证数据的正确性与完整性，即使在系统内部出错的情况？你将如何向众多客户端提供持续一致的良好性能，即使部分系统性能降级？你将如何扩展系统以应对负载上升？对于服务来说一个好的API应该是什么样的？

有许多因素会影响一个数据系统的设计，它包括了参与者的技能与经验，对遗留系统的依赖程度，产品交付的时间表，所在组织对于不同种类风险的忍耐度，各种监管约束等等。这些因素都取决于当前的情况。

在这本书里，我们聚焦于三个方面的考虑，它们对大多数软件系统都非常重要：

*可靠性*

即使不幸遇到错误（硬件或者软件错误，甚至认为错误）系统应当可以继续正常工作（以期望的性能水平执行正确的功能）。详见“可靠性”。

*可扩展性*

随着系统（在数据量，通讯量，或者复杂度方面）增长，应当有合理的方法来应对。详见“可扩展性”。

*可维护性*

随着时间的推移，许多不同的人将在系统上工作（工程方面与运营方面，双方都维护当前的功能并尝试使系统适应新的用例），他们应当可以富有成效地一起工作。详见“可维护性”。

这几个词经常在不清楚它们确切含义的情况下被用来用去。从值得思考的工程学角度出发，在这一章余下的部分我们将探索关于可靠性、可扩展性、可维护性的思维方式。接着，在后续的章节中，我们会看到实现这些目标的众多技术手段、架构以及算法。

## 可靠性

每个人对于一个东西可靠与否的含义都有直观的认识。对于软件来说，典型的期望包含：

* 应用程序可以执行用户期望的功能。
* 它容许用户犯错，或者以意想不到的方式使用。
* 对于必要的用例，在期望的负载与数据量场景下它的性能是足够好的。
* 系统可以阻止任何未授权的访问和滥用。

如果所有这些加起来意味着“工作正常”，那么我们可以理解*可靠性*，大概，意味着“可以持续正常工作，哪怕出错的情况下”。

这些出错的情况被叫做*故障（faults）*，那些预知并可以应对故障的系统被叫做*可容错的*或是*弹性的*。前一个术语有点误导人：它暗示着我们可以构建一个容忍各种可能故障的系统，而事实上这不太切实际。假如整个地球（还有上面所有的服务器）都被一个黑洞吞噬了，要容忍这样一个故障就要求在太空中进行网络托管——祝你在申请经费的时候好运。因而只针对特定类型故障进行容错才是合理的。

注意，故障并不是崩溃。故障通常被定义为系统的某个组件没有按照规范执行，而崩溃是指系统作为一个整体不能再向用户提供必要的服务。把故障的可能性降到零是不可能的；于是通常会设计容错机制从而防止故障引起崩溃。我们会在这本书中介绍集中用不可靠的部件构建可靠系统的技巧。

与我们的直觉恰恰相反的是在这些可容错的系统中，通过有意触发故障*增加*故障率是合理的——比如说在毫无警告的情况下随机杀死一些独立进程。许多严重的错误实际上都是因为差劲的错误处理导致的；通过有意诱发故障，确保容错机制持续地应用与测试，从而在故障真正发生的时候确信它们可以被正确的处理。Netflix公司的工具Chaos Monkey就是这种方式的一个例证。

虽然相对于预防故障我们一般更倾向容错，也有一些例证证明预防更好（比如，根本无法解决故障）。安全问题就是这样一个经典场景，例如：如果攻击者攻陷了系统并获取了敏感信息，事件的影响不能完全消除。然而本书主要解决那些可以修正的故障，比如接下来几节介绍的。

### 硬件故障

每当我们想到系统崩溃的原因，首先想到的就是硬件故障。硬盘崩溃，内存错误越来越多，停电，谁谁谁拔错了网线。任何一个曾经与大型数据中心打过交道的人都会告诉你当机器很多的时候这种事情时时刻刻在发生。

根据报导，硬盘的平均报废时间在10到50年之间。于是，在一个有着10000万个硬盘的存储集群中，我们应当能预见到每一天都应该会有一块硬盘报废。

我们的第一反应通常是为个别硬件模块增加冗余度，从而减少系统的崩溃率。硬盘也许会被设置在磁盘冗余阵列（RAID）中，服务器也许有两个电源以及支持热插拔的CPU，而数据中心也许备有电池和柴油发电机作为备用电源。当一个组件崩溃的时候，冗余的组件可以顶替它的位置好让我们更换它。这种手段无法完全预防硬件问题引发崩溃，但是它很容易理解，并且能很容易的让一台机器持续不下线运转很多年。

直到如今，硬件模块冗余对于绝大部分应用来说都是足够的，毕竟它使一台机器完全崩溃的可能相当低。只要你可以在一台新机器上快速恢复一个备份，崩溃导致的下线时间对于绝大部分应用来说都不会是灾难性的。因而，只有一小部分应用需要多设备冗余，因为这些应用对高可用性的要求是绝对必须的。

然而，随着数据量以及应用计算能力要求的增加，越来越多的应用开始使用大量的设备，同时硬件故障率也在成比例的增加。不仅如此，在一些云平台例如亚马逊云计算服务（AWS），虚拟机实例无预警下线是非常平常的事情，因为平台被设计优先考虑灵活性与弹性，而不是单台设备的可靠性。

于是现如今的发展朝着系统可以容许失去所有设备的方向去了，它优先利用软件故障容错技术，或者再加上硬件冗余。这样的系统也有运行优势：一个单服务器系统需要为设备重启（比如为操作系统打安全补丁）计划下线时间，然而一个可以容许设备崩溃的系统可以每次只更新一个节点，而避免了整个系统下线（*滚动升级*，详见第四章）。

### 软件错误

我们通常认为硬件故障很随机，并且互不相关：一台设备的磁盘即将崩溃并不意味着另一台设备的磁盘马上也要崩溃。或许勉强有些联系（比如说由同一个问题导致的，例如服务器架的温度），但是一大部分硬件模块同时崩溃可不太可能。

另一类故障是发生在系统内部的错误。这类故障更难预知，并且由于它们跨节点相互关联，相比于没有关联的硬件故障它们可能导致更多的系统崩溃。例证包括：

* 一个软件错误，当给定一个特定的错误输入时该错误导致每一个应用服务器实例崩溃。举个例子，2012年6月30日的闰秒由于Linux内核的错误导致许多应用同时挂起。
* 一个失控的进程消耗光了某种共享资源——CPU时间，内存，硬盘空间，或者网络带宽。
* 一个（数据）系统以来的服务越来越慢，甚至变得没有相应，亦或者开始返回破损的响应。
* 级连失效，指一个组件的小故障引发了另一个系统的故障，之后又顺带引发更多故障

导致这些软件故障的错误通常潜伏了很长一段时间，直到被一组不寻常的环境设置触发。在这样的环境中，软件系统对于所在环境进行了某种假设——虽然这些假设通常是对的，可由于某些原因它最终不再是对的了。

软件中的系统级别故障没有快速的解决方案。这里有很多有用的小技巧：仔细思考系统中的假设与交互；覆盖完全的测试；流程隔离；允许进程崩溃、重启；对实际生产系统进行测量、监控并分析系统行为。如果期望系统可以提供某种保证（比如，在一个消息队列中，送出的消息数量总是等于接收的消息数量），那么它可以在运行时经常检查自己，并且在有出入的时候发送警报。

### 人为错误

人设计并构建软件系统，保证这些系统运行的运营者也是人。即使是出自好心，众所周知人是不可靠的。举例来说，研究表明在大型互联网服务中离线的最重要的原因是由于运营者配置错误，而硬件故障（无论是服务器还是网络）仅占所有离线原因的10%到25%。

如何让我们的系统即使在不可靠的人参与情况下更加可靠？好的系统通常结合一下几个方法：

* 设计系统时将错误发生的可能性降到最低。比如，良好设计的抽象、API和管理接口使得做“正确的事情”更容易同时不鼓励“错误的事情”。然而如果接口过于严格人们倾向于绕过它们，忽视它们的好处，这是一个非常难掌控的平衡。
* 将人们最容易弄错的地方与最可能导致崩溃的地方解耦。尤其是应该提供完整功能的非生产性沙盒环境使得人们可以安全地使用真实数据进行探索和实验，却不影响真实用户。
* 在所有级别层面进行覆盖完全的测试，包括单元测试、整个系统的集成测试以及人工测试。自动化测试被广泛应用，充分理解，尤其在覆盖那些正常操作很难触及的边缘用例时非常有价值。
* 允许快速简便地从人为错误中恢复，从而使诸如崩溃的影响降到最低。举个例子，允许快速回滚配置，渐渐推送更新（从而任何意外的错误都只影响一小部分用户），提供工具重新计算数据（在发现旧运算结果不正确的情况下）。
* 建立详细和清晰的监控，例如性能指标和故障率。在其它工程学科它被称作遥测。（当火箭离开地面，遥测对于追踪以及理解失败原因都至关重要。）监控可以尽早告知我们警告信号，允许我们检查情况是否违反了任意假定或者限制 。发生问题的时候，指标对于诊断问题来说是无价之宝。
* 采纳好的管理经验——一个复杂、重要的方面，同时也超出了本书的范围。

### 可靠性到底有多重要？

可靠性可不只是涉及到核电站和空管系统——我们也期望许多一般性的应用也可以很可靠的工作。商业应用中的错误会导致效率降低（如果数据不正确还会有法律风险），电子商务网站下线会导致巨额的营收损失以及声誉的丧失。

即使是在那些“非关键”应用中我们对于我们的用户也有责任。想象一对夫妻把他们孩子的照片视频都存在了你的照片应用中。一旦数据库损坏他们会怎么想？他们会知道如何从悲愤中恢复数据么？

有时我们也许会牺牲可靠性换取更低的开发成本（比如，为未证实的市场开发原型产品），或运营成本（比如，一个利润微薄的服务）——但我们应当非常清楚我们为什么在偷工减料。

## 可扩展性

即使一个系统今天工作正常，也不代表未来它也一定工作正常。性能降低一种常见的原因就是负载的增加：也许同时使用系统的用户从10000增长到了100000，或是从一百万增常到了一千万。或许它目前处理的数据量比先前大了很多。

扩展性是我们用来描述系统应对负载增长能力的术语。然而请注意，它并不是一个单维度的系统标签：说“X是可扩展的”或者“Y不支持扩展”是没有意义的。而应该是说，讨论扩展性意味着考虑诸如“如果系统以特定的方式增长，我们有哪些应对这种增长的选择？”或者“我们如何增加系统的运算能力来应对增加了的负载？”这样的问题。

### 描述负载

首先，我们需要言简意赅地描述系统当前的负载；然后才能开始讨论增长性的问题（如果负载翻倍会发生什么？）。负载可以用一系列数字来描述，我们把他们成为*负载参数*。最佳参数的选择取决于你系统的架构：对于网络服务器来说可以是每秒请求数，对于数据库来说可以是读写比，对于聊天室来说可以是同时在线人数，对于缓存来说可以是命中率，等等等等。也许一般的情况对你来说就是最重要的，也许就是那一小部分极端场景构成了你系统的瓶颈。

为了更具体的解释这个概念，我们拿Twitter举例，使用2012年11月份发布的数据。Twitter两个主要的操作是：

*发推文*

用户可以向他的粉丝发布一条新消息（平均每秒4600个请求，峰值则超过每秒12000个请求）。

*主页时间线*

用户可以浏览他关注的人们发布的推文（每秒300000个请求）。

每秒只是处理12000次写入（发推的峰值数据）是相当容易的。然而，Twitter可扩展性的挑战并不主要是推文数量，而是*扇出*——每个用户关注许多人，而每个用户也被许多人关注。笼统地说这两个操作有两种实现方式：

1. 发布一条推文就是简单地把新推文插入到推文全局集合中。当一个用户请求主页时间线时，查找他关注的所有人，再查找这些人所有的推文，然后合并它们（根据时间排序）。在一个类似图1-2的关系型数据库中，你可以写这样一条查询语句：
```SQL
SELECT tweets.*, users.* FROM tweets
  JOIN users    ON tweets.sender_id     = users.id
  JOIN follows  ON follows.followee_id  = users.id
  WHERE follows.follower_id = current_user
```

2. 维护每个用户主页时间线的缓存——好像每个接收用户都有一个推文邮箱一样（见图1-3）。当一个用户*发推文*，查找所有关注这个用户的人，然后把新推文插入到他们的主页时间线缓存中。于是读取主页时间线的请求代价将很低，因为结果已经提前计算好了。

Twitter第一版用的第一种方式，但是系统一直挣扎于跟上主页时间线请求的负载，于是公司决定切换到第二种方式。这样效果更好，因为发推文的平均速率差不多低于主页时间线读取速率两个数量级，所以这个故事里更倾向于花更多的时间在写入而不是在读取。

然而，第二种方式的缺点是如今发推文需要许多额外的处理。平均一条推文被推送给大约75个粉丝，于是每秒4600条推文变成了34.5万次对主页时间线缓存的写入。但是这个平均数掩盖了一个事实，那就是每个用户的粉丝数量差别非常大，一些用户有3千万粉丝。这意味着单单一条推文将导致超过三千万次的主页时间线写入。要及时做到这一点——Twitter尝试在五秒钟之内将推文推送给粉丝——是一项艰巨的挑战。

在Twitter的例子里，每个用户粉丝数量的分布情况（也许再加上这些用户发推频率的权重）是在讨论可扩展性时的一个关键负载参数，因为它决定着扇出负载。你的应用也许有着非常不一样的特性，但是你可以应用类似的原则来推导它的负载。

Twitter故事最终反转了：现在第二种方式已经实现的很健壮了，Twitter开始朝着两者方式混合的方式进发了。绝大部分用户的推文依旧在被发出的时候扇出到主页时间线，但是一小部分有着海量粉丝的用户（比如名人们）不再使用这种扇出。用户读取时间线时，用户关注的名人的推文是单独获取然后合并进去的，类似第一种方式。这种混合的方式可以提供一致的良好体验。我们会在讲解了更多技术面之后在第12章重新回顾这个例子。

### 描述性能

一旦你可以描述你的系统的负载，你可以开始考察负载增加时发生了什么。你可以用两种方式考察：

* 当你增加一个负载参数而保持系统资源（CPU、内存、网络带宽等等）不变，系统的性能受到了怎样的影响？
* 当你增加一个负载参数时，如果要保持性能不变你需要增加多少系统资源？

两个问题都需要性能数字，于是让我们简单看一下如何描述一个系统的性能。

在一个类似Hadoop的批处理系统中，我们通常关心吞吐量——每秒可处理的记录数，或者在一个特定大小数据集上执行一个任务所需要的总时间。对于在线系统，通常更重要的是服务的响应时间——那是客户端发送请求到收到响应之间的时间。


> #### 延迟与响应时间

>*延迟*与*响应时间*经常被混用，然而他们是不一样的。响应时间是客户端可以观察到的：除了实际处理请求的时间（*服务时间*）之外，它还包含网络延迟以及排队延迟。延迟是一个请求等待处理的时长——这段时间里它是*潜在的*请求，在等待服务处理。

即使你重复发出完全一样的请求，每一次你都会得到稍微不同的响应使劲。实践中当一个系统可以处理一系列不同的请求时，响应时间可以非常不一样。因此我们不能把响应时间视为单个数字，而应该是你可以测量的一系列数字分布。

在图1-4中，每个灰色条代表一次服务请求，它的高度代表了请求所花费的时间。绝大部分请求都非常快，这很合理，然而还是有少数“突出”情况，请求花费了更长时间。或许，这些慢的请求本来处理起来就代价很高，比如它处理了更多的数据。然而即使在某个场景下你认为所有的请求都应该花费同样的时间，你得到的是变化的值：增加的随机延迟可以是由于进程上下文切换到了后台，网络包丢失导致TCP重传，垃圾回收导致的暂停，寻页缺失强制了一次磁盘读取，机架上的机械振动，或者许许多多其它原因。

所以通常我们会看到服务报告的*平均*响应时间。（严格的讲，“平均”这个词并不指代任何特定的公式，而是实践中我们一般认为的*算数平均值*：给定*n*个值，把所有值加起来，然后除以*n*）然而如果你想知道服务通常的响应时间，平均值并不是一个非常好的指标，因为他没有告诉你多少用户在切身体会这种延迟。

通常更好的方案是使用百分位。如果将响应时间列表从快到慢排序，那么*中值*就是中间点：举个例子，如果你的中间点响应时间是200毫秒，那意味着一半请求在200毫秒内返回，而另外一半请求需要的时间比它更长。

如果你想知道用户一般需要等待多久中间点是一项好的指标：一半的用户请求在小于中间点响应时间内被响应而另外一半则超过这个中间点。这个中间点也被称为*第五十百分位*，有时简称为*p50*。注意，中间点指单一请求；如果用户发起了好几次请求（在一次对话内，或是因为某个页面包含了好几个资源），那么至少其中一个请求的时间超过中间点的可能性大于50%。

如果要搞清楚那些少数情况到底有多恶劣，你可以观察更高的百分位：*第95*，*第99*以及*第99.9*百分位都是常见选择（缩写为*p95*，*p99*和*p999*）。它们是响应时间阙值，分别有95%、99%或99.9%的请求响应时间大于这个值。举例来说，如果第95百分位响应时间为1.5秒，那意味着100次请求中的95次响应时间小于1.5秒，而另外5次的响应时间为1.5秒或者更多。这表现在了图1-4中。

响应时间的高百分位，也称为*尾延迟*，是非常重要的，因为它们直接影响服务的用户体验。举个例子，亚马逊对内部服务的响应时间要求为第99.9百分位，哪怕这样只是影响1000次请求中的1次。这是因为用户最慢的请求通常是账户中有着最多数据的用户发出的，而他们完成许多次购买——也就是说，他们是最有价值的客户。通过保证网站对于他们足够快从而使得这些用户开心是很重要的：亚马逊也观察到响应时间增加100毫秒使得销售额降低1%，同时也有其它报导称每延迟1秒会降低16%的用户满意度。

另一方面，优化到第99.99百分位（10000次中只有1次慢的请求）被视为代价太高，且不能产出亚马逊所期待的效益。尝试在非常高百分位降低响应时间是很困难的，因为他们很容易受随机事件的影响，它们超出你的控制范围，而好处非常少。

举个例子，百分位经常被用在*服务等级目标*（SLO）、*服务等级协议*（SLA）以及合约中，从而定义服务所期望的性能与可用性。一个SLA也许会声明服务需要达到低于200毫秒的中间点响应时间以及第99百分位响应时间低于1秒（如果响应时间时间更长，这也被认为是不达标），同时服务也被要求满足至少99.9%的在线时间。这些指标指明了用户对服务的期望，允许客户在SLA未达标的情况下要求退款。

排队延迟经常是高百分位响应时️间中的一大部分。由于服务器只能并行处理一小部分事情（受限于，比如，它的CPU核心数），那么只需要一小部分慢请求就可以阻塞随后请求的处理——有时我们把这种影响称作*队头阻塞*。即使这些随后的请求在服务端被很快的处理了，然而客户端会感受到慢的总体响应时间，毕竟花费了些时间等待先前请求的完成。正是由于这样的影响，在客户端测量响应时间是多么的重要。

当人为生成负载从而测试系统的扩展性时，生成负载的客户端需要持续发送请求而不受响应时间的影响。如果客户端等待前一次请求完成后再发送下一次，这种行为人为地使队列比实际情况中的短，从而影响测量结果。

> #### 百分位实战

> 在响应单一用户请求而被调用了多次的后端服务中，高百分位变得特别重要。即使你并行发起这些请求，完成终端用户请求仍然需要等待并发中最慢的那个完成。如图1-5所示，只要有一个慢请求，整个用户请求就变慢了。即使只有一小部分后端调用很慢，当一个终端请求需要多个后端调用时，得到一个慢调用的可能性就增加了，于是一大部分终端用户请求最终变慢（这个影响被称为*尾延迟放大*）。

> 如果你想提升服务在监控板上的响应时间百分位，你需要持续有效的计算它们。例如，你大概想要保留最近10分钟滚动窗口内请求的响应时间。每一个分钟，你都可以计算窗口内中间点以及不同的百分位值，并且在图标上画出这些指标。

> 最简单的实现就是记录时间窗口内所有请求的响应时间列表，然后每分钟对列表排序。如果这样对你来说太没有效率，有现成的算法来计算一个大概的百分位值且只有最少的CPU和内存消耗，例如forward decay、t-digest或者HdrHistogram。请注意，计算平均百分位值，比如，为了减少运算时间或者是把数台机器的数据合并起来，从数学角度来说是没有意义的——合计响应时间的正确方式是计算直方图的和。

### 应对负载的途径

现在我们已经讨论了描述负载的参数以及衡量性能的指标，我们可以开始认真讨论可扩展性了：如果负载参数增常了一些我们怎么维持系统良好的性能呢？

适合某个级别负载的架构不一定能应对原先10倍的负载量。如果你的工作内容是一个正在快速增长的服务，你很可能需要在每一个数量级增长情况下都重新审视一下系统架构——或者，更加频繁一些。

人们经常会谈起*提升*（*垂直扩展*，迁移到一个更强大的设备）和*拓展*（*水平扩展*，把负载分散到好几台小设备上）这样的二分法。负载分散也被成为*无共享*架构。可以运行在单一设备上的系统通常更简单，但是高端设备会非常昂贵，所以非常集中的负载通常没法避免需要拓展。现实中，优秀的架构通常务实地选择中间路线：比如，相比于使用许多小型虚拟机，使用少数性能相当强悍的设备使架构既简单又便宜。

一些系统是*弹性的*，即相对于其它需要手动扩展（人们分析系统容量并决定添加更多的设备到系统中）的系统，它们在监测到负载增加时能自动增加系统资源。在负载难以预测的情况下弹性系统是非常有用的，然而手动扩展系统更简单，并且操作意外会更少（详见“重新平衡分区”）。

部署无状态服务到多台设备是相当简单直接的，相比之下将一个有状态的数据系统从单节点设计迁移到分布式设计会增添复杂性。因此，目前常见的办法是将数据库保留在单一节点上（提升），直到扩展成本或者高可用性迫使你选择分布式架构。

随着用于分布式系统的工具和抽象层越来越好，这种常见的办法也许会变，至少对于某些类别应用是这样的。可以想像，分布式数据系统在未来会是默认选择，即使使用场景不需要处理大量数据或是流量。接下来在本书中我们将涉及到许多种类的分布式数据系统，并讨论它们是如何解决扩展性问题、易用性和可维护性的。

操作海量数据的系统架构通常是针对应用高度定制化的——世界上没有一种通用的、适合所有量级的架构（也被非正式地叫做*神奇扩展酱汁*）。问题也许是读取量，写入量，需要储存的数据量，数据本身的复杂度，响应时间的需求，访问模式，或者（通常）是这些问题的大杂烩，再外加许多别的问题。

举个例子，一个被设计为每秒处理100000个请求、每个请求1KB大小的系统，与一个被设计为每秒处理3个请求、每个请求2GB大小的系统差别会很大——即使两个系统有一样的数据吞吐量。

一个针对特定应用扩展良好的架构是围绕着相关负载参数常见与否的假设构建的。如果假设最终证明是错的，那么为了扩展性而做的努力最好情况就是完全浪费了，而最坏情况则是适得其反。在刚刚起步或是未证明的产品里，通常更重要的是产品功能可以快速迭代，而不是为了将来可能的负载去做可扩展性的工作。

虽然只是适合于特定应用，然而可扩展的架构通常由通用构建模块构建的，并以熟悉的方式组合起来的。本书也会讨论这些构建模块和组合方式。

## 可维护性

众所周知，软件成本的大头不是起始的开发部分，而是持续的维护——修正错误，维持系统运转，调查崩溃原因，迁移到新的平台，针对新用例的修改，偿还技术债，以及添加新功能。

然而不幸的是，许多工作为软件系统工作的人们不喜欢维护所谓的*遗留*系统——这也许包含修正别人犯下的错误，或者在已经过时了的系统上工作，亦或系统被迫做它们从来没有打算的工作。每一个遗留系统都有着各自令人不愉快的方式，于是很难给出应对它们的一般性建议。

然而，我们可以也应该以最大限度减小维护期痛苦的方式设计软件，从而因此避免我们自己构建遗留系统。为了达成这个目标，我们需要特别关注软件系统的三大设计原则：

*可操作性*

为运营团队降低难度从而使系统平稳运行。

*简单*

尽可能从系统中消除复杂度，从而为新工程师理解系统降低难度。（注意，这可不是简化用户界面。）

*可进化性*

为工程师在将来对系统进行修改降低难度，适应因需求变化引入的意想不到的用例。也被称为*可扩展性*、*可修改性*或者*可塑性*。

类似先前介绍的可靠性与可扩展性，没有解决方案可以轻松达成这样的设计目标。然而我们在思考系统的时候应该把可操作性、简易和可进化性谨记心头。

### 可操作性：使运营变得轻松起来

有人建议“好的运营经常可以绕过糟糕的（或是未完成的）软件引入的限制，但是有着恶劣运营的好软件是不可能可靠的运行的”。虽然运营的某些方面可以并且应该实现自动化，然而这仍旧以来人们预先搭建起自动化过程并且保证它工作正常。

运营团队在保证软件系统平稳运行方面至关重要。一个好的运营团通常负责下列任务，甚至更多：
* 监视系统健康并能在它出问题时快速恢复服务
* 追踪问题成因，例如系统崩溃或是性能降级
* 保持软件与平台的更新，包含安全补丁
* 密切关注不同系统之间的影响，从而在引发问题前就能规避有问题的变更
* 预测未来的问题并在实际发生前解决它们（比如，容量规划）
* 为部署、配置管理以及其它建立良好的实践与工具
* 执行复杂的维护任务，例如将应用从一个平台迁移到另外一个
* 当配置变化时保证系统的安全
* 为保证操作可预期以及生产环境稳定而定义流程
* 保全组织关于系统的知识，即使人们加入或离开团队

良好的可运营性使得日常例行任务变得简单，允许运营团队把精力放在高价值的活动上。数据系统可以做许多事使得日常例行任务变得简单，包括：
* 借助良好的监控使运行时行为以及系统内部可见
* 提供对自动化以及标准工具集成的良好支持
* 避免对特定设备的依赖（允许设备下线以维护，而整体系统的运行不受影响）
* 提供良好的说明文档以及容易理解的运营模式（“如果我做了X操作，Y会发生”）
* 提供良好的默认行为，同时在必要时给予管理员重写默认值的自由
* 合适的情况下自我恢复，同时在必要时给予管理员手动控制系统状态的自由
* 表现出可预见的行为，从而最大限度地减少意外

### 简单：管控复杂度

小型软件项目可以有简单且富于表达的代码，但是随着项目越来越庞大，它们经常变得非常复杂并且难以理解。这种复杂性会拖慢每一个需要在系统上工作的人的进度，更会增加维护成本。深陷复杂度问题泥潭的软件项目有时被成为*大泥球*。

复杂度问题有许多种可能的现象：状态空间爆炸，模组之间强耦合，相互引用，不一致的命名法以及术语，为解决性能问题引入的便捷解决方法，为了绕过其它地方问题引入的特殊用例，等等等等。市面上已经有许多讨论这些主题的文章。

当复杂度问题使得维护变得困难的时候，超出预算以及时间表就成了家常便饭。在复杂的软件中，由于变动而引入错误也是有很高风险的：当开发者越来越难以理解和推理时，隐蔽的假设、意外的运行结果和交互过程更容易被忽视。相反地，极大降低复杂性将提升软件的可维护度，因此简单明了应该是我们构建系统的核心目标。

使系统变得简单并不意味着减少它的功能；它也可以理解为移除*偶然引入的*复杂性。Moseley和Marks给偶然引入的复杂性下了定义：不是那些软件被设计解决的（用户可以看到的）固有问题，而是因为实现引入的问题。

移除偶然引入的复杂性问题的一个最佳工具是*抽象*。好的抽象可以将大量的实现细节隐藏在清晰且易于理解的表象后边。好的抽象也可以广泛用于不同的应用中。重用不仅仅是因为比重复实现更有效率，更因为它使得我们更容易实现高质量的软件，所有使用抽象组件的应用都能因为组件质量改善而受益。

举例来说，高级编程语言就是一种抽象，它隐藏了机器码，CPU寄存器以及系统调用。SQL是一种抽象，它隐藏了复杂的磁盘与内存数据结构，来自不同客户端的并发请求，以及崩溃后的不一致问题。当然在使用高级语言编程时，我们仍然在使用机器码；我们只是没有*直接*使用它，因为编程语言的抽象免去了我们考虑它的必要。

然而，找到合适的抽象是非常难的。在分布式系统领域，虽然已经有了许多好的算法，然而我们应当如何抽象系统使得其复杂度被控制在一个合理的范畴仍不明朗。

纵观全书，我们将始终关注那些允许我们将大型系统部件变成良好定义的、可被重复使用的组件的抽象方法。

### 可进化性：使变动变得简单

系统需求保持永远不变基本上是不可能的。它们更可能是在持续的变化：获得新的认知，之前没有预见的使用场景出现，业务优先级变更，用户要求新功能，新平台替代了就平台，法律或是监管要求变更，系统增长迫使架构变更等等等等。

组织流程术语里，*敏捷*的工作方式提供了适应变更的框架。敏捷社区也开发了技术工具和模式，这些对在快速变动环境中开发软件是有帮助的，例如测试驱动开发和重构。

大部分关于敏捷技术的讨论都居家在相当小的、本地规模（同一个应用的几个源代码文件）。在和本书中，我们在更大型的数据系统层面寻找提升敏捷的方式，系统也许包含几种不同特性的应用或是服务。举例来说，你将如何“重构”Twitter的架构，使得在构建主页时间线时从方式1转到方式2？

可以修改数据系统并且使它适应不断变更的需求之便利，是与系统的简单以及系统的抽象密不可分的：简单且易于理解的系统相比于复杂系统通常更容易修改。但正是由于它如此的重要，我们将用另外一个词指代数据系统界别的敏捷：可进化性。

## 小结

在这一章中，我们探索了一些简单的数据密集型应用程序的思考方式。在本书余下的部分，当我们深入到复杂的技术细节时这些原则将指引我们。

应用程序必须要满足各种不同的需求才能变得有用。这包括*功能性需求*（它应当完成的，比如允许数据以不同的方式存储，读取，搜索以及处理），以及某些*非功能性需求*（一些通用的属性，比如安全性、可靠性、合规性、可扩展性、兼容性以及可维护性）。在这一章中我们具体讨论了可靠性，可扩展性以及可维护性。

*可靠性*是指保证系统工作正常，即使有故障发生。故障可以是硬件的（通常比较随机且互无关联），也可以是软件的（bug通常是系统性的且很难处理），或者是人为的（无法避免的时不时犯错）。而容错技术使得特定类型的故障对最终用户不可见。

*可扩展性*意味着有策略的保持性能良好，即使负载增加。在讨论可扩展性之前，我们首先需要描述负载和量化性能的方法。我们简单地了解了Twitter的主页时间线并把它作为描述负载的例子，以及响应时间百分位并把它作为描述性能的方式。在一个可扩展的系统中，你可以提升处理容量从而保证在高负载下依旧可靠。

*可维护性*有许多方面 ，但是核心是为了让那些需要工作在系统上的工程师和运营团队生活更轻松。良好的抽象可以帮助降低复杂度，并且使得系统变得容易修改并且能适应新的用例。良好的运营性意味着系统的健康状况有良好的可见性，同时有着行之有效的方式管控它。

不幸的是没有办法使得应用程序轻松变得更可靠、更方便扩展或者更容易维护。然而还是有某些模式与技术重复出现在不同种类的应用中。在接下来的几章我们将研究几个数据系统示例并且分析它们是如何做到这些的。

在本书的稍后部分，第三部分，我们将会研究那些由好几个组件协作构成的系统模式，比如图1-1中的那个。
